> 本文内容由[主成分分析](https://zh.wikipedia.org/wiki/主成分分析)转换而来。


[GaussianScatterPCA.png](https://zh.wikipedia.org/wiki/File:GaussianScatterPCA.png "fig:GaussianScatterPCA.png")為(1, 3)、標準差在(0.878, 0.478)方向上為3、在其正交方向為1的[高斯分布](https://zh.wikipedia.org/wiki/高斯分布 "wikilink")。這裡以黑色顯示的兩個向量是這個分布的[共變異數矩陣的](https://zh.wikipedia.org/wiki/共變異數矩陣 "wikilink")[特征向量](https://zh.wikipedia.org/wiki/特征向量 "wikilink")，其長度按對應的[特征值之平方根為比例](https://zh.wikipedia.org/wiki/特征值 "wikilink")，並且移動到以原分布的平均值為原點。\]\]在多元统计分析中，**主成分分析**（，）是一種统计分析、簡化數據集的方法。它利用[正交变换](../Page/正交变换.md "wikilink")来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分（Principal Components）。具体地，主成分可以看做一个线性方程，其包含一系列线性系数来指示投影方向。PCA对原始数据的正则化或预处理敏感（相对缩放）。

**基本思想：**

  - 将坐标轴中心移到数据的中心，然后旋转坐标轴，使得数据在C1轴上的方差最大，即全部n个数据个体在该方向上的投影最为分散。意味着更多的信息被保留下来。C1成为**第一主成分**。
  - C2**第二主成分**：找一个C2，使得C2与C1的协方差（相关系数）为0，以免与C1信息重叠，并且使数据在该方向的方差尽量最大。
  - 以此类推，找到第三主成分，第四主成分。。。。第p个主成分。p个随机变量可以有p个主成分\[1\]。

主成分分析经常用于减少数据集的[维数](https://zh.wikipedia.org/wiki/维数 "wikilink")，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。

主成分分析由[卡尔·皮尔逊](../Page/卡尔·皮尔逊.md "wikilink")於1901年發明\[2\]，用於分析數據及建立數理模型，在原理上与[主轴定理相似](https://zh.wikipedia.org/wiki/主轴定理 "wikilink")。之后在1930年左右由[哈罗德·霍特林独立发展并命名](https://zh.wikipedia.org/wiki/哈罗德·霍特林 "wikilink")。依据应用领域的不同，在信号处理中它也叫做离散[K-L 转换](../Page/K-L_轉換.md "wikilink")（discrete Karhunen–Loève transform (KLT)）。其方法主要是通過對[共變異數矩陣進行特征分解](https://zh.wikipedia.org/wiki/共變異數矩陣 "wikilink")\[3\]，以得出數據的主成分（即[特征向量](https://zh.wikipedia.org/wiki/特征向量 "wikilink")）與它們的權值（即[特征值](https://zh.wikipedia.org/wiki/特征值 "wikilink")\[4\]）。PCA是最簡單的以特征量分析多元統計分布的方法。其結果可以理解為對原數據中的[方差](../Page/方差.md "wikilink")做出解釋：哪一個方向上的數據值對方差的影響最大？換而言之，PCA提供了一種降低數據[維度](../Page/維度.md "wikilink")的有效辦法；如果分析者在原數據中除掉最小的[特征值所對應的成分](https://zh.wikipedia.org/wiki/特征值 "wikilink")，那麼所得的低維度數據必定是最優化的（也即，這樣降低維度必定是失去訊息最少的方法）。主成分分析在分析複雜數據時尤為有用，比如[人臉識別](https://zh.wikipedia.org/wiki/人臉識別 "wikilink")。

PCA是最简单的以特征量分析多元统计分布的方法。通常情况下，这种运算可以被看作是揭露数据的内部结构，从而更好的解释数据的变量的方法。如果一个多元数据集能够在一个高维数据空间坐标系中被显现出来，那么PCA就能够提供一幅比较低维度的图像，这幅图像即为在讯息最多的点上原对象的一个‘投影’。这样就可以利用少量的主成分使得数据的维度降低了。

PCA跟因子分析密切相关，并且已经有很多混合这两种分析的统计包。而真实要素分析则是假定底层结构，求得微小差异矩阵的特征向量。

## 数学定义

PCA的数学定义是：一个[正交化](https://zh.wikipedia.org/wiki/正交化 "wikilink")[线性变换](https://zh.wikipedia.org/wiki/线性变换 "wikilink")，把数据变换到一个新的[坐标系统中](https://zh.wikipedia.org/wiki/坐标系统 "wikilink")，使得这一数据的任何[投影](../Page/投影.md "wikilink")的第一大[方差](../Page/方差.md "wikilink")在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推\[5\]。

定义一个*n* × *m*的[矩阵](../Page/矩阵.md "wikilink"), **X**<sup>T</sup>为去[平均值](https://zh.wikipedia.org/wiki/平均值 "wikilink")（以平均值为中心移动至原点）的数据，其行为数据样本，列为数据类别（注意，这里定义的是**X**<sup>T</sup> 而不是**X**）。则**X**的[奇异值分解](../Page/奇异值分解.md "wikilink")为**X** = **WΣV<sup>T</sup>**，其中*m* × *m*矩阵**W**是**XX**<sup>T</sup>的[特征向量矩阵](https://zh.wikipedia.org/wiki/特征向量 "wikilink")， **Σ**是*m* × *n*的非负矩形对角矩阵，V是*n* × *n*的**X<sup>T</sup>X**的[特征向量矩阵](https://zh.wikipedia.org/wiki/特征向量 "wikilink")。据此，

\[\begin{align}
\boldsymbol{Y}^\top & = \boldsymbol{X}^\top\boldsymbol{W} \\
& = \boldsymbol{V}\boldsymbol{\Sigma}^\top\boldsymbol{W}^\top\boldsymbol{W} \\
& = \boldsymbol{V}\boldsymbol{\Sigma}^\top
\end{align}\]

当 *m* \< *n* − 1时，**V** 在通常情况下不是唯一定义的，而**Y** 则是唯一定义的。**W** 是一个[正交矩阵](../Page/正交矩阵.md "wikilink")，**Y**<sup>T</sup>**W**<sup>T</sup>=**X**<sup>T</sup>，且**Y**<sup>T</sup>的第一列由第一主成分组成，第二列由第二主成分组成，依此类推。

为了得到一种降低数据维度的有效办法，我们可以利用**W**<sub>L</sub>把 **X** 映射到一个只应用前面L个向量的低维空间中去：

\[\mathbf{Y}=\mathbf{W_L}^\top\mathbf{X} = \mathbf{\Sigma_L}\mathbf{V}^\top\] 其中\(\mathbf{\Sigma_L}=\mathbf{I}_{L\times m}\mathbf{\Sigma}\)，且\(\mathbf{I}_{L\times m}\)為\(L\times m\)的[單位矩陣](../Page/單位矩陣.md "wikilink")。

**X** 的单向量矩阵**W**相当于[协方差矩阵](../Page/协方差矩阵.md "wikilink")的[特征向量](https://zh.wikipedia.org/wiki/特征向量 "wikilink") **C** = **X X**<sup>T</sup>,

\[\mathbf{X}\mathbf{X}^\top = \mathbf{W}\mathbf{\Sigma}\mathbf{\Sigma}^\top\mathbf{W}^\top\]

在[欧几里得空间](../Page/欧几里得空间.md "wikilink")给定一组点数，第一主成分对应于通过多维空间平均点的一条线，同时保证各个点到这条直线距离的平方和最小。去除掉第一主成分后，用同样的方法得到第二主成分。依此类推。在**Σ**中的[奇异值均为矩阵](https://zh.wikipedia.org/wiki/奇异值 "wikilink") **XX**<sup>T</sup>的[特征值的平方根](https://zh.wikipedia.org/wiki/特征值 "wikilink")。每一个特征值都与跟它们相关的方差是成正比的，而且所有特征值的总和等于所有点到它们的多维空间平均点距离的平方和。PCA提供了一种降低维度的有效办法，本质上，它利用正交变换将围绕平均点的点集中尽可能多的变量投影到第一维中去，因此，降低维度必定是失去讯息最少的方法。PCA具有保持子空间拥有最大方差的最优正交变换的特性。然而，当与[离散余弦变换](../Page/离散余弦变换.md "wikilink")相比时，它需要更大的计算需求代价。非线性降维技术相对于PCA来说则需要更高的计算要求。

PCA对变量的缩放很敏感。如果我们只有两个变量，而且它们具有相同的样本方差，并且成正相关，那么PCA将涉及两个变量的主成分的旋转。但是，如果把第一个变量的所有值都乘以100，那么第一主成分就几乎和这个变量一样，另一个变量只提供了很小的贡献，第二主成分也将和第二个原始变量几乎一致。这就意味着当不同的变量代表不同的单位（如温度和质量）时，PCA是一种比较武断的分析方法。但是在Pearson的题为 "On Lines and Planes of Closest Fit to Systems of Points in Space"的原始文件里，是假设在欧几里得空间里不考虑这些。一种使PCA不那么武断的方法是使用变量缩放以得到单位方差。

## 讨论

通常，为了确保第一主成分描述的是最大方差的方向，我们会使用平均减法进行主成分分析。如果不执行平均减法，第一主成分有可能或多或少的对应于数据的平均值。另外，为了找到近似数据的最小均方误差，我们必须选取一个零均值\[6\]。

假设零经验均值，数据集 **X** 的主成分*w*<sub>1</sub>可以被定义为：

\[\mathbf{w}_1
 = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{\arg\,max}}\,\operatorname{Var}\{ \mathbf{w}^\top \mathbf{X} \}
 = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{\arg\,max}}\,E\left\{ \left( \mathbf{w}^\top \mathbf{X}\right)^2 \right\}\]

为了得到第 *k*个主成分，必须先从**X**中减去前面的 \(k - 1\) 个主成分：

\[\mathbf{\hat{X}}_{k - 1}
 = \mathbf{X} -
 \sum_{i = 1}^{k - 1}
 \mathbf{w}_i \mathbf{w}_i^\top \mathbf{X}\]

然后把求得的第*k*个主成分带入数据集，得到新的数据集，继续寻找主成分。

\[\mathbf{w}_k
 = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{arg\,max}}\,E\left\{
 \left( \mathbf{w}^\top \mathbf{\hat{X}}_{k - 1}
 \right)^2 \right\}.\]

PCA相当于在气象学中使用的经验正交函数（EOF）,同时也类似于一个线性隐层神经网络。 隐含层 *K* 个神经元的权重向量收敛后，将形成一个由前 *K* 个主成分跨越空间的基础。但是与PCA不同的是，这种技术并不一定会产生正交向量。

PCA是一种很流行且主要的的模式识别技术。然而，它并不能最优化类别可分离性\[7\] 。另一种不考虑这一点的方法是线性判别分析。

## 符号和缩写表

<table>
<thead>
<tr class="header">
<th><p>Symbol符号</p></th>
<th><p>Meaning意义</p></th>
<th><p>Dimensions尺寸</p></th>
<th><p>Indices指数</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><span class="math inline"><strong>X</strong> = {<em>X</em>[<em>m</em>, <em>n</em>]}</span></p></td>
<td><p>由所有数据向量集组成的数据矩阵，一列代表一个向量</p></td>
<td><p><span class="math inline"><em>M</em> × <em>N</em></span></p></td>
<td><p><span class="math inline"><em>m</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>n</em> = 1…<em>N</em></span></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><em>N</em> </span></p></td>
<td><p>数据集中列向量的个数</p></td>
<td><p><span class="math inline">1 × 1</span></p></td>
<td><p><em>标量</em></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><em>M</em> </span></p></td>
<td><p>每个列向量的元素个数</p></td>
<td><p><span class="math inline">1 × 1</span></p></td>
<td><p><em>标量</em></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><em>L</em> </span></p></td>
<td><p>子空间的维数, <span class="math inline">1 ≤ <em>L</em> ≤ <em>M</em></span></p></td>
<td><p><span class="math inline">1 × 1</span></p></td>
<td><p><em>标量</em></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><strong>u</strong> = {<em>u</em>[<em>m</em>]}</span></p></td>
<td><p>经验均值向量</p></td>
<td><p><span class="math inline"><em>M</em> × 1</span></p></td>
<td><p><span class="math inline"><em>m</em> = 1…<em>M</em></span></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><strong>s</strong> = {<em>s</em>[<em>m</em>]}</span></p></td>
<td><p>经验标准方差向量</p></td>
<td><p><span class="math inline"><em>M</em> × 1</span></p></td>
<td><p><span class="math inline"><em>m</em> = 1…<em>M</em></span></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><strong>h</strong> = {<em>h</em>[<em>n</em>]}</span></p></td>
<td><p>所有的单位向量</p></td>
<td><p><span class="math inline">1 × <em>N</em></span></p></td>
<td><p><span class="math inline"><em>n</em> = 1…<em>N</em></span></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><strong>B</strong> = {<em>B</em>[<em>m</em>, <em>n</em>]}</span></p></td>
<td><p>对均值的偏离向量</p></td>
<td><p><span class="math inline"><em>M</em> × <em>N</em></span></p></td>
<td><p><span class="math inline"><em>m</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>n</em> = 1…<em>N</em></span></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><strong>Z</strong> = {<em>Z</em>[<em>m</em>, <em>n</em>]}</span></p></td>
<td><p>Z-分数，利用均值和标准差计算得到</p></td>
<td><p><span class="math inline"><em>M</em> × <em>N</em></span></p></td>
<td><p><span class="math inline"><em>m</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>n</em> = 1…<em>N</em></span></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><strong>C</strong> = {<em>C</em>[<em>p</em>, <em>q</em>]}</span></p></td>
<td><p><a href="../Page/协方差矩阵.md" title="wikilink">协方差矩阵</a></p></td>
<td><p><span class="math inline"><em>M</em> × <em>M</em></span></p></td>
<td><p><span class="math inline"><em>p</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>q</em> = 1…<em>M</em></span></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><strong>R</strong> = {<em>R</em>[<em>p</em>, <em>q</em>]}</span></p></td>
<td><p><a href="https://zh.wikipedia.org/wiki/相关矩阵" title="wikilink">相关矩阵</a></p></td>
<td><p><span class="math inline"><em>M</em> × <em>M</em></span></p></td>
<td><p><span class="math inline"><em>p</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>q</em> = 1…<em>M</em></span></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><strong>V</strong> = {<em>V</em>[<em>p</em>, <em>q</em>]}</span></p></td>
<td><p><strong>C</strong>的所有特征向量集</p></td>
<td><p><span class="math inline"><em>M</em> × <em>M</em></span></p></td>
<td><p><span class="math inline"><em>p</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>q</em> = 1…<em>M</em></span></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><strong>D</strong> = {<em>D</em>[<em>p</em>, <em>q</em>]}</span></p></td>
<td><p>主对角线为特征值的对角矩阵</p></td>
<td><p><span class="math inline"><em>M</em> × <em>M</em></span></p></td>
<td><p><span class="math inline"><em>p</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>q</em> = 1…<em>M</em></span></p></td>
</tr>
<tr class="even">
<td><p><span class="math inline"><strong>W</strong> = {<em>W</em>[<em>p</em>, <em>q</em>]}</span></p></td>
<td><p>基向量矩阵</p></td>
<td><p><span class="math inline"><em>M</em> × <em>L</em></span></p></td>
<td><p><span class="math inline"><em>p</em> = 1…<em>M</em></span><br />
<span class="math inline"><em>q</em> = 1…<em>L</em></span></p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline"><strong>Y</strong> = {<em>Y</em>[<em>m</em>, <em>n</em>]}</span></p></td>
<td><p><strong>X</strong> 和<strong>W</strong>矩阵的投影矩阵</p></td>
<td><p><span class="math inline"><em>L</em> × <em>N</em></span></p></td>
<td><p><span class="math inline"><em>m</em> = 1…<em>L</em></span><br />
<span class="math inline"><em>n</em> = 1…<em>N</em></span></p></td>
</tr>
</tbody>
</table>

## 主成分分析的属性和限制

如上所述，主成分分析的结果依赖于变量的缩放。

主成分分析的适用性受到由它的派生物产生的某些假设\[8\] 的限制。

## 主成分分析和信息理论

通过使用降维来保存大部分数据信息的主成分分析的观点是不正确的。确实如此，当没有任何假设信息的信号模型时，主成分分析在降维的同时并不能保证信息的不丢失，其中信息是由[香农熵](https://zh.wikipedia.org/wiki/香农熵 "wikilink")\[9\]来衡量的。 基于假设得 \(\mathbf{x} = \mathbf{s} +  \mathbf{n}\) 也就是说，向量 *x* 是含有信息的目标信号 *s* 和噪声信号 *n* 之和，从信息论角度考虑主成分分析在降维上是最优的。

特别地，Linsker证明了如果 *s* 是高斯分布，且 *n* 是 与密度矩阵相应的协方差矩阵的高斯噪声，

## 使用统计方法计算PCA

以下是使用统计方法计算PCA的详细说明。但是请注意，如果利用奇异值分解（使用标准的软件）效果会更好。

我们的目标是把一个给定的具有 *M* 维的数据集**X** 变换成具有较小维度 *L*的数据集**Y**。现在要求的就是矩阵**Y**，**Y**是矩阵**X** Karhunen–Loève变换。\[\mathbf{Y} = \mathbb{KLT} \{ \mathbf{X} \}\]

## 组织数据集

假设有一组 *M* 个变量的观察数据，我们的目的是减少数据，使得能够用*L* 个向量来描述每个观察值，*L* \< *M*。进一步假设，该数据被整理成一组具有*N*个向量的数据集，其中每个向量都代表*M* 个变量的单一观察数据。

  - \(\mathbf{x}_1 \ldots \mathbf{x}_N\)为列向量，其中每个列向量有*M* 行。

<!-- end list -->

  - 将列向量放入*M* × *N*的单矩阵**X** 裡。

## 计算经验均值

  - 对每一维*m* = 1, ..., *M*计算经验均值

<!-- end list -->

  - 将计算得到的均值放入一个 *M* × 1维的经验均值向量**u**中

\[u[m] = {1 \over N} \sum_{n=1}^N X[m,n]\]

## 计算平均偏差

对于在最大限度地减少近似数据的均方误差的基础上找到一个主成分来说，均值减去法是该解决方案的不可或缺的组成部分\[10\] 。因此，我们继续如下步骤：

  - 从数据矩阵**X**的每一列中减去经验均值向量 **u**

<!-- end list -->

  - 将平均减去过的数据存储在*M* × *N*矩阵**B**中

\[\mathbf{B} = \mathbf{X} - \mathbf{u}\mathbf{h}\]

  -

      -
        where **h** is a 1 × *N* row vector of all 1s:

其中**h**是一个全 1s:的1 × *N* 的行向量

\[h[n] = 1 \, \qquad \qquad \text{for } n = 1, \ldots, N\]

## 求协方差矩阵

  - 从矩阵**B** 中找到*M* × *M* 的经验协方差矩阵**C**

\[\mathbf{C} = \mathbb{ E } \left[ \mathbf{B} \otimes \mathbf{B} \right] = \mathbb{ E } \left[ \mathbf{B} \cdot \mathbf{B}^{*} \right] = { 1 \over N - 1 } \sum_{} \mathbf{B} \cdot \mathbf{B}^{*}\]

其中 \(\mathbb{E}\)为期望值

\(\otimes\)是最外层运算符

\(* \\)是共轭转置运算符。

请注意，如果B完全由实数组成，那么共轭转置与正常的转置一样。

  - 为什么是N-1,而不是N，[Bessel's correction](https://zh.wikipedia.org/wiki/Bessel's_correction "wikilink")\[11\]给出了解释

### 查找协方差矩阵的特征值和特征向量

  - 计算矩阵**C** 的特征向量

\[\mathbf{V}^{-1} \mathbf{C} \mathbf{V} = \mathbf{D}\]

  -
    其中，**D** 是**C**的特征值对角矩阵，这一步通常会涉及到使用基于计算机的计算特征值和特征向量的算法。在很多矩阵代数系统中这些算法都是现成可用的，如[R语言](../Page/R语言.md "wikilink")，[MATLAB](../Page/MATLAB.md "wikilink"),\[12\]\[13\] [Mathematica](https://zh.wikipedia.org/wiki/Mathematica "wikilink"),\[14\] [SciPy](../Page/SciPy.md "wikilink"), [IDL](../Page/互動式數據語言.md "wikilink")(交互式数据语言), 或者[GNU Octave以及](../Page/GNU_Octave.md "wikilink")[OpenCV](../Page/OpenCV.md "wikilink")。

<!-- end list -->

  - 矩阵**D**为*M* × *M*的对角矩阵

<!-- end list -->

  - 各个特征值和特征向量都是配对的，*m*个特征值对应*m*个特征向量。

## 相关源代码

  - [Cornell Spectrum Imager](http://code.google.com/p/cornell-spectrum-imager/wiki/Home) - An open-source toolset built on ImageJ. Enables quick easy PCA analysis for 3D datacubes.

  - [imDEV](https://web.archive.org/web/20120502214751/http://sourceforge.net/apps/mediawiki/imdev/index.php?title=Main_Page) Free Excel addin to calculate principal components using R package [pcaMethods](http://www.bioconductor.org/packages/1.9/bioc/html/pcaMethods.html).

  - ["ViSta: The Visual Statistics System"](https://web.archive.org/web/20090323151854/http://www.mdp.edu.ar/psicologia/vista/vista.htm) a free software that provides principal components analysis, simple and multiple correspondence analysis.

  - ["Spectramap"](http://www.coloritto.com) is software to create a [biplot](../Page/双标图.md "wikilink") using principal components analysis, correspondence analysis or spectral map analysis.

  - [XLSTAT](https://zh.wikipedia.org/wiki/XLSTAT "wikilink") is a statistical and multivariate analysis software including Principal Component Analysis among other multivariate tools.

  - [FinMath](https://rtmath.net/products/finmath/), a [.NET](../Page/.NET框架.md "wikilink") numerical library containing an implementation of PCA.

  - [The Unscrambler](../Page/化学计量学.md "wikilink") is a multivariate analysis software enabling Principal Component Analysis (PCA) with PCA Projection.

  - [Computer Vision Library](http://sourceforge.net/projects/opencvlibrary/)

  - In the [MATLAB](../Page/MATLAB.md "wikilink") Statistics Toolbox, the functions `princomp` and `wmspca` give the principal components, while the function `pcares` gives the residuals and reconstructed matrix for a low-rank PCA approximation. Here is a link to a MATLAB implementation of PCA [`PcaPress`](http://www.utdallas.edu/~herve/abdi-PCA4Wiley.zip) .

  - In the , principal components analysis is implemented via the `g03aa` routine (available in both the Fortran\[15\] and the C\[16\] versions of the Library).

  - , a proprietary numerical library containing PCA for the [.NET Framework](../Page/.NET框架.md "wikilink").

  - in [Octave](../Page/GNU_Octave.md "wikilink"), a free software computational environment mostly compatible with MATLAB, the function [`princomp`](http://octave.sourceforge.net/statistics/function/princomp.html) gives the principal component.

  - in the [free](../Page/自由软件.md "wikilink") statistical package [R](../Page/R语言.md "wikilink"), the functions [`princomp`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/princomp.html) and [`prcomp`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html) can be used for principal component analysis; `prcomp` uses [singular value decomposition](../Page/奇异值分解.md "wikilink") which generally gives better numerical accuracy. Recently there has been an explosion in implementations of principal component analysis in various R packages, generally in packages for specific purposes. For a more complete list, see here: [1](http://cran.r-project.org/web/views/Multivariate.html).

  - In *XLMiner*, the Principal Components tab can be used for principal component analysis.

  - In [IDL](../Page/互動式數據語言.md "wikilink"), the principal components can be calculated using the function `pcomp`.

  - computes principal components ([javadoc](https://web.archive.org/web/20120411235501/http://weka.sourceforge.net/doc/weka/attributeSelection/PrincipalComponents.html)).

  - [Software for analyzing multivariate data with instant response using PCA](http://www.qlucore.com)

  - supports PCA through its Linear Projection widget.

  - A version of PCA adapted for population genetics analysis can be found in the suite [EIGENSOFT](https://web.archive.org/web/20120217121739/http://genepath.med.harvard.edu/~reich/Software.htm).

  - PCA can also be performed by the statistical software [Partek Genomics Suite](https://web.archive.org/web/20120614230058/http://www.partek.com/partekgs), developed by [Partek](https://www.webcitation.org/5ElLwqoZN?url=http://www.partek.com/).

## 参见

<div style="-moz-column-count:2; column-count:2;">

  -
  - [Canonical correlation](../Page/典型相关.md "wikilink")

  - (can replace of low-rank SVD approximation)

  -
  -
  - [特征脸](../Page/特征脸.md "wikilink")(Eigenface)

  - [多線性主成分分析](../Page/多線性主成分分析.md "wikilink")(Multilinear PCA)

  -
  -
  - [独立成分分析](../Page/独立成分分析.md "wikilink")

  - [核主成分分析](../Page/核主成分分析.md "wikilink")

  - [矩阵分解](https://zh.wikipedia.org/wiki/矩阵分解 "wikilink")

  - [Nonlinear dimensionality reduction](../Page/非线性降维.md "wikilink")

  -
  - (PCA applied to morphometry and computer vision)

  -
  -
  - [奇异值分解](../Page/奇异值分解.md "wikilink")

  -
  - [变换编码](https://zh.wikipedia.org/wiki/变换编码 "wikilink")

  - [最小二乘法](../Page/最小二乘法.md "wikilink")

  -

</div>

  - [圖模式](../Page/圖模式.md "wikilink")
  - [马尔可夫链](../Page/马尔可夫链.md "wikilink")
  - [马尔可夫逻辑网络](../Page/马尔可夫逻辑网络.md "wikilink")

## 注释

## 参考

  -
[Category:多變量統計](https://zh.wikipedia.org/wiki/Category:多變量統計 "wikilink") [Category:矩阵分解](https://zh.wikipedia.org/wiki/Category:矩阵分解 "wikilink") [Category:資料分析](https://zh.wikipedia.org/wiki/Category:資料分析 "wikilink")

1.
2.
3.
4.  Shaw P.J.A. (2003) *Multivariate statistics for the Environmental Sciences*, Hodder-Arnold. ISBN 978-0-340-80763-7.
5.  Jolliffe I.T. [Principal Component Analysis](http://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0), Series: [Springer Series in Statistics](http://www.springer.com/west/home/statistics/statistical+theory+and+methods?SGWID=4-10129-69-173621571-0), 2nd ed., Springer, NY, 2002, XXIX, 487 p. 28 illus. ISBN 978-0-387-95442-4
6.  A. A. Miranda, Y. A. Le Borgne, and G. Bontempi. [New Routes from Minimal Approximation Error to Principal Components](http://www.ulb.ac.be/di/map/yleborgn/pub/NPL_PCA_07.pdf), Volume 27, Number 3 / June, 2008, Neural Processing Letters, Springer
7.
8.  Jonathon Shlens, [A Tutorial on Principal Component Analysis.](http://www.snl.salk.edu/~shlens/pca.pdf)
9.  Geiger, Bernhard; Kubin, Gernot (Sep 2012), [Relative Information Loss in the PCA](http://arxiv.org/abs/1204.0429)
10. A.A. Miranda, Y.-A. Le Borgne, and G. Bontempi. [New Routes from Minimal Approximation Error to Principal Components](http://www.ulb.ac.be/di/map/yleborgn/pub/NPL_PCA_07.pdf), Volume 27, Number 3 / June, 2008, Neural Processing Letters, Springer
11. [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction) Bessel's correction
12. [eig function](http://www.mathworks.com/access/helpdesk/help/techdoc/ref/eig.html#998306) Matlab documentation
13. [MATLAB PCA-based Face recognition software](http://www.mathworks.com/matlabcentral/fileexchange/24634)
14. [Eigenvalues function](http://reference.wolfram.com/mathematica/ref/Eigenvalues.html) Mathematica documentation
15. {{ cite web | last = The Numerical Algorithms Group | first = | title = NAG Library Routine Document: nagf_mv_prin_comp (g03aaf) | date = | work = NAG Library Manual, Mark 23 | url = <http://www.nag.co.uk/numeric/fl/nagdoc_fl23/pdf/G03/g03aaf.pdf> | accessdate = 2012-02-16 }}
16.
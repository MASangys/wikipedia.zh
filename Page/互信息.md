[Entropy-mutual-information-relative-entropy-relation-diagram.svg](https://zh.wikipedia.org/wiki/File:Entropy-mutual-information-relative-entropy-relation-diagram.svg "fig:Entropy-mutual-information-relative-entropy-relation-diagram.svg") 在[概率论和](https://zh.wikipedia.org/wiki/概率论 "wikilink")[信息论](../Page/信息论.md "wikilink")中，两个[随机变量](../Page/随机变量.md "wikilink")的**互信息**（mutual Information，简称MI）或**转移信息**（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息是（PMI）的期望值。互信息最常用的[单位是](../Page/计量单位.md "wikilink")[bit](../Page/位元.md "wikilink")。

## 互信息的定义

一般地，两个离散随机变量 *X* 和 *Y* 的互信息可以定义为：

\[I(X;Y) = \sum_{y \in Y} \sum_{x \in X}
                 p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)}
                              \right) }, \,\!\]

其中 *p*(*x*, *y*) 是 *X* 和 *Y* 的[联合概率分布函数](../Page/联合分布.md "wikilink")，而 \(p(x)\) 和 \(p(y)\) 分别是 *X* 和 *Y* 的[边缘概率分布函数](../Page/条件概率.md "wikilink")。

在[连续随机变量的情形下](../Page/连续函数.md "wikilink")，求和被替换成了[二重定积分](https://zh.wikipedia.org/wiki/二重积分 "wikilink")：

\[I(X;Y) = \int_Y \int_X
                 p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)}
                              \right) } \; dx \,dy,\]

其中 *p*(*x*, *y*) 当前是 *X* 和 *Y* 的联合概率*密度*函数，而 \(p(x)\) 和 \(p(y)\) 分别是 *X* 和 *Y* 的边缘概率密度函数。

如果对数以 2 为基底，互信息的单位是[bit](../Page/位元.md "wikilink")。

直观上，互信息度量 *X* 和 *Y* 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 *X* 和 *Y* 相互独立，则知道 *X* 不对 *Y* 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 *X* 是 *Y* 的一个确定性函数，且 *Y* 也是 *X* 的一个确定性函数，那么传递的所有信息被 *X* 和 *Y* 共享：知道 *X* 决定 *Y* 的值，反之亦然。因此，在此情形互信息与 *Y*（或 *X*）单独包含的不确定度相同，称作 *Y*（或 *X*）的[熵](https://zh.wikipedia.org/wiki/信息熵 "wikilink")。而且，这个互信息与 *X* 的熵和 *Y* 的熵相同。（这种情形的一个非常特殊的情况是当 *X* 和 *Y* 为相同随机变量时。）

互信息是 *X* 和 *Y* 的[联合分布](../Page/联合分布.md "wikilink")相对于假定 *X* 和 *Y* 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：*I*(*X*; *Y*) = 0 [当且仅当](../Page/当且仅当.md "wikilink") *X* 和 *Y* 为独立随机变量。从一个方向很容易看出：当 *X* 和 *Y* 独立时，*p*(*x*,*y*) = *p*(*x*) *p*(*y*)，因此：

\[\log{ \left( \frac{p(x,y)}{p(x)\,p(y)} \right) } = \log 1 = 0. \,\!\]

此外，互信息是非负的（即 \(I(X;Y)\ge0\); 见下文），而且是（即 \(I(X;Y) = I(Y;X)\)）。

## 与其他量的关系

互信息又可以等价地表示成

\[\begin{align}
I(X;Y) & {} = H(X) - H(X|Y) \\
& {} = H(Y) - H(Y|X) \\
& {} = H(X) + H(Y) - H(X,Y) \\
& {} = H(X,Y) - H(X|Y) - H(Y|X)
\end{align}\]

其中 \(\ H(X)\) 和 \(\ H(Y)\) 是边缘[熵](https://zh.wikipedia.org/wiki/信息熵 "wikilink")，*H*(*X*|*Y*) 和 *H*(*Y*|*X*) 是[条件熵](../Page/条件熵.md "wikilink")，而 *H*(*X*,*Y*) 是 *X* 和 *Y* 的[联合熵](../Page/联合熵.md "wikilink")。注意到这组关系和并集、差集和交集的关系类似，于是用Venn图表示。

在互信息定义的基础上使用[琴生不等式](https://zh.wikipedia.org/wiki/琴生不等式 "wikilink")，我们可以证明 *I*(*X*;*Y*) 是非负的，因此 \(\ H(X) \ge H(X|Y)\)。这里我们给出 I(X;Y) = H(Y) - H(Y|X) 的详细推导:

\[\begin{align}
I(X;Y) & {} = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\
& {} = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)} - \sum_{x,y} p(x,y) \log p(y)  \\

& {} = \sum_{x,y} p(x)p(y|x) \log p(y|x) - \sum_{x,y} p(x,y) \log p(y) \\
& {} = \sum_x p(x) \left(\sum_y p(y|x) \log p(y|x)\right) - \sum_y \log p(y) \left(\sum_x p(x,y)\right) \\

& {} = -\sum_x p(x) H(Y|X=x) - \sum_y \log p(y) p(y) \\
& {} = -H(Y|X) + H(Y)  \\
& {} = H(Y) - H(Y|X).  \\
\end{align}\] 上面其他性质的证明类似。

直观地说，如果把熵 *H*(*Y*) 看作一个随机变量於不确定度的量度，那么 *H*(*Y*|*X*) 就是"在已知 *X* 事件後*Y*事件會發生"的不確定度。於是第一个等式的右边就可以读作“將"Y事件的不确定度"，减去 --- "在基於*X*事件後*Y*事件因此發生的不確定度"”。

这证实了互信息的直观意义为: "因X而有Y事件"的熵( 基於已知隨機變量的不確定性) 在"Y事件"的熵之中具有多少影響地位( "Y事件所具有的不確定性" 其中包含了多少 "Y|X事件所具有的不確性" )，意即"Y具有的不確定性"有多少程度是起因於X事件;

`    `*`舉例來說，當``   ``I(X;Y)``   ``=``   ``0時，也就是``   ``H(Y)``   ``=``   ``H(Y|X)時，即代表此時``   ``"Y的不確定性"``   ``即為``   ``"Y|X的不確定性"，這說明了互信息的具體意義是在度量`**`兩個事件彼此之間的關聯性`**`。`*

所以具體的解釋就是: 互信息越小，兩個來自不同事件空間的隨機變量彼此之間的關聯性越低; 互信息越高，關聯性則越高 。

注意到离散情形 *H*(*X*|*X*) = 0，于是 *H*(*X*) = *I*(*X*;*X*)。因此 *I*(*X*;*X*) ≥ *I*(*X*;*Y*)，我们可以制定”一个变量至少包含其他任何变量可以提供的与它有关的信息“的基本原理。

互信息也可以表示为两个随机变量的[边缘分布](https://zh.wikipedia.org/wiki/边缘分布 "wikilink") *X* 和 *Y* 的乘积 *p*(*x*) × *p*(*y*) 相对于随机变量的[联合熵](../Page/联合熵.md "wikilink") *p*(*x*,*y*) 的[相对熵](../Page/相对熵.md "wikilink")：

\[I(X;Y) = D_{\mathrm{KL}}(p(x,y)\|p(x)p(y)).\]

此外，令 *p*(*x*|*y*) = *p*(*x*, *y*) / *p*(*y*)。则

\[\begin{align}
I(X;Y) & {} = \sum_y p(y) \sum_x p(x|y) \log_2 \frac{p(x|y)}{p(x)} \\
& {} =  \sum_y p(y) \; D_{\mathrm{KL}}(p(x|y)\|p(x)) \\
& {} = \mathbb{E}_Y\{D_{\mathrm{KL}}(p(x|y)\|p(x))\}.
\end{align}\]

注意到，这里相对熵涉及到仅对随机变量 *X* 积分，表达式 \(D_{\mathrm{KL}}(p(x|y)\|p(x))\) 现在以 *Y* 为变量。于是互信息也可以理解为相对熵 *X* 的单变量分布 *p*(*x*) 相对于给定 *Y* 时 *X* 的[条件分布](https://zh.wikipedia.org/wiki/条件分布 "wikilink") *p*(*x*|*y*) ：分布 *p*(*x*|*y*) 和 *p*(*x*) 之间的平均差异越大，[信息增益越大](../Page/相对熵.md "wikilink")。

### 連續互信息的量化

對連續型隨機變數量化的定義如下：

\(f(x_i)\Delta=\int_{i\Delta}^{(i+1)\Delta}f(x)dx=p_i\)

量化後的隨機變數\(X^{\Delta}\):

\(X^{\Delta}=x_i, i \Delta \leq X <(i+1)\Delta\)。

則,

\(I(X^{\Delta};Y^{\Delta})=H(X^{\Delta})-H(X^\Delta|Y^\Delta)\)

\(\approx h(X)-log{\Delta}-(h(X|Y)-log{\Delta})\)

\(=I(X;Y)\)

廣義而言，我們可以將互信息定義在有限多個連續隨機變數[值域](../Page/值域.md "wikilink")的[劃分](../Page/集合划分.md "wikilink")。

令\(\chi\)為連續型隨機變數的值域，\(P_i\in P\), 其中\(P\)為\(\chi\)劃分所構成的集合，意即\(\cup_iP_i=\chi\)。

以\(P\)量化連續型隨機變數\(X\)後，所得結果為離散型隨機變數,

\(Pr([X]_P=i)=\int_{P_i} dF(x)\)。

對於兩連續型隨機變數X、Y，其劃分分別為P、Q，則其互信息可表示為：

\(I(X;Y)=\underset{P,Q}{sup}I([X]_P;[Y]_Q)\)。

## 参见

  -
  -
## 注释

<references/>

## 参考文献

  -
  - Cronbach L. J. (1954). On the non-rational application of information measures in psychology, in [Henry Quastler](https://zh.wikipedia.org/wiki/Henry_Quastler "wikilink"), ed., *Information Theory in Psychology: Problems and Methods*, Free Press, Glencoe, Illinois, pp. 14–30.

  -
  -
  -
  - Lockhead G. R. (1970). Identification and the form of multidimensional discrimination space, *Journal of Experimental Psychology* **85**(1), 1–10.

  - David J. C. MacKay. *[Information Theory, Inference, and Learning Algorithms](https://web.archive.org/web/20160217105359/http://www.inference.phy.cam.ac.uk/mackay/itila/book.html)* Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1 (available free online)

  - Haghighat, M. B. A., Aghagolzadeh, A., & Seyedarabi, H. (2011). A non-reference image fusion metric based on mutual information of image features. Computers & Electrical Engineering, 37(5), 744-756.

  - [Athanasios Papoulis](https://zh.wikipedia.org/wiki/Athanasios_Papoulis "wikilink"). *Probability, Random Variables, and Stochastic Processes*, second edition. New York: McGraw-Hill, 1984. *(See Chapter 15.)*

  -
  -
  -
  -
[Category:信息论](https://zh.wikipedia.org/wiki/Category:信息论 "wikilink") [Category:信息學熵](https://zh.wikipedia.org/wiki/Category:信息學熵 "wikilink")
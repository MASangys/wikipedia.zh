> 本文内容由[估计理论](https://zh.wikipedia.org/wiki/估计理论)转换而来。


**估计理论**是[统计学](../Page/统计学.md "wikilink")和[信号处理中的一个分支](https://zh.wikipedia.org/wiki/信号处理 "wikilink")，主要是通过测量或经验数据来估计[概率分布](../Page/概率分布.md "wikilink")[参数的数值](https://zh.wikipedia.org/wiki/参数 "wikilink")。这些参数描述了实质情况或实际对象，它们能够回答[估计函数提出的问题](https://zh.wikipedia.org/wiki/估计函数 "wikilink")。

例如，估计投票人总体中，给特定候选人投票的人的比例。这个比例是一个不可观测的参数，因为投票人总体很大；估计值建立在投票者的一个小的随机采样上。

又如，雷达的目的是物体（飞机、船等）的定位。这种定位是通过分析收到的回声（回波）来实现的，定位提出的问题是“飞机在哪里？”为了回答这个问题，必须估计飞机到雷达之间的距离。如果雷达的绝对位置是已知的，那么飞机的绝对位置也是可以确定的。

在估计理论中，通常假定信息隐藏在包含[雜訊的](https://zh.wikipedia.org/wiki/雜訊_\(通訊學\) "wikilink")[信号中](https://zh.wikipedia.org/wiki/信号 "wikilink")。噪声增加了[不确定性](https://zh.wikipedia.org/wiki/不确定性 "wikilink")，如果没有不确定性，那么也就没有必要估计了。

## 使用估计理论的领域

有非常多的领域使用参数估计理论。这些领域包括（当然不局限于以下列出的领域）:

  - 信号处理
      - [X射線斷層成像](https://zh.wikipedia.org/wiki/X射線斷層成像 "wikilink")
      - [脑电图](https://zh.wikipedia.org/wiki/脑电图 "wikilink")
      - [心电图](../Page/心电图.md "wikilink")
      - [核磁共振](../Page/核磁共振成像.md "wikilink")
      - [医学超声波扫描术](https://zh.wikipedia.org/wiki/医学超声波扫描术 "wikilink")
      - [雷达](../Page/雷达.md "wikilink")、[声纳](https://zh.wikipedia.org/wiki/声纳 "wikilink")、[地震學](https://zh.wikipedia.org/wiki/地震學 "wikilink")——物件的定位
      - 噪声[方差](../Page/方差.md "wikilink")
      - 参数化（例如[周期图和](https://zh.wikipedia.org/wiki/周期图 "wikilink")[相关图谱](https://zh.wikipedia.org/wiki/相关图 "wikilink")）分析
      - 非参数化（例如[MUSIC](https://zh.wikipedia.org/wiki/MUSIC "wikilink")、[Root-MUSIC和](https://zh.wikipedia.org/wiki/Root-MUSIC "wikilink")[ESPRIT](https://zh.wikipedia.org/wiki/ESPRIT "wikilink")）谱分析
      - [维纳滤波](../Page/维纳滤波.md "wikilink")
      - [粒子滤波器](https://zh.wikipedia.org/wiki/粒子滤波器 "wikilink")
  - [临床试验](https://zh.wikipedia.org/wiki/临床试验 "wikilink")
  - [民意调查](https://zh.wikipedia.org/wiki/民意调查 "wikilink")
  - [质量控制](../Page/质量控制.md "wikilink")
  - [通讯](https://zh.wikipedia.org/wiki/通讯 "wikilink")
      - [信道参数](https://zh.wikipedia.org/wiki/信道_\(通讯\) "wikilink")
      - DC增益（请看下边的例子）
  - [控制理论](../Page/控制理论.md "wikilink")
      - [卡尔曼滤波](../Page/卡尔曼滤波.md "wikilink")
      - 随时间改变的[执行器](https://zh.wikipedia.org/wiki/执行器 "wikilink")（英文：）
  - [网络入侵侦查系统](https://zh.wikipedia.org/wiki/网络入侵侦查系统 "wikilink")

测量参数包含噪声或者其他不确定性。通过统计[概率](../Page/概率.md "wikilink")，可以求得最优化的解，用来从数据中提取尽可能多的[信息](../Page/信息.md "wikilink")。

## 估计过程

估计理论的全部目的都是获取一个估计函数，最好是一个可以实现的估计函数。估计函数输入测量数据，输出相应参数的估计。

我们通常希望估计函数能最优，一个最优的估计意味着所有的信息都被提取出来了；如果还有信息没有提取出来，那就意味着它不是最优的。

一般来说，求估计函数需要三步：

  - 为了实现一个预测单个或者多个参数的所期望的估计器，首先需要确定系统的模型。这个模型需要将需要建模的过程以及不确定性和和噪声融合到一起，这个模型将描述参数应用领域的物理场景。
  - 在确定模型之后，需要确定估计器的限制条件。这些限制条件可以通过如[Cramér-Rao不等式这样的方法找到](https://zh.wikipedia.org/wiki/Cramér-Rao不等式 "wikilink")。
  - 下一步，需要开发一个估计器或者应用一个已知的对于模型有效的估计器。这个估计器需要根据限制条件进行测试以确定它是否是最优估计器，如果是的话，它就是最好的估计器。
  - 最后，在估计器上运行试验或者仿真以测试性能。

当实现一个估计器之后，实际的数据有可能证明推导出估计器的模型是不正确的，这样的话就需要重复上面的过程重新寻找估计器。不能实现的估计器需要抛弃，然后开始一个新的过程。总的来说，估计器根据实际测量的数据预测物理模型的参数。

## 基础

为了建立一个模型，需要知道几项统计“因素”。为了保证预测在数学上是可以追踪的而不是仅仅基于“内心感受”来说这是必需的。

第一个是从大小为 \(N\) 的[随机矢量中取出的](https://zh.wikipedia.org/wiki/随机矢量 "wikilink")[统计采样](https://zh.wikipedia.org/wiki/统计采样 "wikilink")，将它们放到一个[矢量中](https://zh.wikipedia.org/wiki/矢量 "wikilink")，

  -
    \(\mathbf{x} = \begin{bmatrix} x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}\).

第二，有相应的 \(M\) 参数

  -
    \(\mathbf{\theta} = \begin{bmatrix} \theta_1 \\ \theta_2 \\ \vdots \\ \theta_M \end{bmatrix}\),

它需要根据[概率密度函数](https://zh.wikipedia.org/wiki/概率密度函数 "wikilink")（pdf）或者[概率聚集函数](https://zh.wikipedia.org/wiki/概率聚集函数 "wikilink")（:en:[probability mass function](https://zh.wikipedia.org/wiki/probability_mass_function "wikilink")）（pmf）建立

  -
    \(p(\mathbf{x} | \mathbf{\theta})\).

参数本身还可能有一个概率分布（[Bayesian statistics](https://zh.wikipedia.org/wiki/Bayesian_statistics "wikilink")），需要定义[epistemic probability](https://zh.wikipedia.org/wiki/epistemic_probability "wikilink")

  -
    \(\pi( \mathbf{\theta})\).

模型形成之后的目标就是预测参数，通常表示为 \(\hat{\mathbf{\theta}}\)，其中“hat”表示预测值。

一个普通的估计器是[最小均方误差](https://zh.wikipedia.org/wiki/最小均方误差 "wikilink")（MMSE）估计器，它利用了参数估计值与实际值之间的误差

  -
    \(\mathbf{e} = \hat{\mathbf{\theta}} - \mathbf{\theta}\)

作为优化的基础。在最小均方误差估计器中误差进行取平方、最小化。

## 估计函数（估计子）

以下是一些相关的[估计函数以及相关的主题](https://zh.wikipedia.org/wiki/估计函数 "wikilink")

  - [最大似然估計](https://zh.wikipedia.org/wiki/最大似然估計 "wikilink")（Maximum likelihood estimation，簡稱MLE）
  - [矩估計](https://zh.wikipedia.org/wiki/矩估計 "wikilink")（Method of moments estimators，簡稱MME）
  - [Cramér-Rao不等式](https://zh.wikipedia.org/wiki/Cramér-Rao不等式 "wikilink")
  - [最小均方差](https://zh.wikipedia.org/wiki/最小均方差 "wikilink")（Minimum mean squared error，简称MMSE）
  - [最大后验概率](../Page/最大后验概率.md "wikilink")（Maximum a posteriori probability，簡稱MAP）
  - [最小方差非偏估计](https://zh.wikipedia.org/wiki/最小方差非偏估计 "wikilink")（Minimum variance unbiased estimator，简称MVUE）
  - [最佳线性非偏估计](https://zh.wikipedia.org/wiki/最佳线性非偏估计 "wikilink")（BLUE）
  - 非偏估计，见[偏差](https://zh.wikipedia.org/wiki/偏差 "wikilink") ([统计学](../Page/统计学.md "wikilink"))。
  - Particle filter
  - Markov chain Monte Carlo，简称MCMC
  - [卡尔曼滤波](../Page/卡尔曼滤波.md "wikilink")
  - [维纳滤波](../Page/维纳滤波.md "wikilink")

## 例子：高斯白噪声中的直流增益

让我们来看一个接收到的\(N\)个[独立](https://zh.wikipedia.org/wiki/statistical_independence "wikilink")[采样点的](https://zh.wikipedia.org/wiki/采样点 "wikilink")[离散信号](https://zh.wikipedia.org/wiki/离散信号 "wikilink")\(x[n]\)，它由一个[直流增益](https://zh.wikipedia.org/wiki/直流 "wikilink")\(A\)和*已知*[方差](../Page/方差.md "wikilink")为\(\sigma^2\) (例如，\(\mathcal{N}(0, \sigma^2)\)）的[叠加白噪声](https://zh.wikipedia.org/wiki/叠加白噪声 "wikilink")\(w[n]\)组成。

由于方差已经知道，所以仅有的未知参数就是\(A\)。

于是信号的模型是

  -
    \(x[n] = A + w[n] \quad n=0, 1, \dots, N-1\)

两个可能的估计器是：

  - \(\hat{A}_1 = x[0]\)
  - \(\hat{A}_2 = \frac{1}{N} \sum_{n=0}^{N-1} x[n]\)它是[采样平均](https://zh.wikipedia.org/wiki/采样平均 "wikilink")

这两个估计器都有一个[平均值](https://zh.wikipedia.org/wiki/平均值 "wikilink")\(A\)，这可以通过代入每个估计器的[期望得到](https://zh.wikipedia.org/wiki/期望 "wikilink")

\[\mathrm{E}\left[\hat{A}_1\right] = \mathrm{E}\left[ x[0] \right] = A\] 和

\[\mathrm{E}\left[ \hat{A}_2 \right]
=
\mathrm{E}\left[ \frac{1}{N} \sum_{n=0}^{N-1} x[n] \right]
=
\frac{1}{N} \left[ \sum_{n=0}^{N-1} \mathrm{E}\left[ x[n] \right] \right]
=
\frac{1}{N} \left[ N A \right]
=
A\]

在这一点上，这两个估计器看起来是一样的。但是，当比较方差部分的时候它们之间的不同就很明显了。

\[\mathrm{var} \left( \hat{A}_1 \right) = \mathrm{var} \left( x[0] \right) = \sigma^2\] 和

\[\mathrm{var} \left( \hat{A}_2 \right)
=
\mathrm{var} \left( \frac{1}{N} \sum_{n=0}^{N-1} x[n] \right)
=
\frac{1}{N^2} \left[ \sum_{n=0}^{N-1} \mathrm{var}(x[n]) \right]
=
\frac{1}{N^2} \left[ N \sigma^2 \right]
=
\frac{\sigma^2}{N}\]

看起来采样平均是一个更好的估计器，因为方差部分\(N \to \infty\)趋向于0。

### 最大似然估计

使用[最大似然估计](../Page/最大似然估计.md "wikilink")继续上面的例子，噪声在一个采样点\(w[n]\)的[概率密度函数](https://zh.wikipedia.org/wiki/概率密度函数 "wikilink")（pdf）是

\[p(w[n]) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(- \frac{1}{2 \sigma^2} w[n]^2 \right)\]

这样\(x[n]\)的概率变为（\(x[n]\)可以认为是\(\mathcal{N}(A, \sigma^2)\)）

\[p(x[n]; A) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(- \frac{1}{2 \sigma^2}(x[n] - A)^2 \right)\]

由于相互独立，\(\mathbf{x}\)的概率变为

\[p(\mathbf{x}; A)
=
\prod_{n=0}^{N-1} p(x[n]; A)
=
\frac{1}{\left(\sigma \sqrt{2\pi}\right)^N}
\exp\left(- \frac{1}{2 \sigma^2} \sum_{n=0}^{N-1}(x[n] - A)^2 \right)\]

概率密度函数取[自然对数](https://zh.wikipedia.org/wiki/自然对数 "wikilink")

\[\ln p(\mathbf{x}; A)
=
-N \ln \left(\sigma \sqrt{2\pi}\right)
- \frac{1}{2 \sigma^2} \sum_{n=0}^{N-1}(x[n] - A)^2\]

于是最大似然估计器是

\[\hat{A} = \arg \max \ln p(\mathbf{x}; A)\]

对数最大似然函数取一阶[导数](../Page/导数.md "wikilink")

\[\frac{\partial}{\partial A} \ln p(\mathbf{x}; A)
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}(x[n] - A) \right]
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}x[n] - N A \right]\]

并且将它赋值为0

\[0
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}x[n] - N A \right]
=
\sum_{n=0}^{N-1}x[n] - N A\]

这就得到最大似然估计器

\[\hat{A} = \frac{1}{N} \sum_{n=0}^{N-1}x[n]\]

它是一个简单的采样平均。

从这个例子中，我们发现对于带有固定未知直流增益的AWGN的\(N\)个采样点来说采样平均就是最大似然估计器。

### Cramér-Rao下限

为了找到采样平均估计器的[Cramér-Rao下限](https://zh.wikipedia.org/wiki/Cramér-Rao下限 "wikilink")（CRLB），需要找到Fisher information数

\[\mathcal{I}(A)
=
\mathrm{E}
\left(
 \left[
  \frac{\partial}{\partial\theta} \ln p(\mathbf{x}; A)
 \right]^2
\right)
=
-\mathrm{E}
\left[
 \frac{\partial^2}{\partial\theta^2} \ln p(\mathbf{x}; A)
\right]\]

从上面得到

\[\frac{\partial}{\partial A} \ln p(\mathbf{x}; A)
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}x[n] - N A \right]\]

取二阶导数

\[\frac{\partial^2}{\partial A^2} \ln p(\mathbf{x}; A)
=
\frac{1}{\sigma^2}(- N)
=
\frac{-N}{\sigma^2}\]

发现负的期望值是无关紧要的（），因为它现在是一个确定的常数

\(-\mathrm{E}
\left[
 \frac{\partial^2}{\partial A^2} \ln p(\mathbf{x}; A)
\right]
=
\frac{N}{\sigma^2}\)

最后，将Fisher information代入

\[\mathrm{var}\left( \hat{A} \right)
\geq
\frac{1}{\mathcal{I}}\]

得到

\[\mathrm{var}\left( \hat{A} \right)
\geq
\frac{\sigma^2}{N}\]

将这个值与前面确定的采样平均的变化比较显示对于所有的\(N\)和\(A\)来说采样平均都是*等于*Cramér-Rao下限。

采样平均除了是[最大似然估计器之外还是](https://zh.wikipedia.org/wiki/最大似然 "wikilink")[最小变化无偏估计器](https://zh.wikipedia.org/wiki/最小变化无偏估计器 "wikilink")（MVUE）。

这个直流增益 + WGN的例子是Kay的*统计信号处理基础*中一个例子的再现。

## 相关书籍

  - *Fundamentals of Statistical Signal Processing: Estimation Theory* by Steven M. Kay (ISBN 0-13-345711-7)
  - *An Introduction to Signal Detection and Estimation* by H. Vincent Poor (ISBN 0-38-794173-8)
  - *Detection, Estimation, and Modulation Theory, Part 1* by Harry L. Van Trees (ISBN 0-47-109517-6; [website](https://web.archive.org/web/20050428233957/http://gunston.gmu.edu/demt/demtp1/))

## 参见

  - [偏差](https://zh.wikipedia.org/wiki/偏差 "wikilink")
  - [检测理论](https://zh.wikipedia.org/wiki/检测理论 "wikilink")
  - [信息论](../Page/信息论.md "wikilink")
  - [最大似然估计](../Page/最大似然估计.md "wikilink")
  - [矩方法](https://zh.wikipedia.org/wiki/矩方法 "wikilink")
  - [最小均方差](https://zh.wikipedia.org/wiki/最小均方差 "wikilink")（MMSE）
  - [最大后验概率](../Page/最大后验概率.md "wikilink")（MAP）
  - [卡尔曼滤波](../Page/卡尔曼滤波.md "wikilink")
  - [维纳滤波](../Page/维纳滤波.md "wikilink")
  - [最小平方頻譜分析法](../Page/最小平方頻譜分析法.md "wikilink")（LSSA）

[Category:估计理论](https://zh.wikipedia.org/wiki/Category:估计理论 "wikilink") [Category:信号处理](https://zh.wikipedia.org/wiki/Category:信号处理 "wikilink") [Category:统计学](https://zh.wikipedia.org/wiki/Category:统计学 "wikilink")
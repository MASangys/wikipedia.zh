在[信息论中](../Page/信息论.md "wikilink")，**信息冗余**是传输消息所用数据位的数目与消息中所包含的实际信息的数据位的数目的差值。[数据压缩是一种用来消除不需要的冗余的方法](../Page/数据压缩.md "wikilink")，[校验和是在经过有限](../Page/校验和.md "wikilink")[信道容量的噪声信道中通信](../Page/信道容量.md "wikilink")，为了进行[错误校正而增加冗余的方法](../Page/错误校正.md "wikilink")。

## 定量定义

在描述原始数据的冗余时，信源[信息率为平均每个符号的](../Page/熵率.md "wikilink")[熵](../Page/熵_\(信息论\).md "wikilink")。对于无记忆信源，这仅是每个符号的熵；而对于一个[随机过程的最普遍形式为前](../Page/随机过程.md "wikilink")
*n* 个符号的[联合熵除以](../Page/联合熵.md "wikilink") *n* 之后，随着 *n* 趋于无穷时的极限

\[r = \lim_{n \to \infty} \frac{1}{n} \Eta(M_1, M_2, \dots M_n),\]

在信息论中经常提及一种语言的“熵率”或者“[信息熵](../Page/信息熵.md "wikilink")”。当信源是英文散文时这是正确的。由于无记忆信源的消息之间没有相互依赖性，所以无记忆信源的信息率为\(\Eta(M)\)。

信源的**绝对信息率**为

\[R = \log |\mathbb M| ,\,\]

即是消息空间[基数的](../Page/基数.md "wikilink")[对数值](../Page/对数.md "wikilink")。这个公式也称作[Hartley函数](../Page/Hartley函数.md "wikilink")。这是传送用这个字母表表示的信息的最大信息率。其中对数要根据所用的测量单位选择合适的[底数](../Page/底数.md "wikilink")。[当且仅当信源是无记忆的且均匀分布的时候](../Page/当且仅当.md "wikilink")，绝对信息率等于信息率。

**绝对信息冗余**定义为

\[D = R - r\]，

即信息率与绝对信息率之间的差。

\(\frac D R\)称为**相对信息冗余**，它表示了最大的[数据压缩率](../Page/数据压缩率.md "wikilink")，这个压缩率用文件大小减小比例所表示。当用原始文件与压缩后的文件表示的时候，\(R : r\)表示能够得到的最大压缩率。与相对信息冗余互补的是**效率**\(\frac r R\)，于是\(\frac r R + \frac D R = 1\)。均匀分布的无记忆信源的冗余为0，效率为100%，因此无法压缩。

## 其它的冗余概念

两个变量之间*冗余*的度量是[互信息或者正规化变量](../Page/互信息.md "wikilink")。多个变量之间冗余的度量是[全相关](../Page/全相关.md "wikilink")（total
correlation）。

压缩数据的冗余是指 \(n\)
个消息的[期望压缩数据长度为](../Page/期望值.md "wikilink")\(L(M^n) \,\!\)（或期望数据熵率
\(L(M^n)/n \,\!\)）与熵值 \(nr \,\!\)（或熵率
\(r \,\!\)）的差。（这里我们假设数据是[遍历的也是](../Page/遍历性.md "wikilink")[平稳的](../Page/平稳过程.md "wikilink")，例如无记忆信源。）虽然熵率之差
\(L(M^n)/n-r \,\!\) 会随着 \(n \,\!\) 增加而任意小，实际的差 \(L(M^n)-nr \,\!\)
已不能（尽管理论上可以）在有限熵的无记忆信源情况下上界为 1。

## 参见

  - [信源编码](../Page/信源编码.md "wikilink")
  - [信源编码定理](../Page/信源编码定理.md "wikilink")
  - [数据压缩](../Page/数据压缩.md "wikilink")
  - [负熵](../Page/负熵.md "wikilink")

## 参考文献

  -
  -
  -
[Category:信息论](https://zh.wikipedia.org/wiki/Category:信息论 "wikilink")
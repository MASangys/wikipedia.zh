> 本文内容由[信息论](https://zh.wikipedia.org/wiki/信息论)转换而来。


^{}p(x)\\log_2(\\frac{1}{p(x)})</math>

其中\(\mathcal{X}\)為有限个事件x的集合，\(X\)是定义在\(\mathcal{X}\)上的随机变量。信息熵是随机事件不确定性的度量。

信息熵与物理学中的[热力学熵有着紧密的联系](https://zh.wikipedia.org/wiki/热力学熵 "wikilink"):

\[S(X)=k_BH(X)\]

其中S(X)為熱力學熵，H(X)為信息熵，\(k_B\)為[波茲曼常數](../Page/波茲曼常數.md "wikilink")。 事實上這個關係也就是廣義的[波茲曼熵公式](https://zh.wikipedia.org/wiki/波茲曼熵公式 "wikilink")，或是在[正則系綜內的熱力學熵表示式](https://zh.wikipedia.org/wiki/正則系綜 "wikilink")。如此可知，[玻尔兹曼与](https://zh.wikipedia.org/wiki/玻尔兹曼 "wikilink")[吉布斯在统计物理学中对熵的工作](https://zh.wikipedia.org/wiki/吉布斯 "wikilink")，啟發了信息論的熵。

信息熵是[信源編碼定理中](https://zh.wikipedia.org/wiki/信源編碼定理 "wikilink")，壓縮率的下限。當我們用少於信息熵的資訊量做編碼，那麼我們一定有資訊的損失。夏農在[大數定律和](https://zh.wikipedia.org/wiki/大數定律 "wikilink")的基础上定義了和典型-{序列}-。典型集是典型-{序列}-的集合。因為一个独立同分布的\(X\)-{序列}-属于由\(X\)定义的典型集的機率大約為1，所以只需要将屬於典型集的无记忆\(X\)信源-{序列}-编为唯一可译碼，其他-{序列}-隨意編碼，就可以達到幾乎無損失的壓縮。

#### 例子

若S為一個三個面的骰子,

P(面一)=1/5,

P(面二)=2/5,

P(面三)=2/5

\(H(X)=\frac{1}{5}\log_2 (5)+\frac{2}{5}\log_2\left(\frac{5}{2}\right)+\frac{2}{5}\log_2\left(\frac{5}{2}\right)\)

### 聯合熵與條件熵

[聯合熵](https://zh.wikipedia.org/wiki/聯合熵 "wikilink")（）由熵的定義出發，由[聯合分布](https://zh.wikipedia.org/wiki/聯合分布 "wikilink")，我們有:

\[H(X,Y)=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}^{}p(x,y)\log(\frac{1}{p(x,y)})\]

[條件熵](https://zh.wikipedia.org/wiki/條件熵 "wikilink")（），顧名思義，從條件機率p(y|x)做定義:

\[H(Y|X)=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}^{}p(x,y)\log(\frac{1}{p(y|x)})\]

因為由[貝氏定理](https://zh.wikipedia.org/wiki/貝氏定理 "wikilink")，我們有\(p(x,y)=p(y|x)p(x)\)，帶入聯合熵的定義，可以分離出條件熵，於是得到聯合熵與條件熵的關係式:

\[H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)=H(Y,X)\]

#### 链式法則

我們可以再對聯合熵與條件熵的關係做推廣，假設現在有n個隨機變量\(X_i, i=1,2,...,n\)，重複分離出條件熵，我們有:

\[\begin{align} H(X_1,X_2,...,X_n)&=H(X_1)+H(X_2,...,X_n|X_1)=H(X_1)+H(X_2|X_1)+H(X_3,...,X_n|X_1,X_2)\\
&=H(X_1)+\sum_{i=2}^{n}H(X_i|X_1,...,X_{i-1})\end{align}\]

他的意義顯而易見，假如我們接收一段數列\(\{X_1,X_2,...,X_n\}\)，且先收到\(X_1\)，再來是\(X_2\)，依此類推。那麼收到\(X_1\)後總訊息量為\(H(X_1)\)，收到\(X_2\)後總訊息量為\(H(X_1)+H(X_2|X_1)\)，直到收到\(X_n\)後我們的總訊息量應為\(H(X_1,...,X_n)\)，於是這個接收過程中就給出了链式法則。

### 互信息

[互信息](../Page/互信息.md "wikilink")（）是另一有用的信息度量，它是指两个事件集合之间的相关性。两个事件\(X\)和\(Y\)的互信息定义为：

\[I(X;Y) = H(X)-H(X|Y)=H(X) + H(Y) - H(X, Y)=H(Y)-H(Y|X)=I(Y;X)\]

其意義為，若我們想知道\(Y\)包含多少\(X\)的資訊，在尚未得到\(Y\)之前，我們的不確定性是\(H(X)\)，得到Y後，不確定性是\(H(X|Y)\)。所以一旦得到\(Y\)後，我們消除了\(H(X)-H(X|Y)\)的不確定量，這就是Y對X的資訊量。

如果\(X,Y\)互為獨立，則\(H(X,Y)=H(X)+H(Y)\)，於是\(I(X;Y)=0\)。

又因為\(H(X|Y)\leq H(X)\)，所以

\[I(X;Y)\leq \min(H(X),H(Y))\]，其中等號成立條件為Y=g(X)，g是一個[-{zh:雙射;zh-hans:双射;zh-hant:對射}-函數](../Page/双射.md "wikilink")

互信息与以及[皮尔森卡方-{A有着密切的联系](../Page/皮爾森卡方檢定.md "wikilink")。

## 应用

信息论被广泛应用在：

  - [編碼理論](https://zh.wikipedia.org/wiki/編碼理論 "wikilink")
  - [密码学](../Page/密码学.md "wikilink")
  - [数据传输](../Page/数据传输.md "wikilink")
  - [数据压缩](../Page/数据压缩.md "wikilink")
  - [检测理论](https://zh.wikipedia.org/wiki/检测理论 "wikilink")
  - [估计理论](../Page/估计理论.md "wikilink")
  - [数据加密](../Page/加密.md "wikilink")

## 参考文献

## 外部链接

  - [香农论文：通信的数学理论](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6773024)

{{-}}

[Category:通信](https://zh.wikipedia.org/wiki/Category:通信 "wikilink") [Category:控制论](https://zh.wikipedia.org/wiki/Category:控制论 "wikilink") [Category:信息论](https://zh.wikipedia.org/wiki/Category:信息论 "wikilink") [Category:形式科學](https://zh.wikipedia.org/wiki/Category:形式科學 "wikilink") [Category:信息时代](https://zh.wikipedia.org/wiki/Category:信息时代 "wikilink")
> 本文内容由[信源编码定理](https://zh.wikipedia.org/wiki/信源编码定理)转换而来。


在[信息论](../Page/信息论.md "wikilink")中，香农的**信源编码定理**（或**无噪声编码定理**）确立了[数据压缩](../Page/数据压缩.md "wikilink")的限度，以及[香农熵的操作意义](https://zh.wikipedia.org/wiki/香农熵 "wikilink")。

**信源编码定理**表明（在极限情况下，随着[独立同分布随机变量数据流的长度趋于无穷](https://zh.wikipedia.org/wiki/独立同分布随机变量 "wikilink")）不可能把数据压缩得码率（每个符号的比特的平均数）比信源的香农熵还小，又不丢失信息。但是有可能使码率任意接近香农熵，且损失的概率极小。

**码符号的信源编码定理**把码字的最小可能期望长度看作输入字（看作[随机变量](../Page/随机变量.md "wikilink")）的[熵和目标编码表的大小的一个函数](../Page/熵_\(信息论\).md "wikilink")，给出了此函数的上界和下界。

## 陈述

信源编码是从信息源的符号（序列）到码符号集（通常是bit）的映射，使得信源符号可以从二进制位元（无损信源编码）或有一些失真（有损信源编码）中准确恢复。这是在[数据压缩](../Page/数据压缩.md "wikilink")的概念。

### 信源编码定理

在信息论中，**信源编码定理**\[1\]非正式地陈述\[2\]\[3\]为：

> 个[熵均为](../Page/熵_\(信息论\).md "wikilink")  的[独立同分布的随机变量在](https://zh.wikipedia.org/wiki/独立同分布的随机变量 "wikilink")  时，可以很小的信息损失风险压缩成多于  [bit](../Page/位元.md "wikilink")；但相反地，若压缩到少于  bit，则信息几乎一定会丢失。

### 码符号的信源编码定理

令  表示两个有限编码表，并令  和  （分别）表示来自那些编码表的[所有有限字的集合](../Page/克莱尼星号.md "wikilink")。

设  为从  取值的随机变量，令  为从  到  的唯一可译码，其中 Σ<sub>2</sub>{{\!}} {{=}} *a*}}。令  表示字长  给出的随机变量。

如果  是对  拥有最小期望字长的最佳码，那么(Shannon 1948)：

\[\frac{H(X)}{\log_2 a} \leq \mathbb{E}S < \frac{H(X)}{\log_2 a} +1\]

## 证明：码符号的信源编码定理

对于  令  表示每个可能的  的字长。定义 \(q_i = a^{-s_i}/C\)，其中  会使得  1}}。于是

\[\begin{align}
H(X) &=    -\sum_{i=1}^n p_i \log_2 p_i \\
     &\leq -\sum_{i=1}^n p_i \log_2 q_i \\
     &=    -\sum_{i=1}^n p_i \log_2 a^{-s_i} + \sum_{i=1}^n p_i \log_2 C \\
     &=    -\sum_{i=1}^n p_i \log_2 a^{-s_i} + \log_2 C \\
     &\leq -\sum_{i=1}^n - s_i p_i \log_2 a \\
     &\leq  \mathbb{E} S \log_2 a \\
\end{align}\]

其中第二行由[吉布斯不等式](../Page/吉布斯不等式.md "wikilink")推出，而第五行由[克拉夫特不等式推出](https://zh.wikipedia.org/wiki/克拉夫特不等式 "wikilink")：

\[C = \sum_{i=1}^n a^{-s_i} \leq 1\]

因此 .

对第二个不等式我们可以令

\[s_i = \lceil - \log_a p_i \rceil\]

于是

\[- \log_a p_i \leq s_i < -\log_a p_i + 1\]

因此

\[a^{-s_i} \leq p_i\]

并且

\[\sum  a^{-s_i} \leq \sum p_i = 1\]

因此由克拉夫特不等式，存在一种有这些字长的无前缀编码。因此最小的  满足

\[\begin{align}
\mathbb{E} S & = \sum p_i s_i \\
& < \sum p_i \left( -\log_a p_i +1 \right) \\
& = \sum - p_i \frac{\log_2 p_i}{\log_2 a} +1 \\
& = \frac{H(X)}{\log_2 a} +1 \\
\end{align}\]

## 参考资料

[Category:信息论](https://zh.wikipedia.org/wiki/Category:信息论 "wikilink")

1.
2.
3.
> 本文内容由[偏最小二乘回归](https://zh.wikipedia.org/wiki/偏最小二乘回归)转换而来。


**偏最小二乘回归（， PLS回归）**是一种[统计学](../Page/统计学.md "wikilink")方法，与[主成分回归有关系](https://zh.wikipedia.org/wiki/主成分回归 "wikilink")，但不是寻找响应和独立变量之间最小[方差](../Page/方差.md "wikilink")的[超平面](https://zh.wikipedia.org/wiki/超平面 "wikilink")，而是通过投影[预测变量和](https://zh.wikipedia.org/wiki/预测变量 "wikilink")[观测变量到一个新空间来寻找一个](https://zh.wikipedia.org/wiki/观测变量 "wikilink")[线性回归模型](https://zh.wikipedia.org/wiki/线性回归 "wikilink")。因为数据*X*和*Y*都会投影到新空间，PLS系列的方法都被称为双线性因子模型。當Y是分类數據時有「偏最小二乘判别分析（， PLS-DA）」，是PLS的一个变形。

偏最小二乘用于查找两个[矩阵](../Page/矩阵.md "wikilink")（*X*和*Y*）的基本关系，即一个在这两个空间对[协方差](../Page/协方差.md "wikilink")结构建模的[隐变量方法](https://zh.wikipedia.org/wiki/隐变量 "wikilink")。偏最小二乘模型将试图找到*X*空间的多维方向来解释*Y*空间方差最大的多维方向。偏最小二乘回归特别适合当预测矩阵比观测的有更多变量，以及*X*的值中有[多重共线性的时候](https://zh.wikipedia.org/wiki/多重共线性 "wikilink")。相比之下，标准的回归在这些情况下不见效（除非它是[吉洪诺夫正则化](https://zh.wikipedia.org/wiki/吉洪诺夫正则化 "wikilink")）。

偏最小二乘算法被用在[偏最小二乘路径建模中](https://zh.wikipedia.org/wiki/偏最小二乘路径建模 "wikilink")，\[1\]\[2\] 一个建立[隐变量](https://zh.wikipedia.org/wiki/隐变量 "wikilink")（原因不能没有实验和拟实验来确定，但一个典型的模型会基于之前理论假设（隐变量影响衡量指标的表现）的隐变量模型）这种技术是[结构方程模型的一种形式](https://zh.wikipedia.org/wiki/结构方程模型 "wikilink")，与经典方法不同的是基于组件而不是基于协方差。\[3\]

偏最小二乘来源于瑞典统计学家[Herman Wold](https://zh.wikipedia.org/wiki/Herman_Wold "wikilink")，然后由他的儿子Svante Wold发展。偏最小二乘的另一个词（根据Svante Wold\[4\]）是*投影到潜在结构*，但偏最小二乘法依然在许多领域占据着主导地位。尽管最初的应用是在社会科学中，偏最小二乘回归今天被广泛用于[化学计量学](../Page/化学计量学.md "wikilink")和相关领域。它也被用于生物信息学，sensometrics，神经科学和人类学。而相比之下，偏最小二乘回歸最常用于社会科学、计量经济学、市场营销和战略管理。

## 底层模型

偏最小二乘的一般多元底层模型是

\[X = T P^{\top} + E\]

\[Y = U Q^{\top} + F\]

其中\(X\)是一个\(n \times m\)的预测矩阵，\(Y\)是一个\(n \times p\)的响应矩阵；\(T\)和\(U\)是\(n \times l\)的矩阵，分别为\(X\)的投影（“X分数”、“组件”或“因子”矩阵）和\(Y\)的投影（“Y分数”）；\(P\)和\(Q\)分别是\(m \times l\)和\(p \times l\)的正交*载荷*矩阵，以及矩阵\(E\)和\(F\)是错误项，假设是独立同分布的随机正态变量。对\(X\)和\(Y\)分解来最大化\(T\)和\(U\)之间的[协方差](../Page/协方差.md "wikilink")。

## 算法

偏最小二乘的许多变量是为了估计因子和载荷矩阵\(T, U, P\)和\(Q\)。它们中大多数构造了\(X\)和\(Y\)之间线性回归的估计\(Y = X \tilde{B} + \tilde{B}_0\)。一些偏最小二乘算法只适合\(Y\)是一个列向量的情况，而其它的算法则处理了\(Y\)是一个矩阵的一般情况。算法也根据他们是否估计因子矩阵\(T\)为一个[正交矩阵](../Page/正交矩阵.md "wikilink")而不同。\[5\]\[6\]\[7\]\[8\]\[9\]\[10\] 最后的预测在所有不同最小二乘算法中都是一样的，但组件是不同的。

### PLS1

PLS1是一个\(Y\)是向量时广泛使用的算法。它估计\(T\)是一个正交矩阵。以下是伪代码（大写字母是矩阵，带上标的小写字母是向量，带下标的小写字母和单独的小写字母都是标量）：

` 1  `**`function`**` PLS1(`\(X, y, l\)`)`
` 2  `\(X^{(0)} \gets X\)
` 3  `\(w^{(0)} \gets X^T y/||X^Ty||\)`, an initial estimate of `\(w\)`.`
` 4  `\(t^{(0)} \gets X w^{(0)}\)` `
` 5  `**`for`**` `\(k\)` = 0 `**`to`**` `\(l\)
` 6      `\(t_k \gets {t^{(k)}}^T t^{(k)}\)` (note this is a scalar)`
` 7      `\(t^{(k)} \gets t^{(k)} / t_k\)
` 8      `\(p^{(k)} \gets {X^{(k)}}^T t^{(k)}\)
` 9      `\(q_k \gets {y}^T t^{(k)}\)` (note this is a scalar)`
`10      `**`if`**` `\(q_k\)` = 0`
`11          `\(l \gets k\)`, `**`break`**` the `**`for``   ``loop`**
`12      `**`if`**` `\(k < l\)
`13          `\(X^{(k+1)} \gets X^{(k)} - t_k t^{(k)} {p^{(k)}}^T\)
`14          `\(w^{(k+1)} \gets {X^{(k+1)}}^T y\)
`15          `\(t^{(k+1)} \gets X^{(k+1)}w^{(k+1)}\)
`16  end `**`for`**
`17  `**`define`**` `\(W\)` to be the matrix with columns `\(w^{(0)},w^{(1)},...,w^{(l-1)}\)`.`
`    Do the same to form the `\(P\)` matrix and `\(q\)` vector.`
`18  `\(B \gets W {(P^T W)}^{-1} q\)
`19  `\(B_0 \gets q_0 - {P^{(0)}}^T B\)
`20  `**`return`**` `\(B, B_0\)

这种形式的算法不需要输入\(X\)和\(Y\)定中心，因为算法隐式处理了。这个算法的特点是收缩于\(X\) （减去\(t_k t^{(k)} {p^{(k)}}^T\)），但向量\(y\)不收缩，因为没有必要（可以证明收缩\(y\)和不收缩的结果是一样的）。用户提供的变量\(l\)是回归中隐藏因子数量的限制；如果它等于矩阵\(X\)的秩，算法将产生\(B\)和\(B_0\)的最小二乘回归估计。

## 扩展

2002年，一个叫做正交投影（， OPLS）的方法提出。在OPLS中，连续变量数据被分为预测的和不相关的信息。这有利于改进诊断，以及更容易解释可视化。然而，这些变化只是改善模型的可解释性，不是生产力。\[11\] L-PLS通过3个连接数据块扩展了偏最小二乘回归。\[12\] 同样，OPLS-DA（， 判别分析）可能被应用在处理离散变量，如分类和生物标志物的研究。

## 软件实现

大多数统计软件包都提供偏最小二乘回归。 R中的‘pls’包提供了一系列算法。\[13\]

## 参见

  - [特征提取](https://zh.wikipedia.org/wiki/特征提取 "wikilink")
  - [数据挖掘](../Page/数据挖掘.md "wikilink")
  - [机器学习](../Page/机器学习.md "wikilink")
  - [回归分析](https://zh.wikipedia.org/wiki/回归分析 "wikilink")
  - [典型相关](../Page/典型相关.md "wikilink")
  - [Deming regression](https://zh.wikipedia.org/wiki/Deming_regression "wikilink")
  - [多线性子空间学习](https://zh.wikipedia.org/wiki/多线性子空间学习 "wikilink")
  - [主成分分析](../Page/主成分分析.md "wikilink")
  - [总平方和](https://zh.wikipedia.org/wiki/总平方和 "wikilink")

## 扩展阅读

  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  - Wan Mohamad Asyraf Bin Wan Afthanorhan. (2013). A Comparison Of Partial Least Square Structural Equation Modeling (PLS-SEM) and Covariance Based Structural EquationModeling (CB-SEM) for Confirmatory Factor Analysis International Journal of Engineering Science and Innovative Technology (IJESIT), 2(5), 9.

## 参考文献

## 外部链接

  - [imDEV](https://sourceforge.net/projects/imdev/) free Excel add-in for PLS and PLS-DA
  - [PLS in Brain Imaging](http://www.rotman-baycrest.on.ca/pls)
  - [on-line PLS](http://www.vcclab.org/lab/pls) regression (PLSR) at Virtual Computational Chemistry Laboratory
  - [Uncertainty estimation for PLS](http://www.chemometry.com/Research/MVC.html)
  - [A short introduction to PLS regression and its history](http://www.utd.edu/~herve/Abdi-PLSR2007-pretty.pdf)

[Category:回归分析](https://zh.wikipedia.org/wiki/Category:回归分析 "wikilink") [Category:隐变量模型](https://zh.wikipedia.org/wiki/Category:隐变量模型 "wikilink") [Category:最小二乘](https://zh.wikipedia.org/wiki/Category:最小二乘 "wikilink")

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
**共递归**在计算机科学重视一类操作，与[递归在范畴论上](../Page/递归_\(计算机科学\).md "wikilink")[对偶](../Page/对偶.md "wikilink")。因而递归是分析地工作，把数据分解为更小的数据直至达到基本情况。共递归是合成地工作，从基本情况构造出数据。共递归的数据是自己一点一点构造出来的。一个类似但不同的概念是*[生成式递归](https://zh.wikipedia.org/wiki/生成式递归 "wikilink")*（generative recursion）。

共递归常与[惰性求值](../Page/惰性求值.md "wikilink")配合，产生一个潜在无穷结构的有限子集。

## 例子

Corecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the [generator](https://zh.wikipedia.org/wiki/Generator_\(computer_programming\) "wikilink") facility in Python. In these examples local variables are used, and [assigned values](https://zh.wikipedia.org/wiki/Assignment_\(computer_science\) "wikilink") imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.

### Factorial

A classic example of recursion is computing the factorial, which is defined recursively as \(0! := 1\) and \(n! := n \times (n-1)!\)

To *recursively* compute its result on a given input, a recursive function calls (a copy of) *itself* with a different ("smaller" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the *base case* has been reached. Thus a [call stack](https://zh.wikipedia.org/wiki/call_stack "wikilink") develops in the process. For example, to compute *fac(3)*, this recursively calls in turn *fac(2)*, *fac(1)*, *fac(0)* ("winding up" the stack), at which point recursion terminates with *fac(0) = 1*, and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame *fac(3)*, where the final result is calculated as *3\*2 =: 6* and finally returned. In this example a function returns a single value.

This stack unwinding can be explicated, defining the factorial *corecursively*, as an [iterator](https://zh.wikipedia.org/wiki/Iteration "wikilink"), where one *starts* with the case of \(1 =: 0!\), then from this starting value constructs factorial values for increasing numbers *1, 2, 3...* as in the above recursive definition with "time arrow" reversed, as it were, by reading it *backwards* as  The corecursive algorithm thus defined produces a *stream* of *all* factorials. This may be concretely implemented as a [generator](https://zh.wikipedia.org/wiki/Generator_\(computer_programming\) "wikilink"). Symbolically, noting that computing next factorial value requires keeping track of both *n* and *f* (a previous factorial value), this can be represented as:

\[n, f = (0, 1) : (n + 1, f \times (n+1))\] or in [Haskell](https://zh.wikipedia.org/wiki/Haskell_\(programming_language\) "wikilink"),

``` haskell
  (\(n,f) -> (n+1, f*(n+1))) `iterate` (0,1)
```

meaning, "starting from \(n, f = 0, 1\), on each step the next values are calculated as \(n+1, f \times (n+1)\)". This is mathematically equivalent and almost identical to the recursive definition, but the \(+1\) emphasizes that the factorial values are being built *up*, going forwards from the starting case, rather than being computed after first going backwards, *down* to the base case, with a \(-1\) decrement. Note also that the direct output of the corecursive function does not simply contain the factorial \(n!\) values, but also includes for each value the auxiliary data of its index *n* in the sequence, so that any one specific result can be selected among them all, as and when needed.

Note the connection with [denotational semantics](https://zh.wikipedia.org/wiki/denotational_semantics "wikilink"), where the [denotations of recursive programs](https://zh.wikipedia.org/wiki/Denotational_semantics#Denotations_of_recursive_programs "wikilink") is built up corecursively in this way.

In Python, a recursive factorial function can be defined as:

``` python
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
```

This could then be called for example as `factorial(5)` to compute *5\!*.

A corresponding corecursive generator can be defined as:

``` python
def factorials():
    n, f = 0, 1
    while True:
        yield f
        n, f = n + 1, f * (n + 1)
```

This generates an infinite stream of factorials in order; a finite portion of it can be produced by:

``` python
def n_factorials(k):
    n, f = 0, 1
    while n <= k:
        yield f
        n, f = n + 1, f * (n + 1)
```

This could then be called to produce the factorials up to *5\!* via:

``` python
for f in n_factorials(5):
    print(f)
```

If we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,

``` python
def nth_factorial(k):
    n, f = 0, 1
    while n < k:
        n, f = n + 1, f * (n + 1)
    yield f
```

As can be readily seen here, this is practically equivalent (just by substituting `return` for the only `yield` there) to the accumulator argument technique for tail recursion, unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.

### Fibonacci sequence

In the same way, the [Fibonacci sequence](https://zh.wikipedia.org/wiki/Fibonacci_sequence "wikilink") can be represented as:

\[a, b = (0, 1) : (b, a+b)\] Note that because the Fibonacci sequence is a [recurrence relation](https://zh.wikipedia.org/wiki/recurrence_relation "wikilink") of order 2, the corecursive relation must track two successive terms, with the \((b, -)\) corresponding to shift forward by one step, and the \((-, a+b)\) corresponding to computing the next term. This can then be implemented as follows (using [parallel assignment](https://zh.wikipedia.org/wiki/parallel_assignment "wikilink")):

``` python
def fibonacci_sequence():
    a, b = 0, 1
    while True:
        yield a
        a, b = b, a + b
```

In Haskell,

``` haskell
 map fst ( (\(a,b) -> (b,a+b)) `iterate` (0,1) )
```

### Tree traversal

[Tree traversal](https://zh.wikipedia.org/wiki/Tree_traversal "wikilink") via a [depth-first](https://zh.wikipedia.org/wiki/depth-first "wikilink") approach is a classic example of recursion. Dually, [breadth-first](https://zh.wikipedia.org/wiki/breadth-first "wikilink") traversal can very naturally be implemented via corecursion.

Without using recursion or corecursion, one may traverse a tree by starting at the root node, placing the child nodes in a data structure, then removing the nodes in the data structure in turn and iterating over its children. If the data structure is a [stack](https://zh.wikipedia.org/wiki/Stack_\(abstract_data_type\) "wikilink") (LIFO), this yields depth-first traversal, while if the data structure is a [queue](https://zh.wikipedia.org/wiki/Queue_\(abstract_data_type\) "wikilink") (FIFO), this yields breadth-first traversal.

Using recursion, a (post-order) depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.

Using corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value, then breadth-first traversing the subtrees – i.e., passing on the *whole list* of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc. In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, *n*) was pushed forward, in addition to the actual output of *n*\!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:

`v,t = ([], FullTree) : (RootValues, ChildTrees)`

meaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list).

These can be compared as follows. The recursive traversal handles a *leaf node* (at the *bottom*) as the base case (when there are no children, just output the value), and *analyzes* a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off *below*). By contrast, the corecursive traversal handles a *root node* (at the *top*) as the base case (given a node, first output the value), treats a tree as being *synthesized* of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off *above*). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.

Notably, given an infinite tree, the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.

In Python, this can be implemented as follows. The usual post-order depth-first traversal can be defined as:

``` python
def df(node):
    if node is not None:
        df(node.left)
        df(node.right)
        print(node.value)
```

This can then be called by `df(t)` to print the values of the nodes of the tree in post-order depth-first order.

The breadth-first corecursive generator can be defined as:

``` python
def bf(tree):
    tree_list = [tree]
    while tree_list:
        new_tree_list = []
        for tree in tree_list:
            if tree is not None:
                yield tree.value
                new_tree_list.append(tree.left)
                new_tree_list.append(tree.right)
        tree_list = new_tree_list
```

This can then be called to print the values of the nodes of the tree in breadth-first order:

``` python
for i in bf(t):
    print(i)
```

## Definition

[Initial data types](https://zh.wikipedia.org/wiki/Initial_and_terminal_objects "wikilink") can be defined as being the [least fixpoint](https://zh.wikipedia.org/wiki/least_fixpoint "wikilink") ([up to isomorphism](https://zh.wikipedia.org/wiki/up_to_isomorphism "wikilink")) of some type equation; the [isomorphism](https://zh.wikipedia.org/wiki/isomorphism "wikilink") is then given by an [initial](https://zh.wikipedia.org/wiki/Initial_algebra "wikilink") [algebra](https://zh.wikipedia.org/wiki/F-algebra "wikilink"). Dually, final (or terminal) data types can be defined as being the [greatest fixpoint](https://zh.wikipedia.org/wiki/greatest_fixpoint "wikilink") of a type equation; the isomorphism is then given by a final [coalgebra](https://zh.wikipedia.org/wiki/F-coalgebra "wikilink").

If the domain of discourse is the [category of sets](https://zh.wikipedia.org/wiki/category_of_sets "wikilink") and total functions, then final data types may contain infinite, [non-wellfounded](https://zh.wikipedia.org/wiki/Non-well-founded_set_theory "wikilink") values, whereas initial types do not.\[1\]\[2\] On the other hand, if the domain of discourse is the category of [complete partial orders](https://zh.wikipedia.org/wiki/complete_partial_order "wikilink") and [continuous functions](https://zh.wikipedia.org/wiki/Scott_continuity "wikilink"), which corresponds roughly to the [Haskell](https://zh.wikipedia.org/wiki/Haskell_\(programming_language\) "wikilink") programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.\[3\]

Corecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary [recursion](https://zh.wikipedia.org/wiki/recursion "wikilink") recursively defines functions whose domain is an initial data type.\[4\]

The discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell.\[5\] Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.

## Discussion

The rule for *primitive corecursion* on [codata](https://zh.wikipedia.org/wiki/codata_\(computer_science\) "wikilink") is the dual to that for [primitive recursion](https://zh.wikipedia.org/wiki/primitive_recursion "wikilink") on data. Instead of descending on the argument by [pattern-matching](https://zh.wikipedia.org/wiki/pattern-matching "wikilink") on its constructors (that *were called up before*, somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. "fields"), we ascend on the result by filling-in its "destructors" (or "observers", that *will be called afterwards*, somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion *creates* (potentially infinite) codata, whereas ordinary recursion *analyses* (necessarily finite) data. Ordinary recursion might not be applicable to the codata because it might not terminate. Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.

In "Programming with streams in Coq: a case study: the Sieve of Eratosthenes"\[6\] we find

``` haskell
hd (conc a s) = a
tl (conc a s) = s

(sieve p s) = if div p (hd s) then sieve p (tl s)
              else conc (hd s) (sieve p (tl s))

hd (primes s) = (hd s)
tl (primes s) = primes (sieve (hd s) (tl s))
```

where primes "are obtained by applying the primes operation to the stream (Enu 2)". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as

\[p, s = (0, [2..]) : (hd(s), sieve(hd(s),tl(s)))\] or in Haskell,

``` haskell
(\(p, s@(h:t)) -> (h, sieve h t)) `iterate` (0, [2..])
```

The authors discuss how the definition of `sieve` is not guaranteed always to be *productive*, and could become stuck e.g. if called with `[5,10..]` as the initial stream.

Here is another example in Haskell. The following definition produces the list of [Fibonacci numbers](https://zh.wikipedia.org/wiki/Fibonacci_numbers "wikilink") in linear time:

``` haskell
fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
```

This infinite list depends on lazy evaluation; elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.

This can be done in Python as well:\[7\]

``` python
from itertools import tee, chain, islice, imap

def add(x, y):
    return x + y

def fibonacci():
    def deferred_output():
        for i in output:
            yield i
    result, c1, c2 = tee(deferred_output(), 3)
    paired = imap(add, c1, islice(c2, 1, None))
    output = chain([0, 1], paired)
    return result

for i in islice(fibonacci(), 20):
    print(i)
```

The definition of `zipWith` can be inlined, leading to this:

``` haskell
fibs = 0 : 1 : next fibs
  where
    next (a: t@(b:_)) = (a+b):next t
```

This example employs a self-referential *data structure*. Ordinary recursion makes use of self-referential *functions*, but does not accommodate self-referential data. However, this is not essential to the Fibonacci example. It can be rewritten as follows:

``` haskell
fibs = fibgen (0,1)
fibgen (x,y) = x : fibgen (y,x+y)
```

This employs only self-referential *function* to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with [non-strict](https://zh.wikipedia.org/wiki/lazy_evaluation "wikilink") list constructor this guarded recursion gradually produces an indefinitely defined list.

Corecursion need not produce an infinite object; a corecursive queue\[8\] is a particularly good example of this phenomenon. The following definition produces a [breadth-first traversal](https://zh.wikipedia.org/wiki/Breadth-first_search "wikilink") of a binary tree in linear time:

``` haskell
data Tree a b = Leaf a  |  Branch b (Tree a b) (Tree a b)

bftrav :: Tree a b -> [Tree a b]
bftrav tree = queue
  where
    queue = tree : gen 1 queue

    gen  0   p                 =         []
    gen len (Leaf   _     : s) =         gen (len-1) s
    gen len (Branch _ l r : s) = l : r : gen (len+1) s
```

This definition takes an initial tree and produces a list of subtrees. This list serves dual purpose as both the queue and the result (<small>*`gen len p`*</small> produces its output <small>*`len`*</small> notches after its input back-pointer, <small>*`p`*</small>, along the <small>*`queue`*</small>). It is finite if and only if the initial tree is finite. The length of the queue must be explicitly tracked in order to ensure termination; this can safely be elided if this definition is applied only to infinite trees.

Another particularly good example gives a solution to the problem of breadth-first labeling.\[9\] The function `label` visits every node in a binary tree in a breadth first fashion, and replaces each label with an integer, each subsequent integer is bigger than the last by one. This solution employs a self-referential data structure, and the binary tree can be finite or infinite.

``` haskell
label :: Tree a b -> Tree Int Int
label t = t′
    where
    (t′, ns) = go t (1:ns)

    go :: Tree a b    -> [Int]  -> (Tree Int Int, [Int])
    go   (Leaf   _    ) (n:ns) = (Leaf   n       , n+1 : ns  )
    go   (Branch _ l r) (n:ns) = (Branch n l′ r′ , n+1 : ns′′)
                                where
                                  (l′, ns′ ) = go l ns
                                  (r′, ns′′) = go r ns′
```

An [apomorphism](https://zh.wikipedia.org/wiki/apomorphism "wikilink") (such as an [anamorphism](https://zh.wikipedia.org/wiki/anamorphism "wikilink"), such as [unfold](https://zh.wikipedia.org/wiki/Unfold_\(higher-order_function\) "wikilink")) is a form of corecursion in the same way that a [paramorphism](https://zh.wikipedia.org/wiki/paramorphism "wikilink") (such as a [catamorphism](https://zh.wikipedia.org/wiki/catamorphism "wikilink"), such as [fold](https://zh.wikipedia.org/wiki/Fold_\(higher-order_function\) "wikilink")) is a form of recursion.

The [Coq](../Page/Coq.md "wikilink") proof assistant supports corecursion and [coinduction](https://zh.wikipedia.org/wiki/coinduction "wikilink") using the CoFixpoint command.

## 历史

Corecursion, referred to as *circular programming,* dates at least to , who credits [John Hughes](https://zh.wikipedia.org/wiki/John_Hughes_\(computer_scientist\) "wikilink") and [Philip Wadler](https://zh.wikipedia.org/wiki/Philip_Wadler "wikilink"); more general forms were developed in . The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.

## 参见

  -
  -
  - [递归](../Page/递归.md "wikilink")

  -
## 注释

## 参考文献

  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
  -
[Category:范畴论](https://zh.wikipedia.org/wiki/Category:范畴论 "wikilink") [Category:递归论](https://zh.wikipedia.org/wiki/Category:递归论 "wikilink")

1.  Barwise and Moss 1996.
2.  Moss and Danner 1997.
3.  Smyth and Plotkin 1982.
4.  Gibbons and Hutton 2005.
5.  Doets and van Eijck 2004.
6.  Leclerc and Paulin-Mohring, 1994
7.  Hettinger 2009.
8.  Allison 1989; Smith 2009.
9.  Jones and Gibbons 1992.
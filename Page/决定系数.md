> 本文内容由[决定系数](https://zh.wikipedia.org/wiki/决定系数)转换而来。


\[\[<File:Coefficient> of Determination.svg|thumb|400px|决定系数\(R^2 = 1 - \frac{\color{blue}{SS_\text{res}}}{\color{red}{SS_\text{tot}}}\)示意图

线性回归（右侧）的效果比起平均值（左侧）越好，决定系数的值就越接近于1。 蓝色正方形表示线性回归的残差的平方， 红色正方形数据表示对于平均值的残差的平方。\]\] **决定系数**（，记为*R*<sup>2</sup>或*r*<sup>2</sup>）在[统计学](../Page/统计学.md "wikilink")中用于度量因变量的变异中可由自变量解释部分所占的比例，以此来判断[统计模型的解释力](https://zh.wikipedia.org/wiki/统计模型 "wikilink")。\[1\]\[2\]\[3\]

对于简单[线性回归而言](https://zh.wikipedia.org/wiki/线性回归 "wikilink")，决定系数为样本[相关系数的平方](https://zh.wikipedia.org/wiki/相关系数 "wikilink")。\[4\]当加入其他回归自变量后，决定系数相应地变为多重相关系数的平方。

假设一数据集包括*y*<sub>1</sub>,...,*y*<sub>*n*</sub>共*n*个观察值，相对应的模型预测值分别为*f*<sub>1</sub>,...,*f*<sub>*n*</sub>。定义[残差](https://zh.wikipedia.org/wiki/残差 "wikilink")*e*<sub>*i*</sub> = *y*<sub>*i*</sub> − *f*<sub>*i*</sub>，平均观察值为

\[\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i.\]

于是可以得到总平方和

  -
    \(SS_\text{tot}=\sum_i (y_i-\bar{y})^2,\)

回归平方和

\[SS_\text{reg}=\sum_i (f_i -\bar{y})^2,\] 残差平方和

\[SS_\text{res}=\sum_i (y_i - f_i)^2=\sum_i e_i^2\,\]

由此，决定系数可定义为

\[R^2 \equiv 1 - {SS_{\rm res}\over SS_{\rm tot}}.\,\]

## 参考文献

[Category:回归分析](https://zh.wikipedia.org/wiki/Category:回归分析 "wikilink")

1.
2.
3.
4.
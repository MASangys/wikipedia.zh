[Loss_function_surrogates.svg](https://zh.wikipedia.org/wiki/File:Loss_function_surrogates.svg "fig:Loss_function_surrogates.svg")

在[機器學習和](https://zh.wikipedia.org/wiki/機器學習 "wikilink")[最佳化領域中](https://zh.wikipedia.org/wiki/最佳化 "wikilink")，分類問題之損失函數可以用來表達預測不準確之程度，其中分類問題主要是用來判斷所偵測到的物件屬於什麼類別。將一個向量空間\(X\)做為所有的輸入值，而向量空間\(Y=\{-1, 1\}\)做為所有的輸出值。我们希望能夠找到最佳的公式\(f: X\rightarrow\Re\)將\(\vec{x}\)映射到\(y\)\[1\]。然而，由于信息不完整、雜訊、计算過程中的非确定性模块等因素，有可能會有相同的輸入值\(\vec{x}\)映射到不同的輸出值\(y\)\[2\]。因此，這個學習過程的目的就是要最小化預期風險（更详细的介绍参见[统计学习理论](../Page/统计学习理论.md "wikilink")），預期風險之定義為：

\[I[f]=\textstyle \int_{X \times Y}^{} \displaystyle V(f(\vec{x},y))p(\vec{x},y)d\vec{x}dy\]

其中\(V(f(\vec{x},y))\)即損失函數，而\(p(\vec{x},y)\)為機率密度函數。而實作上概率分布\(p(\vec{x},y)\)通常是未知的，因此我们使用由数据[样本空间中取出的](https://zh.wikipedia.org/wiki/样本空间 "wikilink")\(n\)個[獨立且同分布](../Page/独立同分布.md "wikilink")（i.i.d.）的樣本點

\[S=\{(\vec{x_1},y_1),...,(\vec{x_n},y_n)\}\]作为训练集，將樣本空間所得到的经验風險做為預期風險的替代，其定義為：

\[I_S[f]=\frac{1}{n}\sum_{i=1}^n V(f(\vec{x_i},y_i))\]

基於分類問題的二元性，可定義0-1函數做為匹配值之基準。因此損失函數為：

\[V(f(\vec{x},y))=H(-yf(\vec{x}))\]

其中\(H\)為[步階函數](../Page/单位阶跃函数.md "wikilink")。然而損失函數並不是[凸函數或平滑函數](../Page/凸函数.md "wikilink")，是一種[NP-hard的問題](../Page/NP困难.md "wikilink")，因此做為替代，需要使用可以追蹤的機器學習演算法（透過凸損失函數）。

## 分類問題之界線

使用貝式定理，可以基於問題的二元性最佳化映射公式\(f^*\)為：

\[f^*(\vec{x}) = \begin{cases}
1, & \text{if }p(1\mid\vec{x})>p(-1\mid\vec{x}) \\
-1, & \text{if }p(1\mid\vec{x})<p(-1\mid\vec{x})
\end{cases}\]

當\(p(1\mid\vec{x})\neq p(-1\mid\vec{x})\)

## 簡化分類問題預期風險

\[\begin{alignat}{4}
I[f(x)] & = \int_{X\times Y}^{} V(f(\vec{x},y))p(\vec{x},y)d\vec{x}dy \\
& = \int_{X}^{}\int_{Y}^{} V(f(\vec{x},y))p(\vec{x},y)p(\vec{x})dyd\vec{x} \\
& = \int_{X}^{} [V(-f(\vec{x})p(1\mid x)+V(f(\vec{x})p(-1\mid x)]p(\vec{x})d\vec{x} \\
& = \int_{X}^{} [V(-f(\vec{x})p(1\mid x)+V(f(\vec{x})(1-p(1\mid x))]p(\vec{x})d\vec{x}
\end{alignat}\]

## 平方損失

\[V(f(\vec{x},y))=(1-yf(\vec{x}))^2\]

平方損失凸且平滑，但容易過度懲罰錯誤預測，導致收斂速度比邏輯損失和鏈結損失慢。它的優點為有助於簡化[交叉驗證之正則化](../Page/交叉驗證.md "wikilink")（[regularization](https://zh.wikipedia.org/wiki/regularization "wikilink")）。

最小化預期風險之映射函數為：

\[f^*_{Square}=2p(1\mid x)-1\]

## 鏈結損失

\[V(f(\vec{x}),y) = \max(0, 1-yf(\vec{x})) = |1 - yf(\vec{x}) |_{+}\]
鏈結損失公式等同於[支持向量機](https://zh.wikipedia.org/wiki/支持向量機 "wikilink")（SVM）的損失公式。鏈結損失凸但不平滑（在\(yf(\vec{x})) = 1\)不可微分），因此不適用於[梯度下降法和](https://zh.wikipedia.org/wiki/梯度下降法 "wikilink")[隨機梯度下降法](https://zh.wikipedia.org/wiki/隨機梯度下降法 "wikilink")，但適用[次梯度下降法](https://zh.wikipedia.org/wiki/次梯度下降法 "wikilink")。
最小化預期風險之映射函數為：

\[f^*_{Square}=2p(1\mid x)-1\]

## 廣義平滑鏈結損失

\[f^*_\alpha(z) \;=\; \begin{cases} \frac{\alpha}{\alpha + 1}& \text{if }z< 0 \\ \frac{1}{\alpha + 1}z^{\alpha + 1} - z + \frac{\alpha}{\alpha + 1} & \text{if } 0<z<1 \\ 0 & \text{if } z \geq 1 \end{cases}\]
其中\(z = yf(\vec{x})\)

## 邏輯損失

\[V(f(\vec{x}),y) = \frac{1}{\ln 2}\ln(1+e^{-yf(\vec{x})})\]
適用於梯度下降法，但不會對錯誤預測做懲罰。 最小化預期風險之映射函數為：

\[f^*_\text{Logistic}= \ln\left(\frac{p(1\mid x)}{1-p(1\mid x)}\right).\]

## 交叉熵損失

\[V(f(\vec{x}),t) = -t\ln(f(\vec{x}))-(1-t)\ln(1-f(\vec{x}))\]
其中\(t=(1+y)/2\) so that \(t \in \{0,1\}\) 屬於凸函數，適用於隨機梯度下降法。

## 指數損失

\[V(f(\vec{x}),y) = e^{-\beta yf(\vec{x})}\]

1.
2.
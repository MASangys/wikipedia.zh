> 本文内容由[卷积神经网络](https://zh.wikipedia.org/wiki/卷积神经网络)转换而来。


{{ request translation }}

**卷积神经网络**（Convolutional Neural Network, **CNN**）是一种[前馈神经网络](https://zh.wikipedia.org/wiki/前馈神经网络 "wikilink")，它的人工神经元可以响应一部分覆盖范围内的周围单元，\[1\]对于大型图像处理有出色表现。

卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和[语音识别](../Page/语音识别.md "wikilink")方面能够给出更好的结果。这一模型也可以使用[反向传播算法](../Page/反向传播算法.md "wikilink")进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构\<ref name=""STANCNN"\></ref>。

## 定义

“卷积神经网络”表示在网络采用称为卷积的数学运算。卷积是一种特殊的线性操作。卷积网络是一种特殊的神经网络，它们在至少一个层中使用卷积代替一般矩阵乘法

## 概览

## 发展

## 结构

### 卷積層

卷積層是一組平行的特徵圖（feature map），它通過在輸入圖像上滑動不同的卷積核並執行一定的運算而組成。此外，在每一個滑動的位置上，卷積核與輸入圖像之間會執行一個元素對應乘積並求和的運算以將感受野內的信息投影到特徵圖中的一個元素。這一滑動的過程可稱爲步幅 Z_s，步幅 Z_s 是控制輸出特徵圖尺寸的一個因素。卷積核的尺寸要比輸入圖像小得多，且重疊或平行地作用於輸入圖像中，一張特徵圖中的所有元素都是通過一個卷積覈計算得出的，也即一張特徵圖共享了相同的權重和偏置項。

### 線性整流層

線性整流層（Rectified Linear Units layer, ReLU layer）使用[線性整流](../Page/线性整流函数.md "wikilink")（Rectified Linear Units, ReLU）\(f(x)=\max(0,x)\)作为這一層神經的激勵函數（Activation function）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。

事实上，其他的一些函数也可以用于增强网络的非线性特性，如[双曲正切函数](https://zh.wikipedia.org/wiki/双曲正切函数 "wikilink") \(f(x)=\tanh(x)\), \(f(x)=|\tanh(x)|\)，或者[Sigmoid函数](../Page/S函数.md "wikilink")\(f(x)=(1+e^{-x} )^{-1}\)。相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍\[2\]，而并不会对模型的泛化准确度造成显著影响。

### 池化層

[Max_pooling.png](https://zh.wikipedia.org/wiki/File:Max_pooling.png "fig:Max_pooling.png") 池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了[过拟合](https://zh.wikipedia.org/wiki/过拟合 "wikilink")。通常来说，CNN的卷积层之间都会周期性地插入池化层。

池化层通常会分别作用于每个输入的特征并减小其大小。目前最常用形式的池化层是每隔2个元素从图像划分出\(2 \times 2\)的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。

除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“[L2-范数池化](https://zh.wikipedia.org/wiki/Lp空间#.E9.95.BF.E5.BA.A6.E3.80.81.E8.B7.9D.E7.A6.BB.E4.B8.8E.E8.8C.83.E6.95.B0 "wikilink")”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。

由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，\[3\]甚至不再使用池化层。\[4\]

### 损失函数层

损失函数层（loss layer）用于决定训练过程如何来“惩罚”网络的预测结果和真实结果之间的差异，它通常是网络的最后一层。各种不同的损失函数适用于不同类型的任务。例如，[Softmax交叉熵损失函数常常被用于在K个类别中选出一个](../Page/Softmax函数.md "wikilink")，而[Sigmoid交叉熵损失函数常常用于多个独立的二分类问题](../Page/邏輯函數.md "wikilink")。欧几里德损失函数常常用于结果取值范围为任意实数的问题。

## 应用

### 影像辨識

卷积神经网络通常在影像辨識系统中使用。

### 視訊分析

相比影像辨識问题，視訊分析要难许多。CNN也常被用于这类问题。

### 自然语言处理

卷积神经网络也常被用于[自然语言处理](../Page/自然语言处理.md "wikilink")。 CNN的模型被证明可以有效的处理各种自然语言处理的问题，如语义分析\[5\]、搜索结果提取\[6\]、句子建模\[7\] 、分类\[8\]、预测\[9\]、和其他传统的NLP任务\[10\] 等。

### 药物发现

卷积神经网路已在药物发现中使用。卷积神经网络被用来预测的分子与蛋白质之间的相互作用，以此来寻找靶向位点，寻找出更可能安全和有效的潜在治疗方法。

### 围棋

卷积神经网络在计算机围棋领域也被使用。2016年3月，[AlphaGo](../Page/AlphaGo.md "wikilink")对战[李世乭](../Page/李世乭.md "wikilink")的比赛，展示了深度学习在围棋领域的重大突破。

## 微调（fine-tuning）

卷积神经网络（例如Alexnet、VGG网络）在网络的最后通常为softmax分类器。微调一般用来调整softmax分类器的分类数。例如原网络可以分类出2种图像，需要增加1个新的分类从而使网络可以分类出3种图像。微调（fine-tuning）可以留用之前训练的大多数参数，从而达到快速训练收敛的效果。例如保留各个卷积层，只重构卷积层后的全连接层与softmax层即可。

## 經典模型

  - [LeNet](https://zh.wikipedia.org/wiki/LeNet "wikilink")

<!-- end list -->

  - [AlexNet](https://zh.wikipedia.org/wiki/AlexNet "wikilink")

<!-- end list -->

  - [VGG](https://zh.wikipedia.org/wiki/VGG "wikilink")

<!-- end list -->

  - [GoogLeNet](https://zh.wikipedia.org/wiki/GoogLeNet "wikilink")

<!-- end list -->

  - [ResNet](https://zh.wikipedia.org/wiki/ResNet "wikilink")

## 可用包

  - [roNNie](https://www.ronnie.ai): 是一個簡易入門級框架,使用Tensorflow 計算層.可於python下載 pip3 ronnie
  - [Caffe](../Page/Caffe.md "wikilink"): Caffe包含了CNN使用最广泛的库。它由伯克利视觉和学习中心（BVLC）研发，拥有比一般实现更好的结构和更快的速度。同时支持[CPU和](https://zh.wikipedia.org/wiki/CPU "wikilink")[GPU计算](https://zh.wikipedia.org/wiki/GPU "wikilink")，底层由[C++](../Page/C++.md "wikilink")实现，并封装了Python和[MATLAB](../Page/MATLAB.md "wikilink")的接口。
  - Torch7（www.torch.ch）
  - OverFeat
  - Cuda-convnet
  - MatConvnet
  - [Theano](https://zh.wikipedia.org/wiki/Theano "wikilink")：用[Python](../Page/Python.md "wikilink")实现的神经网络包\[11\]
  - [TensorFlow](../Page/TensorFlow.md "wikilink")
  - Paddlepaddle([www.paddlepaddle.org](http://www.paddlepaddle.org/))
  - [Keras](../Page/Keras.md "wikilink")

## 参考

[Category:人工智能](https://zh.wikipedia.org/wiki/Category:人工智能 "wikilink") [Category:人工神经网络](https://zh.wikipedia.org/wiki/Category:人工神经网络 "wikilink")

1.
2.
3.
4.
5.
6.
7.
8.
9.  Collobert, Ronan, and Jason Weston. "A unified architecture for natural language processing: Deep neural networks with multitask learning."Proceedings of the 25th international conference on Machine learning. ACM, 2008.
10.
11. [深度网络：Theano项目主页。](http://deeplearning.net/software/theano/)
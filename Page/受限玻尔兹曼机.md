[Restricted_Boltzmann_machine.svg](https://zh.wikipedia.org/wiki/File:Restricted_Boltzmann_machine.svg "fig:Restricted_Boltzmann_machine.svg")
 **受限玻尔兹曼机**（,
RBM）是一种可通过输入数据集学习概率分布的[随机](https://zh.wikipedia.org/wiki/随机神经网络 "wikilink")[生成](https://zh.wikipedia.org/wiki/生成模型 "wikilink")[神经网络](https://zh.wikipedia.org/wiki/神经网络 "wikilink")。RBM最初由发明者于1986年命名为**簧风琴**（Harmonium）\[1\]，但直到[杰弗里·辛顿及其合作者在](../Page/杰弗里·辛顿.md "wikilink")2000年代中叶发明快速学习算法后，受限玻兹曼机才变得知名。受限玻兹曼机在[降维](../Page/降维.md "wikilink")\[2\]、[分类](https://zh.wikipedia.org/wiki/统计分类 "wikilink")\[3\]、[协同过滤](https://zh.wikipedia.org/wiki/协同过滤 "wikilink")、[特征学习](https://zh.wikipedia.org/wiki/特征学习 "wikilink")\[4\]和[主题建模](https://zh.wikipedia.org/wiki/主题建模 "wikilink")\[5\]中得到了应用。根据任务的不同，受限玻兹曼机可以使用[监督学习或](https://zh.wikipedia.org/wiki/监督学习 "wikilink")[无监督学习的方法进行训练](https://zh.wikipedia.org/wiki/非監督式學習 "wikilink")。

正如名字所提示的那样，受限玻兹曼机是一种[玻兹曼机的变体](../Page/玻尔兹曼机.md "wikilink")，但限定模型必须为[二分图](../Page/二分图.md "wikilink")。模型中包含对应输入参数的输入（可见）单元和对应训练结果的隐单元，图中的每条边必须连接一个可见单元和一个隐单元。（与此相对，“无限制”玻兹曼机包含隐单元间的边，使之成为[递归神经网络](../Page/递归神经网络.md "wikilink")。）这一限定使得相比一般玻兹曼机更高效的训练算法成为可能，特别是基于[梯度的对比分歧](https://zh.wikipedia.org/wiki/梯度下降法 "wikilink")（contrastive
divergence）算法\[6\]。

受限玻兹曼机也可被用于[深度学习网络](../Page/深度学习.md "wikilink")。具体地，[深度信念网络可使用多个RBM堆叠而成](https://zh.wikipedia.org/wiki/深度信念网络 "wikilink")，并可使用[梯度下降法和](https://zh.wikipedia.org/wiki/梯度下降法 "wikilink")[反向传播算法进行调优](../Page/反向传播算法.md "wikilink")\[7\]。

## 结构

标准的受限玻尔兹曼机由二值（[布尔](../Page/布尔代数.md "wikilink")/[伯努利](../Page/伯努利分布.md "wikilink")）隐层和可见层单元组成。权重矩阵\(W = (w_{i,j})\)中的每个元素指定了隐层单元\(h_j\)和可见层单元\(v_i\)之间边的权重。此外对于每个可见层单元\(v_i\)有偏置\(a_i\)，对每个隐层单元\(h_j\)有偏置\(b_j\)。在这些定义下，一种受限玻尔兹曼机配置（即给定每个单元取值）的“能量”被定义为

\[E(v,h) = -\sum_i a_i v_i - \sum_j b_j h_j -\sum_i \sum_j h_j w_{i,j} v_i\]

或者用矩阵的形式表示如下：

\[E(v,h) = -a^{\mathrm{T}} v - b^{\mathrm{T}} h -h^{\mathrm{T}} W v\]

这一能量函数的形式与[霍普菲尔德神经网络相似](https://zh.wikipedia.org/wiki/霍普菲尔德神经网络 "wikilink")。在一般的玻尔兹曼机中，隐层和可见层之间的联合概率分布由能量函数给出：\[8\]

\[P(v,h) = \frac{1}{Z} e^{-E(v,h)}\]

其中，\(Z\)为[配分函数](../Page/配分函数.md "wikilink")，定义为在节点的所有可能取值下\(e^{-E(v,h)}\)的和（亦即使得概率分布和为1的[归一化常数](https://zh.wikipedia.org/wiki/归一化常数 "wikilink")）。类似地，可见层取值的[边缘分布可通过对所有隐层配置求和得到](https://zh.wikipedia.org/wiki/边缘分布 "wikilink")：\[9\]

\[P(v) = \frac{1}{Z} \sum_h e^{-E(v,h)}\]

由于RBM为一个二分图，层内没有边相连，因而隐层是否激活在给定可见层节点取值的情况下是[条件独立的](../Page/条件独立.md "wikilink")。类似地，可见层节点的激活状态在给定隐层取值的情况下也条件独立\[10\]。亦即，对\(m\)个可见层节点和\(n\)个隐层节点，可见层的配置对于隐层配置的[条件概率如下](../Page/条件概率.md "wikilink")：

\[P(v|h) = \prod_{i=1}^m P(v_i|h)\].

类似地，对于的条件概率为

\[P(h|v) = \prod_{j=1}^n P(h_j|v)\].

其中，单个节点的激活概率为

\[P(h_j=1|v) = \sigma \left(b_j + \sum_{i=1}^m w_{i,j} v_i \right)\,\]和\(\,P(v_i=1|h) = \sigma \left(a_i + \sum_{j=1}^n w_{i,j} h_j \right)\)

其中\(\sigma\)代表[逻辑函数](https://zh.wikipedia.org/wiki/逻辑函数 "wikilink")。

### 与其他模型的关系

受限玻尔兹曼机是玻尔兹曼机和[马尔科夫随机场的一种特例](../Page/马尔可夫网络.md "wikilink")\[11\]\[12\]。这些[概率图模型可以对应到](https://zh.wikipedia.org/wiki/概率图模型 "wikilink")[因子分析](https://zh.wikipedia.org/wiki/因子分析 "wikilink")\[13\]。

## 训练算法

受限玻尔兹曼机的训练目标是针对某一训练集\(V\)，最大化概率的乘积。其中，\(V\)被视为一矩阵，每个行向量作为一个可见单元向量\(v\)：

\[\arg\max_W \prod_{v \in V} P(v)\]

或者，等价地，最大化\(V\)的[对数概率](https://zh.wikipedia.org/wiki/对数概率 "wikilink")[期望](https://zh.wikipedia.org/wiki/期望 "wikilink")：\[14\]\[15\]

\[\arg\max_W \mathbb{E} \left[\sum_{v \in V} \log P (v)\right]\]

训练受限玻尔兹曼机，即最优化权重矩阵\(W\)，最常用的算法是[杰弗里·辛顿提出的对比分歧](../Page/杰弗里·辛顿.md "wikilink")（contrastive
divergence，CD）算法。这一算法最早被用于训练辛顿提出的“专家积”模型\[16\]。这一算法在[梯度下降的过程中使用](https://zh.wikipedia.org/wiki/梯度下降 "wikilink")[吉布斯采样完成对权重的更新](../Page/吉布斯采样.md "wikilink")，与训练前馈神经网络中利用反向传播算法类似。

基本的针对一个样本的单步对比分歧（CD-1）步骤可被总结如下：

1.  取一个训练样本，计算隐层节点的概率，在此基础上从这一概率分布中获取一个隐层节点激活向量的样本；
2.  计算和的[外积](../Page/外积.md "wikilink")，称为“正梯度”；
3.  从获取一个重构的可见层节点的激活向量样本，此后从再次获得一个隐层节点的激活向量样本；
4.  计算和的外积，称为“负梯度”；
5.  使用正梯度和负梯度的差以一定的学习率更新权重\(w_{i,j}\)：\(\Delta w_{i,j} = \epsilon (vh^\mathsf{T} - v'h'^\mathsf{T})\)。

偏置和也可以使用类似的方法更新。

## 参见

  - [自編碼](https://zh.wikipedia.org/wiki/自編碼 "wikilink")
  - [自编码机](https://zh.wikipedia.org/wiki/自编码机 "wikilink")
  - [深度学习](../Page/深度学习.md "wikilink")
  - [Hopfield神經網絡](https://zh.wikipedia.org/wiki/Hopfield神經網絡 "wikilink")

## 参考资料

## 外部链接

  - [Introduction to Restricted Boltzmann
    Machines](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/).
    Edwin Chen's blog, July 18, 2011.

[Category:人工神经网络](https://zh.wikipedia.org/wiki/Category:人工神经网络 "wikilink")

1.

2.

3.

4.

5.  Ruslan Salakhutdinov and Geoffrey Hinton (2010). [Replicated
    softmax: an undirected topic
    model](http://books.nips.cc/papers/files/nips22/NIPS2009_0817.pdf).
    *[Neural Information Processing
    Systems](https://zh.wikipedia.org/wiki/Neural_Information_Processing_Systems "wikilink")*
    **23**.

6.  Miguel Á. Carreira-Perpiñán and Geoffrey Hinton (2005). On
    contrastive divergence learning. *Artificial Intelligence and
    Statistics*.

7.

8.  Geoffrey Hinton (2010). *[A Practical Guide to Training Restricted
    Boltzmann
    Machines](http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)*.
    UTML TR 2010–003, University of Toronto.

9.
10.
11.

12. Asja Fischer and Christian Igel. [Training Restricted Boltzmann
    Machines: An
    Introduction](http://image.diku.dk/igel/paper/TRBMAI.pdf) . Pattern
    Recognition 47, pp. 25-39, 2014

13.

14.
15.
16.
**坐标下降法**（）是一种非梯度优化算法。算法在每次迭代中，在当前点处沿一个坐标方向进行以求得一个函数的局部极小值。在整个过程中循环使用不同的坐标方向。对于不可拆分的函数而言，算法可能无法在较小的迭代步数中求得最优解。为了加速收敛，可以采用一个适当的坐标系，例如通过[主成分分析](../Page/主成分分析.md "wikilink")获得一个坐标间尽可能不相互关联的新坐标系（参考）。

## 算法描述

坐标下降法基于的思想是多变量函数\(F(\mathbf{x})\)可以通过每次沿一个方向优化来获取最小值。与通过梯度获取最速下降的方向不同，在坐标下降法中，优化方向从算法一开始就予以固定。例如，可以选择[线性空间的一组](https://zh.wikipedia.org/wiki/线性空间 "wikilink")[基](../Page/基_\(線性代數\).md "wikilink")\(\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n\)作为搜索方向。
在算法中，循环最小化各个坐标方向上的目标函数值。亦即，如果\(\mathbf{x}^k\)已给定，那么，\(\mathbf{x}^{k+1}\)的第\(i\)个维度为

\[\mathbf{x}^{k+1}_i = \underset{y\in\mathbb R}{\operatorname{arg\,min}}\; f(x^{k+1}_1,...,x^{k+1}_{i-1},y,x^k_{i+1},...,x^k_n);\]

因而，从一个初始的猜测值\(\mathbf{x}_0\)以求得函数\(F\)的局部最优值，可以迭代获得\(\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \dots\)的序列。

通过在每一次迭代中采用，可以很自然地获得不等式

\[F(\mathbf{x}_0)\ge F(\mathbf{x}_1)\ge F(\mathbf{x}_2)\ge \cdots,\]

可以知道，这一序列与最速下降具有类似的收敛性质。如果在某次迭代中，函数得不到优化，说明一个[驻点](../Page/驻点.md "wikilink")已经达到。

这一过程可以用下图表示。

[Coordinate_descent.svg](https://zh.wikipedia.org/wiki/File:Coordinate_descent.svg "fig:Coordinate_descent.svg")

### 例子

对于非平滑函数，坐标下降法可能会遇到问题。下图展示了当函数等高线非平滑时，算法可能在非驻点中断执行。

[Nonsmooth_coordinate_descent.svg](https://zh.wikipedia.org/wiki/File:Nonsmooth_coordinate_descent.svg "fig:Nonsmooth_coordinate_descent.svg")

## 应用

坐标下降法在[机器学习](../Page/机器学习.md "wikilink")中有应用，例如训练线性[支持向量机](../Page/支持向量机.md "wikilink")\[1\]（可见）以及\[2\]。

## 参见

  -
  - [共轭梯度法](../Page/共轭梯度法.md "wikilink")

  - [梯度下降法](https://zh.wikipedia.org/wiki/梯度下降法 "wikilink")

  - [牛顿法](../Page/牛顿法.md "wikilink")

  - [最优化](../Page/最优化.md "wikilink")

  -
## 参考

  -
  - Bertsekas, Dimitri P. (1999). *Nonlinear Programming, Second
    Edition* Athena Scientific, Belmont, Massachusetts. ISBN
    1-886529-00-0.

  - .

  - .

  - .

  - .

  - .

[Category:最优化算法](https://zh.wikipedia.org/wiki/Category:最优化算法 "wikilink")

1.
2.
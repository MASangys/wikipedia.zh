> 本文内容由[声学模型](https://zh.wikipedia.org/wiki/声学模型)转换而来。


**声学模型**（Acoustic model）是[语音识别](../Page/语音识别.md "wikilink")系统中最为重要的部分之一，目前的主流系统多采用[隐马尔科夫模型进行建模](https://zh.wikipedia.org/wiki/隐马尔科夫模型 "wikilink")。 隐马尔可夫模型的概念是一个离散时域有限状态自动机，隐马尔可夫模型HMM是指这一马尔可夫模型的内部状态外界不可见，外界只能看到各个时刻的输出值。对语音识别系统，输出值通常就是从各个帧计算而得的声学特征。用HMM刻画语音信号需作出两个假设，一是内部状态的转移只与上一状态有关，另一是输出值只与当前状态（或当前的状态转移）有关，这两个假设大大降低了模型的复杂度。HMM的打分、解码和训练相应的算法是前向算法、[维特比算法](../Page/维特比算法.md "wikilink")和前向后向算法。

## 输出概率

声学模型的输入是由特征提取模块提取的特征。一般来说，这些特征是多维的向量，并且其取值可以是离散或连续的。早期的声学模型常常采用[矢量量化](https://zh.wikipedia.org/wiki/矢量量化 "wikilink")(Vector Quantification)的方法，将信号直接映射到某个码本\(k\)，而后再计算某个模型\(j\)输出该码本的概率\(b_j(k)\)。但是这一方法是比较粗糙的，其性能受到VQ算法的极大影响，如果VQ本身性能就很差，声学模型的估计就会很不准确。因此，对于连续取值的特征应当采用连续的概率分布。由于语音信号特征的分布并不能用简单的概率分布，例如高斯分布等来直接描述，故而常用[高斯混合模型或](https://zh.wikipedia.org/wiki/高斯混合模型 "wikilink")[混合拉普拉斯模型等方法对语音信号的分布进行拟合](https://zh.wikipedia.org/wiki/混合拉普拉斯模型 "wikilink")。在此，高斯混合分布可以表示为若干高斯分量\(G_i\)的加权组合。即：

\[G(x) = \sum_{i=1}^{n}w_i\cdot G_i(x)\]

其中\(G_i(x)\)是均值为\(\mu_i\)方差为\(\sigma_i\)的高斯分布。从数学角度看，当\(i\)趋向于无穷时，任何连续分布都可以用混合高斯模型来逼近。但是，高斯混合模型也存在着问题，那就是其计算量偏大。假设对于一个包含\(n\)个混合分量的高斯混合模型，其维度为\(m\)维，那么至少要进行\(m\times n\)次运算才能得到结果，如果有\(i\)个模型需要计算，那么时间复杂度就是\(O(mnk)\)。相比之下，离散HMM就相对简单，只需要进行一次VQ，再进行\(i\)次查表操作，就能够计算所有模型的概率值。因此，也出现了将二者结合起来的半连续隐马模型。其思路是输出概率不仅仅由\(b_j(k)\)来决定，还乘上了VQ的概率，亦即该信号属于次码本的概率。

从精确度上看，连续隐马模型要优于半连续隐马模型，而半连续隐马模型又优于离散隐马模型。从算法复杂度上来看则正好相反。\[1\]

[高斯混合模型](https://zh.wikipedia.org/wiki/高斯混合模型 "wikilink")（Gaussian Mixture Model, GMM）是语音信号处理中的一种常用的统计模型，该模型的一个基本理论前提是只要高斯混合的数目足够多，一个任意的分布就可以在任意的精度下用这些高斯混合的加权平均来逼近。一个包含M个分量的高斯混合分布的概率密度函数是M个高斯概率密度分布函数的加权组合，定义为\[2\]:

\[p(x|\lambda) = \sum_{i}^{M}\omega_ip_i(x)\]

其中的\(x\)是\(D\)维随机矢量，\(p_i(x), i = 1, 2, \cdots,M\)为\(M\)个概率密度函数分量，\(\omega_i, i = 1,2,\cdots,M\)为各个概率密度函数分量的权重。在上式中，每个概率密度函数分量\(p_i(x)\)都服从\(D\)维高斯分布，即

\[p_i(x)=\frac{1}{(2\pi)^{D/2}|\Sigma_i|} \exp\left\{-\frac{1}{2}(x-\mu_i)'\Sigma_i^{-1}(x-\mu_i)\right\}\]

其中，\(\mu_i\)表示该高斯分量的均值，\(\Sigma_i\)表示该高斯分量的协方差矩阵。另外，为了满足概率密度函数分布的要求，上式中各个概率密度函数分量的权重必须满足\(\sum_{i=1}^{M}w_i = 1\)的要求。

在高斯混合模型中，每一个高斯概率密度函数分量\(p_i(x)\)都可以由其权重\(w_i\)、均值\(\mu_i\)和协方差矩阵\(\Sigma_i\)来描述。这样，一个完整的\(M\)分量混合的高斯分布就可以由以下的三元组集合来表示：

\[\lambda=\left\{w_i,\mu_i,\Sigma_i\right\} \quad\quad i=1,2,\cdots,M\]

GMM模型的主要问题为训练问题，亦即参数估计问题数估计，使得GMM模型和训练数据之间达到最佳的匹配程度。GMM的参数估 计方法有多种方法，其中应用最广泛的是基于最大似然准则(Maximum Likelihood Estimation, MLE)的方法。

对于一段给定的训练语音特征序列\(O = O_1,O_2,\cdots,O_T\) ，GMM模型的似然度定义为：

\[p(O|\lambda) = \prod_{t=1}^{T}p(O_t|\lambda)\]

最大似然估计的主要思想就是要找到使得GMM模型对于训练语料的似然度最大的模型参数\(\lambda\)。同HMM的训练类似，GMM训练也可以通过EM进行训练，其模型参数更新公式为：

\[\hat{w}_i = \frac{1}{T}\sum_t^{T}p(i|x_t,\lambda)\]

\[\hat{\mu}_i = \frac{\sum_{t=1}^{T}p(i|x_t,\lambda)x_t}{\sum_{t=1}^{T}p(i|x_t,\lambda)}\]

\[\hat{\sigma}_i = \frac{\sum_{t=1}^{T}p(i|x_t,\lambda)x^{2}_t}{\sum_{t=1}^{T}p(i|x_t,\lambda)}-\hat{\mu}_i^2\]

其中\(p(i|x_t,\lambda)\)表示\(x_t\)属于第\(i\)个高斯分量的后验概率。而\(w_i,\mu_i,\sigma_i^2\)分表表示上一步迭代中模型的 权重、均值、协方差矩阵，而\(\hat{w_i},\hat{\mu_i},\hat{\sigma}_i^2\)则是更新后的对应参数。\(p(i|x_t,\lambda)\)的定义为：

\[p(i|x_t,\lambda) = \frac{w_ip_i(x_i)}{\sum_{k=1}^M w_kp_k(x_i)}\]

如果随机矢量各维间的是独立的，那么可以采用对角协方差阵，亦即仅估计方差。这种方法能够极大减少模型参数，让模型训练更加充分。同时，需要注意的是，在某些情况下，对角协方差阵可能会出现非常小的方差值，从而使得协方差阵奇异。因此在训练对角协方差阵的时候必须采用最小方差约束。亦即当新估计出的某维方差\(\hat{\sigma}_i\)小于设定\(\sigma_{min}\)时，让\(\hat{\sigma}_i\)等于\(\sigma_{min}\)。

在声学模型训练中常用GMM为状态输出概率建模，同时GMM也常用于其他声音分类任务中，例如声音分割与分类，说话人识别等。

## 模型拓扑结构

由于语音的时序性，隐马模型的拓扑结构一般都取为自左向右的结构。一般每个状态都包括自跳转弧。是否允许跨状态 跳转则没有一个定论。

下图是典型的模型拓扑结构示意图。

[Hidden-Markov-Model-Topology-Sample.png](https://zh.wikipedia.org/wiki/File:Hidden-Markov-Model-Topology-Sample.png "fig:Hidden-Markov-Model-Topology-Sample.png")

状态个数的选择对于系统性能的影响是很大的。

## 建模单元选择

声学模型的建模单元的选择需要考虑三方面的因素。其一是该单元的可训练性，亦即是否能够得到足够的语料对每个单元进行训练，以及训练所需要的时间长短是否可接受。其二是该单元的可推广性，当语音识别系统所针对的词汇集\(\mathcal{W}\)发生变化时，原有建模单元是否能够不加修改的满足新的词汇集\(\mathcal{W'}\)。最后还需要考虑建模的精确性。

根据时间尺度的长短，建模单元可以选择为句子，短语，词，音节，音子乃至更小的半音子。一般可以认为有这样的原则， 时间尺度越短的建模单元，其可训练性及推广性就越强，而时间尺度越长的单元，其精确性就越强。同时，可以看出，句子、 短语、词三个概念是语言学上的概念，而音节、音子则是语音学上的概念，一般来说，如果声学模型所针对的应用环境不是 确定词汇量的系统，那么采用语言学的概念的建模单元是不具备推广性的。

为了将[协同发音现象](https://zh.wikipedia.org/wiki/协同发音 "wikilink")()融入建模中，上下文相关的建模单元(, CD uinits)是一个很好的选择。其思路是，对于某个音子*ah*，根据上下文的不同将其拆分成不同的建模单元。例如，用*b-ah+d*表示*ah*在*b*之后，*d*之前发音的具体实现。上下文的选择方法有很多，最常见的是三音子建模单元，也就是考虑左上文右下文各一个音子，加上中心音子形成三音子对。

上下文相关建模大大提高了建模的准确性，但是同时也使得模型数量急剧膨胀，使得模型的可训练性大大降低。为了解决这一 问题，就需要引入某些聚类算法来减少模型中需要训练的参数。

## 聚类方法

为了解决模型参数过多的问题，可以使用某些聚类方法来减小模型中的参数数量，提高模型的可训练性。聚类可以在模型层次， 状态层次乃至混合高斯模型中每个混合的层次进行。可以将半连续隐马模型看作进行高斯混合进行聚类后的连续隐马模型。目前 应用最多的方法是对状态聚类的方法。其思路是，根据状态间混合高斯模型概率输出的相似性，将输出概率接近的状态聚合在 一起，以便对其的训练更加充分。聚类的方法有基于规则的方法和数据驱动方法两类。

聚类后的状态被称为Senone，每个Senone都是完整独立的高斯混合模型，它也是解码过程中的最基本单元\[3\]。

## 参数估计

传统上，参数估计使用的方法为[Baum-Welch算法](../Page/Baum-Welch算法.md "wikilink")，属于最大似然准则下的EM算法。目前研究者提出了多种区分性训练方法（仍然属于产生式模型，但使用区分性准则）进行训练，取得了较好的效果。

## 參考資料

<div class="references-small">

<references />

</div>

[Category:计算语言学](https://zh.wikipedia.org/wiki/Category:计算语言学 "wikilink") [Category:语音识别](https://zh.wikipedia.org/wiki/Category:语音识别 "wikilink")

1.  L.R. Rabiner, “A tutorial on Hidden Markov Models and selected applications in speech recognition”, in Proceedings of the IEEE, vol. 77, pp. 257–287, 1989
2.  D.A. Reynolds and R.C. Rose, “Robust text-independent speaker identification using Gaussian mixture speaker models”, IEEE Transaction on Speech Audio Process, vol. 3, pp. 72–83, 1995.
3.  K.F. Lee, Large-vocabulary speaker independent continuous speech recognition, the Sphinx system, Ph.D. thesis, Carnegie Mellon University, 1988.
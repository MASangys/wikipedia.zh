> 本文内容由[强化学习](https://zh.wikipedia.org/wiki/强化学习)转换而来。


**强化学习**（，簡稱）是[机器学习](../Page/机器学习.md "wikilink")中的一个领域，强调如何基于[环境而行动](https://zh.wikipedia.org/wiki/环境 "wikilink")，以取得最大化的预期利益。其灵感来源于心理学中的[行为主义](../Page/行为主义.md "wikilink")理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如[博弈论](../Page/博弈论.md "wikilink")、[控制论](../Page/控制论.md "wikilink")、[运筹学](https://zh.wikipedia.org/wiki/运筹学 "wikilink")、[信息论](../Page/信息论.md "wikilink")、仿真优化、[多主体系统学习](https://zh.wikipedia.org/wiki/多主体系统学习 "wikilink")、[群体智能](../Page/群体智能.md "wikilink")、[统计学](../Page/统计学.md "wikilink")以及[遗传算法](../Page/遗传算法.md "wikilink")。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在[最优控制理论中也有研究这个问题](https://zh.wikipedia.org/wiki/最优控制 "wikilink")，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在[经济学](../Page/经济学.md "wikilink")和[博弈论](../Page/博弈论.md "wikilink")中，强化学习被用来解释在[有限理性的条件下如何出现平衡](https://zh.wikipedia.org/wiki/有限理性 "wikilink")。

在机器学习问题中，环境通常被规范为[马尔可夫决策过程](https://zh.wikipedia.org/wiki/马尔可夫决策过程 "wikilink")（Markov decision processes，MDP），所以许多强化学习算法在这种情况下使用[动态规划](../Page/动态规划.md "wikilink")技巧。传统的技术和强化学习算法的主要区别是，后者不需要关于MDP的知识，而且针对无法找到确切方法的大规模MDP。\[1\]

强化学习和标准的[监督式学习之间的区别在于](https://zh.wikipedia.org/wiki/监督式学习 "wikilink")，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。强化学习中的“探索-遵从”的交换，在问题和有限MDP中研究得最多。

## 导论

基本的强化学习模型包括：

1.  环境状态的集合\(S\);
2.  动作的集合\(A\);
3.  在状态之间转换的规则；
4.  规定转换后“即时奖励”的规则；
5.  描述主体能够观察到什么的规则。

规则通常是[随机的](https://zh.wikipedia.org/wiki/随机 "wikilink")。主体通常可以观察即时奖励和最后一次转换。在许多模型中，主体被假设为可以观察现有的环境状态，这种情况称为“完全可观测”（full observability），反之则称为“部分可观测”（partial observability）。通常，主体被允许的动作是有限的，例如，在棋盤中棋子只能上、下、左、右移動，或是使用的钱不能多于所拥有的。

强化学习的主体与环境基于离散的时间步作用。在每一个时间\(t\)，主体接收到一个观测\(o_t\)，通常其中包含奖励\(r_t\)。然后，它从允许的集合中选择一个动作\(a_t\)，然后送出到环境中去。环境则变化到一个新的状态\(s_{t+1}\)，然后决定了和这个变化\((s_t,a_t,s_{t+1})\)相关联的奖励\(r_{t+1}\)。强化学习主体的目标，是得到尽可能多的奖励。主体选择的动作是其历史的函数，它也可以选择随机的动作。

将这个主体的表现和自始自终以最优方式行动的主体相比较，它们之间的行动差异产生了“悔过”的概念。如果要接近最优的方案来行动，主体必须根据它的长时间行动序列进行推理：例如，要最大化我的未来收入，我最好现在去上学，虽然这样行动的即时货币奖励为负值。

因此，强化学习对于包含长期反馈的问题比短期反馈的表现更好。它在许多问题上得到应用，包括[机器人控制](https://zh.wikipedia.org/wiki/机器人控制 "wikilink")、电梯调度、[电信](../Page/电信.md "wikilink")通讯、[双陆棋和](https://zh.wikipedia.org/wiki/双陆棋 "wikilink")[西洋跳棋](../Page/西洋跳棋.md "wikilink")。\[2\]

强化学习的强大能来源于两个方面：使用样本来优化行为，使用函数近似来描述复杂的环境。它们使得强化学习可以使用在以下的复杂环境中：

  - 模型的环境已知，且解析解不存在；
  - 仅仅给出环境的模拟模型（模拟优化方法的问题）<ref>

</ref>

  - 从环境中获取信息的唯一办法是和它互动。前两个问题可以被考虑为规划问题，而最后一个问题可以被认为是genuine learning问题。使用强化学习的方法，这两种规划问题都可以被转化为[机器学习](../Page/机器学习.md "wikilink")问题。

## 探索机制

强化学习需要比较聪明的探索机制，直接随机的对动作进行采样的方法性能比较差。虽然小规模的马氏过程已经被认识的比较清楚，这些性质很难在状态空间规模比较大的时候适用，这个时候相对简单的探索机制是更加现实的。

其中的一种方法就是 \(\epsilon\)-[貪婪演算法](https://zh.wikipedia.org/wiki/貪婪演算法 "wikilink")，这种方法会以比较大的概率(1-\(\epsilon\))去选择现在最好的动作。如果没有选择最优动作，就在剩下的动作中随机选择一个。\(\epsilon\) 在这里是一个可调节的参数，更小的 \(\epsilon\) 意味着算法会更加贪心。\[3\]

## 参考文献

{{-}}

[Category:機器學習](https://zh.wikipedia.org/wiki/Category:機器學習 "wikilink") [Category:人工智能](https://zh.wikipedia.org/wiki/Category:人工智能 "wikilink") [Category:機器學習演算法](https://zh.wikipedia.org/wiki/Category:機器學習演算法 "wikilink")

1.
2.  Sutton1998|Sutton and Barto 1998 Chapter 11
3.
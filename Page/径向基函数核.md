> 本文内容由[径向基函数核](https://zh.wikipedia.org/wiki/径向基函数核)转换而来。


在[机器学习](../Page/机器学习.md "wikilink")中，（**高斯**）**[径向基函数](../Page/径向基函数.md "wikilink")核**（），或称为**RBF核**，是一种常用的[核函数](../Page/支持向量机.md "wikilink")。它是[支持向量机](../Page/支持向量机.md "wikilink")[分类中最为常用的核函数](https://zh.wikipedia.org/wiki/统计分类 "wikilink")。\[1\]

关于两个样本**x**和**x**'的RBF核可表示为某个“输入空间”（input space）的特征向量，它的定义如下所示：\[2\]

\[K(\mathbf{x}, \mathbf{x'}) = \exp\left(-\frac{||\mathbf{x} - \mathbf{x'}||_2^2}{2\sigma^2}\right)\]

\(\textstyle||\mathbf{x} - \mathbf{x'}||_2^2\)可以看做两个特征向量之间的[平方欧几里得距离](https://zh.wikipedia.org/wiki/欧几里得距离#平方欧几里得距离 "wikilink")。\(\sigma\)是一个自由参数。一种等价但更为简单的定义是设一个新的参数\(\gamma\)，其表达式为\(\textstyle\gamma = \tfrac{1}{2\sigma^2}\)：

\[K(\mathbf{x}, \mathbf{x'}) = \exp(-\gamma||\mathbf{x} - \mathbf{x'}||_2^2)\]

因为RBF核函数的值随距离增大而减小，并介于0（极限）和1（当**x** = **x**'的时候）之间，所以它是一种现成的[相似性度量表示法](https://zh.wikipedia.org/wiki/相似性度量 "wikilink")。\[3\] 核的[特征空间有无穷多的维数](https://zh.wikipedia.org/wiki/特徵_\(机器学习\) "wikilink")；对于\(\sigma = 1\)，它的展开式为：\[4\]

\[\exp\left(-\frac{1}{2}||\mathbf{x} - \mathbf{x'}||_2^2\right) = \sum_{j=0}^\infty \frac{(\mathbf{x}^\top \mathbf{x'})^j}{j!} \exp\left(-\frac{1}{2}||\mathbf{x}||_2^2\right)
\exp\left(-\frac{1}{2}||\mathbf{x'}||_2^2\right)\]

## 近似

因为支持向量机和其他模型使用了，它在处理输入空间中大量的训练样本或含有大量特征的样本的时表现不是很好。所以，目前已经设计出了多种RBF核（或相似的其他核）的近似方法。\[5\] 典型的情况下，这些方法使用*z*(**x**)的形式，也就是用一个函数对一个与其他向量（例如支持向量机中的支持向量）无关的单向量进行变换，例如：

\[z(\mathbf{x})z(\mathbf{x'}) \approx \varphi(\mathbf{x})\varphi(\mathbf{x'}) = K(\mathbf{x}, \mathbf{x'})\]

其中\(\textstyle\varphi\)是RBF核中植入的隐式映射。

一种建构这样的*z*函数的方法，是对核函数作[傅里叶变换](../Page/傅里叶变换.md "wikilink")，然后从中随机抽出所需函数。\[6\]

## 参见

  - [多项式核](https://zh.wikipedia.org/wiki/多项式核 "wikilink")
  - [径向基函数网络](https://zh.wikipedia.org/wiki/径向基函数网络 "wikilink")

## 参考资料

[Category:机器学习](https://zh.wikipedia.org/wiki/Category:机器学习 "wikilink")

1.  Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard and Chih-Jen Lin (2010). *Training and testing low-degree polynomial data mappings via linear SVM*. J. Machine Learning Research **11**: 1471–1490.
2.  Vert, Jean-Philippe, Koji Tsuda, and Bernhard Schölkopf (2004). "A primer on kernel methods." Kernel Methods in Computational Biology.
3.
4.
5.  Andreas Müller (2012). [Kernel Approximations for Efficient SVMs (and other feature extraction methods)](http://peekaboo-vision.blogspot.de/2012/12/kernel-approximations-for-efficient.html).
6.  Ali Rahimi and Benjamin Recht (2007). Random features for large-scale kernel machines. Neural Information Processing Systems.
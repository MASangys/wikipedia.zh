**微分熵**是[消息理論中的一個概念](https://zh.wikipedia.org/wiki/消息理論 "wikilink")，是從以[離散隨機變數所計算出的夏農熵推廣](https://zh.wikipedia.org/wiki/随机变量#离散型随机变量 "wikilink")，以[連續型隨機變數計算所得之](https://zh.wikipedia.org/wiki/连续随机变量 "wikilink")[熵](../Page/熵_\(信息论\).md "wikilink")，微分熵與[離散隨機變數所計算出之夏農熵](https://zh.wikipedia.org/wiki/随机变量#离散型随机变量 "wikilink")，皆可代表描述一信息所需碼長的下界，然而，微分熵與夏農熵仍存在著某些相異的性質。

## 定義

令\(X\)為一[連續型隨機變數](https://zh.wikipedia.org/wiki/连续随机变量 "wikilink")，其[機率密度函數為](../Page/機率密度函數.md "wikilink")\(f_X(x)\)，其中\(X\)的[支撐集為](../Page/支撑集.md "wikilink")\(S=\{x\in X|f_X(x)>0}\\)。微分熵\(h_X(x)\):

\(h_X(x)=-\int_{S} f_X(x)log(f_X(x))dx\)。

與夏農熵為類比，計算夏農熵之算式中的\(\log\)通常以2為底，而微分熵為計算方便，常以\(ln\)計算後再轉換為\(log_2\)的結果。微分熵與夏農熵最大的不同點在於\(f_X(x)\)可為大於1的數值，此時可能會造成\(h_X(x)\)為負值，而夏農熵\(H_X(x)\)恆不為負。

例如，\(X\)為[均勻分布](../Page/連續型均勻分布.md "wikilink")\(U(0,a),a<1\)：

\(f_X(x)=\)\(1\over a\)\(; h_X(x)=-\int\limits_{0}^{a}\)\(1\over a\)\(ln\)\(1\over a\)\(dx\)

\(h_X(x)=ln(a)\)\(<0\)

## 相關計算

### [條件熵](../Page/条件熵.md "wikilink")

\(f(x,y)\)為\(X,Y\)之聯合機率密度函數，其條件熵為:

\(h(X|Y)=-\int f(x,y)log{f(x|y)}dxdy\)。

### [相對熵](../Page/相对熵.md "wikilink")

又稱**KL散度**（**Kullback–Leibler divergence)，**兩機率密度函數f、g的相對熵定義為:

\(D(f||g)=\int flog{f\over g}\)。

### [互信息](../Page/互信息.md "wikilink")

兩連續型隨機變數的聯合機率密度函數為\(f(x,y)\)，其[互信息](../Page/互信息.md "wikilink"):

\(I(X;Y)=D(f(x,y)||f(x)f(y))\)

廣義而言，我們可以將互信息定義在有限多個連續隨機變數值域的劃分。
可參考[連續互信息的量化](https://zh.wikipedia.org/wiki/互信息#連續互信息的量化 "wikilink")。

## 性質

### [相對熵恆正](https://zh.wikipedia.org/wiki/相对熵#特性 "wikilink")

與夏農相對熵性質相同，恆正。

\(-{\displaystyle D(f||g)=\int flog{g \over f}}\)

\(\leq log\int f{g \over f}\) (**[延森不等式](../Page/延森不等式.md "wikilink")**)

\(\leq 0\)。

### 鏈式法則

一次觀測所有隨機變數所測得的的[聯合熵](../Page/联合熵.md "wikilink")，與個別接收隨機變數後計算的[條件熵總和相同](../Page/条件熵.md "wikilink")，即觀測順序與間隔不影響微分熵。

\(h(X_1,X_2,...,X_n)=\sum_{k=1}^nh(X_i|X_1,X_2,...,X_{i-1})\)。

### 平移

隨機變數的平移不影響微分熵，因為固定的平移不會增加隨機變數的方差。

\(h(X+c)=h(X)\)

### 縮放

將隨機變數縮放會增加其[方差](../Page/方差.md "wikilink")，微分熵亦會隨之增加。

\(h(AX)=h(X)+log|det(A)|\)

### 上界

期望值為0，方差為\(\sigma ^2\)且值域為\(R\)之隨機變數\(X\)的微分熵，其上界為常態分佈\(N(0,\sigma ^2)\)的微分熵。

\(h(X)\leq{1\over2}log(2\pi e\sigma^2)\)

### 估計誤差

隨機變數\(X\)與其估計子\(\widehat{X}\)之均方誤差存在下界，當\(X\)為常態分佈且\(\widehat{X}\)為[無偏估計子時](../Page/估计量的偏差.md "wikilink")，等號成立。

\(E[(X-\widehat{X})^2]\geq {1\over{2\pi e}}e^{2h(X)}\)

## 漸進等分性

### [漸進等分性](https://zh.wikipedia.org/wiki/漸進等分性 "wikilink")

離散隨機變數的夏農熵中，[獨立同分布的隨機變數序列](../Page/独立同分布.md "wikilink")，在漸進等分性(Asymptotic
equipartition
property)之下其[機率質量函數](https://zh.wikipedia.org/wiki/機率質量函數 "wikilink")\(p(X_1,X_2,...,X_n)\)趨近於\(2^{-nH(X)}\)。

連續型隨機變數之漸進等分性：

\(-{1\over n}log(f(X_1,X_2,...,X_n))\rightarrow h(X)\)

### 典型集

典型集(Typical set)定義如下

\(A_\epsilon^{(n)}=\{(x_1,x_2,...,x_n)\in S^n:|-{1\over n}logf(x_1,x_2,...,x_n)-h(X)|\leq\epsilon}\\),\(\epsilon >0\)

### 體積

集合包含於\(R^n\),\(A\subset R^n\)，其體積(Volume)\(Vol(A)\)定義如下:

\(Vol(A)=\int\limits_{A} dx_1dx_2...dx_n\)。

典型集\(A_\epsilon^{(n)}\)的體積有以下性質:

1\.\(Vol(A_\epsilon^{(n)})\leq2^{n(h(X)+\epsilon)}\)

2\.\(Vol(A_\epsilon^{(n)})\geq(1-\epsilon)2^{n(h(X)-\epsilon)}\)

**證明**

1\.

由\(-{1\over n}log(f(X_1,X_2,...,X_n))\rightarrow h(X)\)，

可得：

\(1=\int_{S^n} f(x_1,x_2,...,x_n)dx_1dx_2...dx_n\)

\(\geq \int_{A_\epsilon^{(n)}} f(x_1,x_2,...,x_n)dx_1dx_2...dx_n\)

\(\geq \int_{A_\epsilon^{(n)}} 2^{-n(h(X)+\epsilon)}dx_1dx_2...dx_n\)

\(=2^{-n(h(X)+\epsilon)}\int_{A_\epsilon^{(n)}} dx_1dx_2...dx_n\)

\(=2^{-n(h(X)+\epsilon )}Vol(A_\epsilon^{(n)})\)

2\.

當n足夠大時，\(Pr(A_\epsilon^{(n)})>1-\epsilon\)，

因此：

\(1-\epsilon \leq \int_{A_\epsilon^{(n)}} f(x_1,x_2,...,x_n)dx_1dx_2...dx_n\)

\(\leq \int_{A_\epsilon^{(n)}} 2^{-n(h(X)-\epsilon)}dx_1dx_2...dx_n\)

\(=2^{-n(h(X)-\epsilon)} \int_{A_\epsilon^{(n)}}dx_1dx_2...dx_n\)

\(=2^{-n(h(X)-\epsilon)} Vol(A_\epsilon^{(n)})\)

## 量化

我們可以將[機率密度函數](../Page/機率密度函數.md "wikilink")[量化後](../Page/量化_\(信号处理\).md "wikilink")，以夏農熵來計算微分熵。首先將連續隨機變數X以\(\Delta\)分為數個區間，根據[均值定理](../Page/中值定理.md "wikilink")，\(x_i\)滿足：

\(f(x_i)\Delta=\int_{i\Delta}^{(i+1)\Delta}f(x)dx=p_i\)

量化後的隨機變數\(X^{\Delta}\):

\(X^{\Delta}=x_i, i \Delta \leq X <(i+1)\Delta\)

夏農熵為:

\(H(X^{\Delta})=-\sum_{-\infin}^{\infin}f(x_i)\Delta log(f(x_i))-log\Delta\)

意即，當\(\Delta\rightarrow0\)，\(h(f)=h(X)\)。

### 例子：

1\.

對X做n位元量化\(X\sim U(0,{1\over8})\)。

\(H(X^{\Delta})=-3+n\)

上式表示，若我們想得到n位元精確度，則需要n-3個位元來表示。

2\.

對X做n位元量化\(X\sim N(0,{\sigma}^2)\)。

\(H(X^{\Delta})={1\over2}log(2\pi e \sigma ^2)+n\)

上式表示，若我們想得到n位元精確度，需要\({1\over2}log(2\pi e \sigma ^2)+n\)個位元來表示。

## 最大熵

### 常態分佈

隨機變數\(X\)，\(X_N\)值域為\((-\infin,\infin)\)，方差為\(\sigma^2\)，\(X\)為任意分佈，\(X_N\)為常態分佈，機率密度函數分別為\(f(x),g(x)\)。

則\(h_X(X)\leq {1\over2}log(2\pi e\sigma^2)\)

證明:

\(\begin{align}
 0 & \leq D(f||g)\\
 &=\int f(x)log({f(x)\over{g(x)}})dx\\
 &= -h(X)-\int f(x)log(g(x))dx\\

 &= -h(X)+h(x)
\end{align}\)

其中，

\(\begin{align}
-\int _{-\infin}^{\infin} f(x)log(g(x))dx

 &= -\int _{-\infin}^{\infin} f(x)({1\over 2}log(2\pi\sigma^2)+{1\over 2}({{x-\mu}\over \sigma})^2)dx\\

 &= {1\over2}log(2\pi e\sigma^2)
 \end{align}\)

### 指數分佈

隨機變數\(X\)，\(Y\)值域為\((0,\infin)\)，期望值為\(\lambda\)，\(X\)為任意分佈，\(Y\)為指數分佈，機率密度函數分別為\(f(x),g(x)\)。

則\(h_X(X)\leq 1+log\lambda\)。

證明:

\(\begin{align}
 0 & \leq D(f||g)\\
 &=\int f(x)log({f(x)\over{g(x)}})dx\\
 &= -h(X)-\int f(x)log(g(x))dx\\

 &= -h(X)+h(Y)
\end{align}\)

其中，

\(\begin{align}
-\int \limits_{0}^{\infin} f(x)log(g(x))dy

 &= -\int \limits_{0}^{\infin} f(x)(log\lambda +{x\over \lambda})dx\\
 &= 1+log\lambda
 \end{align}\)

## 參考文獻

  - Thomas M. Cover, Joy A. Thomas, *Elements of Information Theory*,
    1991 John Wiley & Sons, Inc, 1971. ISBN 0-471-20061-1

[Category:微分代数](https://zh.wikipedia.org/wiki/Category:微分代数 "wikilink")
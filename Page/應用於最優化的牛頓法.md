[Newton_optimization_vs_grad_descent.svg](https://zh.wikipedia.org/wiki/File:Newton_optimization_vs_grad_descent.svg "fig:Newton_optimization_vs_grad_descent.svg")
(綠色) 與牛頓法 (紅色) 在求最小值問題上的比較 (帶有步長). 可見牛頓法根據曲率選擇了一條「捷徑」. \]\]

[牛頓法是](https://zh.wikipedia.org/wiki/牛頓法 "wikilink")[微積分學中](https://zh.wikipedia.org/wiki/微積分學 "wikilink"),
通過[疊代以求解](https://zh.wikipedia.org/wiki/疊代法 "wikilink")[可微函數](https://zh.wikipedia.org/wiki/可微函數 "wikilink")\(f\)的[零點的一種算法](https://zh.wikipedia.org/wiki/零點 "wikilink")
(即求\(x\)使得\(f(x)=0\)).
而在[最優化中](https://zh.wikipedia.org/wiki/最優化 "wikilink"),
牛頓法通常被運用於求解一個[二次可微函數](https://zh.wikipedia.org/wiki/光滑函數 "wikilink")\(f\)的[一階導數](https://zh.wikipedia.org/wiki/導數 "wikilink")\(f^\prime\)的零點
(即求\(x\)使得\(f^\prime(x)=0\)),
同時也是\(f\)的[駐點](https://zh.wikipedia.org/wiki/駐點 "wikilink").
因此從另一個角度而言，**應用於最優化的牛頓法**是搜索函數\(f(x)\)的最小值或最大值的一種算法。

一維問題的牛頓法主要步驟如下: 取一個點\(x_0\)為**初值**, 依如下公式**疊代**:

\[x_{n+1}=x_n-\frac{f^\prime(x_n)}{f^{\prime\prime}(x_n)},\] 直至滿足一定條件
(如\(f^\prime(x_n)=0\)或\(x_{n+1}-x_n<\varepsilon\),
其中\(\varepsilon\)為一個給定的足夠小的常數) 後, 算法終止。

## 方法描述

在一維問題中, 牛頓法將構造一個以\(x_0\)為首項,
[收斂到](https://zh.wikipedia.org/wiki/收斂 "wikilink")\(x^*\)的數列\(\{x_n\}\),
其中\(x^*\)使得\(f^\prime(x^*)=0\)成立.

\(f(x)\)在\(x=x_n\)處的二階[泰勒展開式](https://zh.wikipedia.org/wiki/泰勒展開式 "wikilink")\(f_T(x)\)為:

\[f_T(x)=f_T(x_n+\Delta x)\approx f(x_n)+f^\prime(x_n)\Delta x+\frac{1}{2}f^{\prime\prime}(x_n)\Delta x^2.\]
我們希望找到\(\Delta x\)使\(x_n+\Delta x\)為\(f_T(x)\)的一個駐點。則將上式對\(\Delta x\)進行求導:

\[0=\frac{\text{d}}{\text{d}\Delta x}(f(x_n)+f^\prime(x_n)\Delta x+\frac{1}{2}f^{\prime\prime}(x_n)\Delta x^2)=f^\prime(x_n)+f^{\prime\prime}(x_n)\Delta x.\]
上述方程的解\(\Delta x=-\frac{f^\prime(x_n)}{f^{\prime\prime}(x_n)}\)滿足

\[x_{n+1}=x_{n}+\Delta x=x_n-\frac{f^\prime(x_n)}{f^{\prime\prime}(x_n)}\]
收斂於\(f_T(x)\)的駐點\(x^*\).

## 幾何意義

牛頓法的幾何意義為:
在每一次疊代中，均以一個[二次函數去逼近](https://zh.wikipedia.org/wiki/二次函數 "wikilink")\(f(x)\).
具體而言: 在一維問題中，已知\(x_n\), \(f(x_n)\),
\(f^\prime(x_n)\)及\(f^{\prime\prime}(x_n)\),
設[二次函數表逹式為](https://zh.wikipedia.org/wiki/二次函數 "wikilink")\(ax^2+bx+c\),
依下列方程求解未知數\(a,\ b,\ c,\)

\[ax_n^2+bx_n+c=f(x_n),\]

\[2ax_n+b=f^\prime(x_n),\]

\[2a=f^{\prime\prime}(x_n).\]
然後[二次函數](https://zh.wikipedia.org/wiki/二次函數 "wikilink")\(ax^2+bx+c\)的[極值點即為下一個疊代點](https://zh.wikipedia.org/wiki/極值 "wikilink"),

\[x_{n+1}=-\frac{b}{2a}.\]

而在高維問題中,
上述的[極值點也可以是](https://zh.wikipedia.org/wiki/極值 "wikilink")[鞍點](../Page/鞍點.md "wikilink").
值得一提的是,
如果\(f(x)\)恰為一個[二次函數](https://zh.wikipedia.org/wiki/二次函數 "wikilink"),
則其[極值點只需一次疊代中即可找到](https://zh.wikipedia.org/wiki/極值 "wikilink").

## 高維問題求解

上述的一維問題的疊代法可以被推廣至多維問題.
只需將[導數替換為](https://zh.wikipedia.org/wiki/導數 "wikilink")[梯度](../Page/梯度.md "wikilink")
(\(\nabla f(x)\)),
並將二階導數的[倒數替換為](https://zh.wikipedia.org/wiki/倒數 "wikilink")[Hessian矩陣的](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[逆矩陣](https://zh.wikipedia.org/wiki/逆矩陣 "wikilink")
(\(\mathbf{H}f(x)\)), 即:

\[x_{n+1}=x_n-[\mathbf{H}f(x_n)]^{-1}\nabla f(x_n), n\ge 0.\] 通常,
使用牛頓法時會加入一個步長變量\(\gamma\in(0,1)\)作微調以使每一步疊代都滿足[Wolfe條件](https://zh.wikipedia.org/wiki/Wolfe條件 "wikilink"),
即,

\[x_{n+1}=x_n-\gamma[\mathbf{H}f(x_n)]^{-1}\nabla f(x_n).\]
這個方法被稱為**無約束牛頓法**, 通常用於第一步之後的疊代.

只要牛頓法適用,
其[收斂於最小值或最大值的速度均頗快於](https://zh.wikipedia.org/wiki/收斂 "wikilink")[最速下降法](https://zh.wikipedia.org/wiki/最速下降法 "wikilink").
事實上, 對於每一個極小值,
都存在一個[鄰域](https://zh.wikipedia.org/wiki/鄰域 "wikilink")\(N\)使得,
只要[Hessian矩陣是](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[可逆的且是一個關於](https://zh.wikipedia.org/wiki/可逆矩陣 "wikilink")\(x\in N\)的[Lipschitz連續函數](../Page/利普希茨連續.md "wikilink"),
以\(x_0\in N\)為初值,
步長\(\gamma=1\)的牛頓法是**[二次收斂](https://zh.wikipedia.org/wiki/收斂速度 "wikilink")**的.

求一個高維問題的[Hessian矩陣的](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[逆矩陣是一件頗費工夫的事情](https://zh.wikipedia.org/wiki/逆矩陣 "wikilink").
在實際應用中,
通常會用[向量](../Page/向量.md "wikilink")\(\Delta x=x_{n+1}-x_n\)作為[線性方程組](https://zh.wikipedia.org/wiki/線性方程組 "wikilink")

\[[\mathbf{H}f(x_n)]\Delta x=-\nabla f(x_n)\] 的解. 這個求解過程中,
透過使用各種矩陣分解方法同近似求解方法, 求解速度可以大大提升. 然而,
這些矩陣分解方法或近似求解方法的使用需要滿足一定條件; 例如,
[Cholesky分解同](https://zh.wikipedia.org/wiki/Cholesky分解 "wikilink")[共軛梯度法只有在](https://zh.wikipedia.org/wiki/共軛梯度法 "wikilink")\(\mathbf{H}f(x)\)是[正定矩陣時才適用](https://zh.wikipedia.org/wiki/正定矩陣 "wikilink").
這看似是一個限制, 但有時也能充當檢驗答案的工具; 例如, 在一個最小化問題 (\(\text{min  }f(x)\)) 中,
求出一個\(x^\prime\)使得\(\nabla f(x^\prime)=0\)但\(\mathbf{H}f(x)\)不是[正定矩陣](https://zh.wikipedia.org/wiki/正定矩陣 "wikilink"),
那麽\((x^\prime,f(x^\prime))\)只是\(f(x)\)的一個[鞍點而非極小值點](../Page/鞍點.md "wikilink").

另一方面, 一個有約束的問題的求解過程可能會遇到**當前解**陷入一個鞍點的情況,
這時的[Hessian矩陣是](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[對稱](../Page/對稱.md "wikilink")[不定的](https://zh.wikipedia.org/wiki/正定矩陣#負定、半定及不定矩陣 "wikilink");
此時則要使用其他方法,
例如[Cholesky分解的\(\mathbf{LDL}^\mathbf{T}\)變形或](https://zh.wikipedia.org/wiki/Cholesky分解 "wikilink")[共軛梯度法等的方法](https://zh.wikipedia.org/wiki/共軛梯度法 "wikilink"),
來疊代得出\(x_{n+1}\).

此外,
為規避求[Hessian矩陣的繁瑣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink"),
也存在多種[擬牛頓法](https://zh.wikipedia.org/wiki/擬牛頓法 "wikilink"),
通過調整[梯度以求出](../Page/梯度.md "wikilink")[Hessian矩陣的近似](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink").

如果[Hessian矩陣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")\(\mathbf{H}f(x)\)接近一個[奇異矩陣](https://zh.wikipedia.org/wiki/非奇異矩陣 "wikilink"),
則其逆矩陣會變得數值不穩定且疊代不會收斂. 此種情形下, 前人探索出了很多成功的方法來解決問題.
目標之一是通過引入**修正矩陣**\(B_n\)使得\(\mathbf{H}f(x_n):= \mathbf{H}f(x_n) + B_n\)是[對稱正定的](https://zh.wikipedia.org/wiki/正定矩陣 "wikilink").
其中一種方法是將\(\mathbf{H}f(x_n)\)[對角化](https://zh.wikipedia.org/wiki/對角化 "wikilink"),
選擇\(B_n\)使\(\mathbf{H}f(x_n) + B_n\)有相同的[特徵向量](https://zh.wikipedia.org/wiki/特徵向量 "wikilink"),
但每一個\(\mathbf{H}f(x_n)\)的負[特徵值都被替換成](https://zh.wikipedia.org/wiki/特徵值 "wikilink")\(\epsilon>0.\)

一個應用於[萊文貝格－馬夸特方法](https://zh.wikipedia.org/wiki/萊文貝格－馬夸特方法 "wikilink")
(其中用到了近似的[Hessian矩陣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink"))
的方法是引入一個帶係數的[單位矩陣](../Page/單位矩陣.md "wikilink")\(\mu\mathbf{I}\),
係數在每一步疊代中調整.
對於較大的\(\mu\)及較小的[Hessian矩陣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink"),
疊代將變得與以\(\mu^{-1}\)為步長的[最速下降法相似](https://zh.wikipedia.org/wiki/最速下降法 "wikilink"),
這將使得疊代收斂變慢,
但在[Hessian矩陣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[不定或](https://zh.wikipedia.org/wiki/正定矩陣#負定、半定及不定矩陣 "wikilink")[半定的情況下](https://zh.wikipedia.org/wiki/正定矩陣#負定、半定及不定矩陣 "wikilink"),
收斂更穩定.

## 參閱

  - [擬牛頓法](https://zh.wikipedia.org/wiki/擬牛頓法 "wikilink")
  - [最速下降法](https://zh.wikipedia.org/wiki/最速下降法 "wikilink")
  - [高斯–牛頓算法](https://zh.wikipedia.org/wiki/高斯–牛頓算法 "wikilink")
  - [萊文貝格－馬夸特方法](https://zh.wikipedia.org/wiki/萊文貝格－馬夸特方法 "wikilink")
  - [置信域方法](../Page/置信域方法.md "wikilink")
  - [最優化](https://zh.wikipedia.org/wiki/最優化 "wikilink")
  - [Nelder–Mead方法](https://zh.wikipedia.org/wiki/Nelder–Mead方法 "wikilink")

## 參考文獻

  -
  -
  -
  -
## 外部連結

  -
[Category:最佳化演算法](https://zh.wikipedia.org/wiki/Category:最佳化演算法 "wikilink")
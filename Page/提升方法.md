**提升方法**（Boosting），是一种可以用来减小[監督式學習](../Page/監督式學習.md "wikilink")中[偏差的](https://zh.wikipedia.org/wiki/偏差 "wikilink")[机器学习](../Page/机器学习.md "wikilink")算法。面對的问题是邁可·肯斯（Michael
Kearns）提出的：\[1\]一組“弱学习者”的集合能否生成一个“强学习者”？弱学习者一般是指一个分类器，它的结果只比随机分类好一点点；强学习者指分类器的结果非常接近真值。

## 提升算法

大多数提升算法包括由迭代使用弱学习分類器組成，並將其結果加入一個最終的成强学习分類器。加入的过程中，通常根据它们的分类准确率给予不同的权重。加和弱学习者之后，数据通常会被重新加权，来强化对之前分类错误数据点的分类。

一个经典的提升算法例子是[AdaBoost](../Page/AdaBoost.md "wikilink")。一些最近的例子包括[LPBoost](https://zh.wikipedia.org/wiki/LPBoost "wikilink")、[TotalBoost](https://zh.wikipedia.org/wiki/TotalBoost "wikilink")、[BrownBoost](https://zh.wikipedia.org/wiki/BrownBoost "wikilink")、[MadaBoost及](https://zh.wikipedia.org/wiki/MadaBoost "wikilink")[LogitBoost](https://zh.wikipedia.org/wiki/LogitBoost "wikilink")。许多提升方法可以在[AnyBoost框架下解释为在](https://zh.wikipedia.org/wiki/AnyBoost "wikilink")[函数空间](../Page/函数空间.md "wikilink")利用一个凸的误差函数作[梯度下降](https://zh.wikipedia.org/wiki/梯度下降 "wikilink")。

## 批评

2008年，[谷歌的菲利普](https://zh.wikipedia.org/wiki/谷歌 "wikilink")·隆（Phillip
Long）與[哥倫比亞大學的羅可](https://zh.wikipedia.org/wiki/哥倫比亞大學 "wikilink")·A·瑟維迪歐（Rocco
A.
Servedio）发表论文指出这些方法是有缺陷的：在训练集有错误的标记的情况下，一些提升算法雖會尝试提升这种样本点的正确率，但卻無法产生一个正确率大于1/2的模型。\[2\]

## 相關條目

  - [AdaBoost](../Page/AdaBoost.md "wikilink")
  - [随机森林](../Page/随机森林.md "wikilink")
  - [Logit模型](https://zh.wikipedia.org/wiki/Logit模型 "wikilink")
  - [人工神经网络](../Page/人工神经网络.md "wikilink")
  - [支持向量機](https://zh.wikipedia.org/wiki/支持向量機 "wikilink")
  - [机器学习](../Page/机器学习.md "wikilink")

## 实现

  - [Orange](https://zh.wikipedia.org/wiki/Orange_\(software\) "wikilink"),
    a free data mining software suite, module
    [Orange.ensemble](http://orange.biolab.si/docs/latest/reference/rst/Orange.ensemble)
  - [Weka](https://zh.wikipedia.org/wiki/Weka_\(machine_learning\) "wikilink")
    is a machine learning set of tools that offers variate
    implementations of boosting algorithms like AdaBoost and LogitBoost
  - R package
    [GBM](http://cran.r-project.org/web/packages/gbm/index.html)
    (Generalized Boosted Regression Models) implements extensions to
    Freund and Schapire's AdaBoost algorithm and Friedman's gradient
    boosting machine.
  - jboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and
    alternating decision trees

## 参考文献

### 腳註

### 其他參考資料

  - Yoav Freund and Robert E. Schapire (1997); [*A Decision-Theoretic
    Generalization of On-line Learning and an Application to
    Boosting*](http://www.cse.ucsd.edu/~yfreund/papers/adaboost.pdf),
    Journal of Computer and System Sciences, 55(1):119-139
  - Robert E. Schapire and Yoram Singer (1999); [*Improved Boosting
    Algorithms Using Confidence-Rated
    Predictors*](http://citeseer.ist.psu.edu/schapire99improved.html),
    Machine Learning, 37(3):297-336

## 外部链接

  - Robert E. Schapire (2003); [*The Boosting Approach to Machine
    Learning: An
    Overview*](http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf),
    MSRI (Mathematical Sciences Research Institute) Workshop on
    Nonlinear Estimation and Classification
  - [An up-to-date collection of papers on
    boosting](https://web.archive.org/web/20140419015925/http://www.cs.princeton.edu/~schapire/boost.html)

[Category:分类算法](https://zh.wikipedia.org/wiki/Category:分类算法 "wikilink")

1.  Michael Kearns (1988); [*Thoughts on Hypothesis
    Boosting*](http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf),
    Unpublished manuscript (Machine Learning class project, December
    1988)
2.  [Philip M. Long, Rocco A. Servedio, "Random Classiﬁcation Noise
    Defeats All Convex Potential
    Boosters"](http://www.phillong.info/publications/LS10_potential.pdf)
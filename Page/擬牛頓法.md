> 本文内容由[擬牛頓法](https://zh.wikipedia.org/wiki/擬牛頓法)转换而来。


**擬牛頓法**是一種以[牛頓法為基礎設計的](https://zh.wikipedia.org/wiki/牛頓法 "wikilink")，求解[非線性方程組或](https://zh.wikipedia.org/wiki/非線性方程組 "wikilink")[連續的](https://zh.wikipedia.org/wiki/連續函數 "wikilink")[最優化問題函數的](https://zh.wikipedia.org/wiki/最優化 "wikilink")[零點或](https://zh.wikipedia.org/wiki/零點 "wikilink")[極大、極小值的](https://zh.wikipedia.org/wiki/極值 "wikilink")[算法](../Page/算法.md "wikilink")。當[牛頓法中所要求計算的](https://zh.wikipedia.org/wiki/牛頓法 "wikilink")[雅可比矩陣或](https://zh.wikipedia.org/wiki/雅可比矩陣 "wikilink")[Hessian矩陣難以甚至無法計算時](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")，擬牛頓法便可派上用場。

## 搜索極值

與牛頓法相同, **擬牛頓法**是用一個[二次函數以近似](https://zh.wikipedia.org/wiki/二次函數 "wikilink")**目標函數**\(f(x)\). \(f(x)\)的二階[泰勒展開是](https://zh.wikipedia.org/wiki/泰勒展開 "wikilink")

\[f(x_k + \Delta x) \approx f(x_k) + \nabla f(x_k)^T \Delta x + \frac{1}{2} \Delta x^T B \Delta x.\] 其中, \(\nabla f\)表示\(f(x)\)的[梯度](../Page/梯度.md "wikilink"), \(B\)表示[Hessian矩陣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")\(\mathbf{H}[f(x)]\)的近似. 梯度\(\nabla f\)可進一步近似為下列形式

\[\nabla f(x_k + \Delta x) \approx \nabla f(x_k) + B \Delta x.\] 令上式等於\(0\), 計算出**Newton步長**\(\Delta x\),

\[\Delta x = - B^{-1} \nabla f(x_k).\] 然後構造\(\mathbf{H}[f(x)]\)的近似\(B\)滿足

\[\nabla f(x_k + \Delta x) = \nabla f(x_k) + B \Delta x.\] 上式稱作**割線方程組**. 但當\(f(x)\)是定義在多維空間上的函數時, 從該式計算\(B\)將成為一個*[不定問題](https://zh.wikipedia.org/wiki/不定方程組 "wikilink")* (未知數個數比方程式個數多). 此時, 構造\(B\), 根據**Newton步長**更新**當前解**的處理需要回歸到求解割線方程. 幾乎不同的擬牛頓法就有不同的選擇割線方程的方法. 而大多數的方法都假定\(B\)具有[對稱性](../Page/對稱矩陣.md "wikilink") (即滿足\(B=B^\text{T}\)). 另外, 下表所示的方法可用於求解\(B_{k+1}\); 在此, \(B_{k+1}\)於某些[範數與](https://zh.wikipedia.org/wiki/範數 "wikilink")\(B_k\)盡量接近. 即對於某些[正定矩陣](https://zh.wikipedia.org/wiki/正定矩陣 "wikilink")\(V\), 以以下方式更新\(B\):

\[B_{k+1} = \arg \min_B \| B - B_k \|_V.\] 近似[Hessian矩陣一般以](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[單位矩陣](../Page/單位矩陣.md "wikilink")等作為初期值\[1\]. 最優化問題的解\(x_k\)由根據近似所得的\(B_k\)計算出的**Newton步長**更新得出.

以下為該算法的總結:

  - \(\Delta x_k = -\alpha B_k^{-1} \nabla f(x_k)\)
  - \(x_{k+1} = x_k + \Delta x_k\)
  - 計算新一個疊代點下的[梯度](../Page/梯度.md "wikilink")\(\nabla f(x_{k+1})\)
  - 令\(y_k = \nabla f(x_{k+1}) - \nabla f(x_k)\)
  - 利用\(y_k\), 直接近似[Hessian矩陣的](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")[逆矩陣](https://zh.wikipedia.org/wiki/逆矩陣 "wikilink")\(B_{k+1}^{-1}\). 近似的方法如下表:

<table>
<thead>
<tr class="header">
<th><p>Method</p></th>
<th><p><span class="math inline"><em>B</em><sub><em>k</em> + 1</sub>=</span></p></th>
<th><p><span class="math inline"><em>H</em><sub><em>k</em> + 1</sub> = <em>B</em><sub><em>k</em> + 1</sub><sup> − 1</sup>=</span></p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><p><span class="math inline">$\left (I-\frac {y_k \, \Delta x_k^T} {y_k^T \, \Delta x_k} \right ) B_k \left (I-\frac {\Delta x_k y_k^T} {y_k^T \, \Delta x_k} \right )+\frac{y_k y_k^T} {y_k^T \, \Delta x_k}$</span></p></td>
<td><p><span class="math inline">$H_k + \frac {\Delta x_k \Delta x_k^T}{y_k^{T} \, \Delta x_k} - \frac {H_k y_k y_k^T H_k^T} {y_k^T H_k y_k}$</span></p></td>
</tr>
<tr class="even">
<td></td>
<td><p><span class="math inline">$B_k + \frac {y_k y_k^T}{y_k^{T} \Delta x_k} - \frac {B_k \Delta x_k (B_k \Delta x_k)^T} {\Delta x_k^{T} B_k \, \Delta x_k}$</span></p></td>
<td><p><span class="math inline">$\left (I-\frac {y_k \Delta x_k^T} {y_k^T \Delta x_k} \right )^T H_k \left (I-\frac { y_k \Delta x_k^T} {y_k^T \Delta x_k} \right )+\frac
{\Delta x_k \Delta x_k^T} {y_k^T \, \Delta x_k}$</span></p></td>
</tr>
<tr class="odd">
<td></td>
<td><p><span class="math inline">$B_k+\frac {y_k-B_k \Delta x_k}{\Delta x_k^T \, \Delta x_k} \, \Delta x_k^T$</span></p></td>
<td><p><span class="math inline">$H_{k}+\frac {(\Delta x_k-H_k y_k) \Delta x_k^T H_k}{\Delta x_k^T H_k \, y_k}$</span></p></td>
</tr>
<tr class="even">
<td><p>Broyden族</p></td>
<td><p><span class="math inline">(1 − <em>φ</em><sub><em>k</em></sub>)<em>B</em><sub><em>k</em> + 1</sub><sup><em>B</em><em>F</em><em>G</em><em>S</em></sup> + <em>φ</em><sub><em>k</em></sub><em>B</em><sub><em>k</em> + 1</sub><sup><em>D</em><em>F</em><em>P</em></sup>,   <em>φ</em> ∈ [0, 1]</span></p></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><p><span class="math inline">$B_{k}+\frac {(y_k-B_k \, \Delta x_k) (y_k-B_k \, \Delta x_k)^T}{(y_k-B_k \, \Delta x_k)^T \, \Delta x_k}$</span></p></td>
<td><p><span class="math inline">$H_{k}+\frac {(\Delta x_k-H_k y_k) (\Delta x_k-H_k y_k)^T}{(\Delta x_k-H_k y_k)^T y_k}$</span></p></td>
</tr>
</tbody>
</table>

## 與逆矩陣的關聯

若\(f\)是一個[凸](https://zh.wikipedia.org/wiki/凸函數 "wikilink")[二次函數](https://zh.wikipedia.org/wiki/二次函數 "wikilink")，且[Hessian矩陣](https://zh.wikipedia.org/wiki/Hessian矩陣 "wikilink")\(B\)[正定](https://zh.wikipedia.org/wiki/正定 "wikilink")，我們總是希望由擬牛頓法生成的矩陣\(H_k\)收斂於Hessian矩陣的逆\(H=B^{-1}\)。這是基於疊代值更新最小 (least-change update) 的擬牛頓法系列的一個實例。\[2\]

## 實現

擬牛頓法是現在普遍使用的一種最優化[算法](../Page/算法.md "wikilink"), 存在多種[程式語言的實現方法](https://zh.wikipedia.org/wiki/程式語言 "wikilink").

## 參見

  - [牛頓法](https://zh.wikipedia.org/wiki/牛頓法 "wikilink")
  - [應用於最優化的牛頓法](../Page/應用於最優化的牛頓法.md "wikilink")

## 參考文獻

<references />

[Category:最优化算法](https://zh.wikipedia.org/wiki/Category:最优化算法 "wikilink") [Category:求根算法](https://zh.wikipedia.org/wiki/Category:求根算法 "wikilink")

1.
2.  {{ cite arxiv | author1 = Robert Mansel Gower |author2= Peter Richtarik | title = Randomized Quasi-Newton Updates are Linearly Convergent Matrix Inversion Algorithms | date = 2015 |eprint=1602.01768| class = math.NA }}
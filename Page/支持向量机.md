在[机器学习](../Page/机器学习.md "wikilink")中，**支持向量机**（，常简称為**SVM**，又名**支持向量网络**\[1\]）是在[分类与](https://zh.wikipedia.org/wiki/分类问题 "wikilink")[迴歸分析](../Page/迴歸分析.md "wikilink")中分析数据的[監督式學習](../Page/監督式學習.md "wikilink")模型与相关的学习[算法](../Page/算法.md "wikilink")。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法建立一个将新的实例分配给两个类别之一的模型，使其成为非概率[线性分类器](../Page/线性分类器.md "wikilink")。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。

除了进行线性分类之外，SVM还可以使用所谓的有效地进行非线性分类，将其输入隐式映射到高维特征空间中。

当数据未被标记时，不能进行监督式学习，需要用[非監督式學習](https://zh.wikipedia.org/wiki/非監督式學習 "wikilink")，它会尝试找出数据到簇的自然聚类，并将新数据映射到这些已形成的簇。将支持向量机改进的聚类算法被称为**支持向量聚类**\[2\]，当数据未被标记或者仅一些数据被标记时，支持向量聚类经常在工业应用中用作分类步骤的预处理。

## 动机

[Svm_separating_hyperplanes_(SVG).svg](https://zh.wikipedia.org/wiki/File:Svm_separating_hyperplanes_\(SVG\).svg "fig:Svm_separating_hyperplanes_(SVG).svg") [分类数据是](https://zh.wikipedia.org/wiki/分类问题 "wikilink")[机器学习](../Page/机器学习.md "wikilink")中的一项常见任务。 假设某些给定的数据点各自属于两个类之一，而目标是确定新数据点将在哪个类中。对于支持向量机来说，数据点被视为 \(p\) 维向量，而我们想知道是否可以用 \((p-1)\) 维[超平面来分开这些点](https://zh.wikipedia.org/wiki/超平面 "wikilink")。这就是所谓的[线性分类器](../Page/线性分类器.md "wikilink")。可能有许多超平面可以把数据分类。最佳超平面的一个合理选择是以最大间隔把两个类分开的超平面。因此，我们要选择能够让到每边最近的数据点的距离最大化的超平面。如果存在这样的超平面，则称为最大间隔超平面，而其定义的线性分类器被称为最大，或者叫做最佳稳定性[感知器](../Page/感知器.md "wikilink")。''

## 定义

更正式地来说，支持向量机在[高维或无限维空间中构造](../Page/維度.md "wikilink")[超平面或超平面集合](https://zh.wikipedia.org/wiki/超平面 "wikilink")，其可以用于分类、回归或其他任务。直觀來說，分類邊界距離最近的訓練資料點越遠越好，因為這樣可以缩小分類器的[泛化誤差](https://zh.wikipedia.org/wiki/泛化誤差 "wikilink")。

[Kernel_Machine.png](https://zh.wikipedia.org/wiki/File:Kernel_Machine.png "fig:Kernel_Machine.png") 尽管原始问题可能是在有限维空间中陈述的，但用于区分的集合在该空间中往往。为此，有人提出将原有限维空间映射到维数高得多的空间中，在该空间中进行分离可能会更容易。为了保持计算负荷合理，人们选择适合该问题的 \(k(x,y)\) 来定义SVM方案使用的映射，以确保用原始空间中的变量可以很容易计算[点积](https://zh.wikipedia.org/wiki/数量积 "wikilink")。\[3\] 高维空间中的超平面定义为与该空间中的某向量的点积是常数的点的集合。定义超平面的向量可以选择在数据基中出现的特征向量 \(x_i\) 的图像的参数 \(\alpha_i\) 的线性组合。通过选择超平面，被映射到超平面上的特征空间中的点集 \(x\) 由以下关系定义：\(\textstyle\sum_i \alpha_i k(x_i,x) = \mathrm{constant}.\) 注意，如果随着 \(y\) 逐渐远离 \(x\)，\(k(x,y)\) 变小，则求和中的每一项都是在衡量测试点 \(x\) 与对应的数据基点 \(x_i\) 的接近程度。这样，上述内核的总和可以用于衡量每个测试点相对于待分离的集合中的数据点的相对接近度。

## 应用

  - 用于文本和超文本的分类，在归纳和直推方法中都可以显著减少所需要的有类标的样本数。
  - 用于图像分类。实验结果显示：在经过三到四轮相关反馈之后，比起传统的查询优化方案，支持向量机能够取得明显更高的搜索准确度。这同样也适用于图像分割系统，比如使用Vapnik所建议的使用特权方法的修改版本SVM的那些图像分割系统。\[4\]\[5\]
  - 用于手写字体识别。
  - 用于医学中分类蛋白质，超过90%的化合物能够被正确分类。基于支持向量机权重的置换测试已被建议作为一种机制，用于解释的支持向量机模型。\[6\]\[7\] 支持向量机权重也被用来解释过去的SVM模型。\[8\] 为识别模型用于进行预测的特征而对支持向量机模型做出事后解释是在生物科学中具有特殊意义的相对较新的研究领域。

## 历史

原始SVM算法是由[弗拉基米尔·万普尼克](../Page/弗拉基米尔·万普尼克.md "wikilink")和[亞歷克塞·澤范蘭傑斯于](https://zh.wikipedia.org/wiki/亞歷克塞·澤范蘭傑斯 "wikilink")1963年发明的。1992年，Bernhard E. Boser、Isabelle M. Guyon和[弗拉基米尔·万普尼克](../Page/弗拉基米尔·万普尼克.md "wikilink")提出了一种通过将核技巧应用于最大间隔超平面来创建非线性分类器的方法。\[9\] 当前标准的前身（软间隔）由Corinna Cortes和Vapnik于1993年提出，并于1995年发表。\[10\]

## 线性SVM

我们考虑以下形式的 \(n\) 点测试集：

  -
    \((\vec{x}_1, y_1),\, \ldots ,\, (\vec{x}_n, y_n)\)

其中 \(y_i\) 是 1 或者 −1，表明点 \(\vec{x}_i\) 所属的类。\(\vec{x}_i\) 中每个都是一个 \(p\) 维[实向量](../Page/实数.md "wikilink")。我们要求将 \(y_i=1\) 的点集 \(\vec{x}_i\) 与 \(y_i=-1\) 的点集分开的 “最大间隔超平面”，使得超平面与最近的点 \(\vec{x}_i\) 之间的距离最大化。

任何超平面都可以写作满足下面方程的点集 \(\vec{x}\)

  -
    \(\vec{w}\cdot\vec{x} - b=0,\,\) [Svm_max_sep_hyperplane_with_margin.png](https://zh.wikipedia.org/wiki/File:Svm_max_sep_hyperplane_with_margin.png "fig:Svm_max_sep_hyperplane_with_margin.png")

其中 \({\vec{w}}\)（不必是归一化的）是该[法向量](../Page/法线.md "wikilink")。参数 \(\tfrac{b}{\|\vec{w}\|}\) 决定从原点沿法向量 \({\vec{w}}\) 到超平面的偏移量。

### 硬间隔

如果这些训练数据是线性可分的，可以选择分离两类数据的两个平行超平面，使得它们之间的距离尽可能大。在这两个超平面范围内的区域称为“间隔”，最大间隔超平面是位于它们正中间的超平面。这些超平面可以由方程：

  -
    \(\vec{w}\cdot\vec{x} - b=1\,\)

或是

  -
    \(\vec{w}\cdot\vec{x} - b=-1.\,\)

来表示。通过几何不难得到这两个超平面之间的距离是 \(\tfrac{2}{\|\vec{w}\|}\)，因此要使两平面间的距离最大，我们需要最小化 \(\|\vec{w}\|\)。同时为了使得样本数据点都在超平面的间隔区以外，我们需要保证对于所有的 \(i\) 满足其中的一个条件：

  -
    \(\vec{w}\cdot\vec{x}_i - b \ge 1,\) 若 \(y_i = 1\)

或是

  -
    \(\vec{w}\cdot\vec{x}_i - b \le -1,\) 若 \(y_i = -1.\)

这些约束表明每个数据点都必须位于间隔的正确一侧。

这两个式子可以写作：

  -
    \(y_i(\vec{w}\cdot\vec{x}_i - b) \ge 1, \quad \text{ for all } 1 \le i \le n.\qquad\qquad(1)\)

可以用这个式子一起来得到优化问题：

> “在 \(y_i(\vec{w}\cdot\vec{x_i} - b) \ge 1\) 条件下，最小化 \(\|\vec{w}\|\)，对于 \(i = 1,\,\ldots,\,n\)"

这个问题的解 \(\vec w\) 与 \(b\) 决定了我们的分类器 \(\vec{x} \mapsto \sgn(\vec{w} \cdot \vec{x} - b)\)。

此几何描述的一个显而易见却重要的结果是，最大间隔超平面完全是由最靠近它的那些 \(\vec{x}_i\) 确定的。这些 \(\vec{x}_i\) 叫做**支持向量**。

### 软间隔

为了将SVM扩展到数据线性不可分的情况，我们引入**铰链损失**函数，

> \(\max\left(0, 1-y_i(\vec{w}\cdot\vec{x_i} - b)\right).\)

当约束条件 (1) 满足时（也就是如果 \(\vec{x}_i\) 位于边距的正确一侧）此函数为零。对于间隔的错误一侧的数据，该函数的值与距间隔的距离成正比。

然后我们希望最小化

> \(\left[\frac 1 n \sum_{i=1}^n \max\left(0, 1 - y_i(\vec{w}\cdot \vec{x_i} - b)\right) \right] + \lambda\lVert \vec{w} \rVert^2,\)

其中参数 \(\lambda\) 用来权衡增加间隔大小与确保 \(\vec{x}_i\) 位于间隔的正确一侧之间的关系。因此，对于足够小的 \(\lambda\) 值，如果输入数据是可以线性分类的，则软间隔SVM与硬间隔SVM将表现相同，但即使不可线性分类，仍能学习出可行的分类规则。

## 非线性分类

[Kernel_Machine.png](https://zh.wikipedia.org/wiki/File:Kernel_Machine.png "fig:Kernel_Machine.png") 万普尼克在1963年提出的原始最大间隔超平面算法构造了一个[线性分类器](../Page/线性分类器.md "wikilink")。而1992年，[Bernhard E. Boser](https://zh.wikipedia.org/wiki/Bernhard_E._Boser "wikilink")、[Isabelle M. Guyon和](https://zh.wikipedia.org/wiki/Isabelle_M._Guyon "wikilink")[弗拉基米尔·万普尼克](../Page/弗拉基米尔·万普尼克.md "wikilink")提出了一种通过将（最初由Aizerman et al. \[11\]提出）应用于最大边界超平面来创建非线性分类器的方法。\[12\] 所得到的算法形式上类似，除了把[点积换成了非线性](https://zh.wikipedia.org/wiki/数量积 "wikilink")[核函数](../Page/积分变换.md "wikilink")。这就允许算法在变换后的[特征空间中拟合最大间隔超平面](https://zh.wikipedia.org/wiki/特征空间 "wikilink")。该变换可以是非线性的，而变换空间是高维的；虽然分类器是变换后的特征空间中的超平面，但它在原始输入空间中可以是非线性的。

值得注意的是，更高维的特征空间增加了支持向量机的[泛化误差](https://zh.wikipedia.org/wiki/泛化误差 "wikilink")，但给定足够多的样本，算法仍能表现良好。\[13\]

常见的核函数包括：

  - [齊次多項式](https://zh.wikipedia.org/wiki/齊次多項式 "wikilink")：\(k(\vec{x_i},\vec{x_j})=(\vec{x_i} \cdot \vec{x_j})^d\)

  - ：\(k(\vec{x_i},\vec{x_j})=(\vec{x_i} \cdot \vec{x_j} + 1)^d\)

  - 高斯[径向基函数](../Page/径向基函数核.md "wikilink")：\(k(\vec{x_i},\vec{x_j})=\exp(-\gamma \|\vec{x_i} - \vec{x_j}\|^2)\)，其中 \(\gamma > 0\)。有时参数化表示 \(\gamma=1/{2 \sigma^2}\)

  - [双曲正切](../Page/双曲函数.md "wikilink")：\(k(\vec{x_i},\vec{x_j})=\tanh(\kappa \vec{x_i} \cdot \vec{x_j}+c)\)，其中一些（而非所有）\(\kappa > 0\) 且 \(c < 0\)

由等式 \(k(\vec{x_i}, \vec{x_j}) = \varphi(\vec{x_i})\cdot \varphi(\vec{x_j})\)，核函数与变换 \(\varphi(\vec{x_i})\) 有关。变换空间中也有 **w** 值，\(\textstyle\vec{w} = \sum_i \alpha_i y_i \varphi(\vec{x}_i)\)。与 **w** 的点积也要用核技巧来计算，即 \(\textstyle \vec{w}\cdot\varphi(\vec{x}) = \sum_i \alpha_i y_i k(\vec{x}_i, \vec{x})\)。

## 计算SVM分类器

计算（软间隔）SVM分类器等同于使下面表达式最小化

> \(\left[\frac 1 n \sum_{i=1}^n \max\left(0, 1 - y_i(w\cdot x_i + b)\right) \right] + \lambda\lVert w \rVert^2. \qquad(2)\)

如上所述，由于我们关注的是软间隔分类器，\(\lambda\) 选择足够小的值就能得到线性可分类输入数据的硬间隔分类器。下面会详细介绍将(2)简化为[二次规划](../Page/二次规划.md "wikilink")问题的经典方法。之后会讨论一些最近才出现的方法，如次梯度下降法和坐标下降法。

### 原型

最小化(2)可以用下面的方式改写为目标函数可微的约束优化问题。

对所有 \(i \in \{1,\,\ldots,\,n\}\) 我们引入变量 \(\zeta_i = \max\left(0, 1 - y_i(w\cdot x_i + b)\right)\)。注意到 \(\zeta_i\) 是满足 \(y_i(w\cdot x_i + b) \geq 1- \zeta_i\) 的最小非负数。

因此，我们可以将优化问题叙述如下

> \(\text{minimize } \frac 1 n \sum_{i=1}^n \zeta_i + \lambda\|w\|^2\)

> \(\text{subject to } y_i(x_i \cdot w + b) \geq 1 - \zeta_i \,\text{ and }\,\zeta_i \geq 0,\,\text{for all }i.\)

这就叫做*原型*问题。

### 对偶型

通过求解上述问题的，得到简化的问题

> \(\text{maximize}\,\, f(c_1 \ldots c_n) =  \sum_{i=1}^n c_i - \frac 1 2 \sum_{i=1}^n\sum_{j=1}^n y_ic_i(x_i \cdot x_j)y_jc_j,\)

> \(\text{subject to } \sum_{i=1}^n c_iy_i = 0,\,\text{and } 0 \leq c_i \leq \frac{1}{2n\lambda}\;\text{for all }i.\)

这就叫做*对偶*问题。由于对偶最小化问题是受线性约束的 \(c_i\) 的二次函数，所以它可以通过[二次规划](../Page/二次规划.md "wikilink")算法高效地解出。

这里，变量 \(c_i\) 定义为满足

> \(\vec w = \sum_{i=1}^n c_iy_i \vec x_i\).

此外，当 \(\vec x_i\) 恰好在间隔的正确一侧时 \(c_i = 0\)，且当 \(\vec x_i\) 位于间隔的边界时 \(0 < c_i <(2n\lambda)^{-1}\)。因此，\(\vec w\) 可以写为支持向量的线性组合。

可以通过在间隔的边界上找到一个 \(\vec x_i\) 并求解

> \(y_i(\vec w \cdot \vec x_i + b) = 1 \iff b = y_i - \vec w \cdot \vec x_i.\)

得到偏移量 \(b\)。（注意由于 \(y_i=\pm 1\) 因而 \(y_i^{-1}=y_i\)。）

### 核技巧

假设我们要学习与变换后数据点 \(\varphi(\vec x_i)\) 的线性分类规则对应的非线性分类规则。此外，我们有一个满足 \(k(\vec x_i, \vec x_j) = \varphi(\vec x_i) \cdot \varphi(\vec x_j)\) 的核函数 \(k\)。

我们知道变换空间中的分类向量 \(\vec w\) 满足

> \(\vec w = \sum_{i=1}^n c_iy_i\varphi( \vec x_i),\)

其中 \(c_i\) 可以通过求解优化问题

> \(\begin{align}
> \text{maximize}\,\, f(c_1 \ldots c_n) &=  \sum_{i=1}^n c_i - \frac 1 2 \sum_{i=1}^n\sum_{j=1}^n y_ic_i(\varphi(\vec x_i) \cdot \varphi(\vec x_j))y_jc_j \\
>                                       &=  \sum_{i=1}^n c_i - \frac 1 2 \sum_{i=1}^n\sum_{j=1}^n y_ic_ik(\vec x_i,\vec x_j)y_jc_j \\
> \end{align}\)

> \(\text{subject to } \sum_{i=1}^n c_iy_i = 0,\,\text{and } 0 \leq c_i \leq \frac{1}{2n\lambda}\;\text{for all }i.\)

得到。与前面一样，可以使用二次规划来求解系数 \(c_i\)。同样，我们可以找到让 \(0 < c_i <(2n\lambda)^{-1}\) 的索引 \(i\)，使得 \(\varphi(\vec x_i)\) 位于变换空间中间隔的边界上，然后求解

> \(\begin{align}
> b = \vec w \cdot \varphi(\vec x_i) - y_i &= \left[\sum_{k=1}^n c_ky_k\varphi(\vec x_k)\cdot\varphi(\vec x_i)\right] - y_i \\
>   &= \left[\sum_{k=1}^n c_ky_kk(\vec x_k, \vec x_i)\right] - y_i.
> \end{align}\)

最后，可以通过计算下式来分类新点

> \(\vec z \mapsto \sgn(\vec w \cdot \varphi(\vec z) + b) = \sgn\left(\left[\sum_{i=1}^n c_iy_ik(\vec x_i, \vec z)\right] + b\right).\)

### 现代方法

用于找到SVM分类器的最近的算法包括次梯度下降和坐标下降。当处理大的稀疏数据集时，这两种技术已经被证明有着显著的优点——当存在许多训练实例时次梯度法是特别有效的，并且当特征空间的维度高时，坐标下降特别有效。

#### 次梯度下降

SVM的[次梯度下降算法直接用表达式](../Page/次梯度法.md "wikilink")

> \(f(\vec w, b) = \left[\frac 1 n \sum_{i=1}^n \max\left(0, 1 - y_i(w\cdot x_i + b)\right) \right] + \lambda\lVert w \rVert^2.\)

注意 \(f\) 是 \(\vec w\) 与 \(b\) 的[凸函数](../Page/凸函数.md "wikilink")。因此，可以采用传统的[梯度下降](https://zh.wikipedia.org/wiki/梯度下降法 "wikilink")（或）方法，其中不是在函数梯度的方向上前进，而是在从函数的[次梯度中选出的向量的方向上前进](../Page/次导数.md "wikilink")。该方法的优点在于，对于某些实现，迭代次数不随着数据点的数量 \(n\) 而增加或减少。\[14\]

#### 坐标下降

SVM的[坐标下降算法基于对偶问题](../Page/坐标下降法.md "wikilink")

\(\text{maximize}\,\, f(c_1 \ldots c_n) =  \sum_{i=1}^n c_i - \frac 1 2 \sum_{i=1}^n\sum_{j=1}^n y_ic_i(x_i \cdot x_j)y_jc_j,\)

> \(\text{subject to } \sum_{i=1}^n c_iy_i = 0,\,\text{and } 0 \leq c_i \leq \frac{1}{2n\lambda}\;\text{for all }i.\)

对所有 \(i \in \{1,\, \ldots,\, n\}\) 进行迭代，使系数 \(c_i\) 的方向与 \(\partial f/ \partial c_i\) 一致。然后，将所得的系数向量 \((c_1',\,\ldots,\,c_n')\) 投影到满足给定约束的最接近的系数向量。（通常使用欧氏距离。）然后重复该过程，直到获得接近最佳的系数向量。所得的算法在实践中运行非常快，尽管已经证明的性能保证很少。\[15\]

## 性质

SVM属于广义[线性分类器](../Page/线性分类器.md "wikilink")的一族，并且可以解释为[感知器](../Page/感知器.md "wikilink")的延伸。它们也可以被认为是[吉洪诺夫正则化的特例](https://zh.wikipedia.org/wiki/吉洪诺夫正则化 "wikilink")。它们有一个特别的性质，就是可以同时最小化经验误差和最大化几何边缘区; 因此它们也被称为**最大间隔分类器**。

Meyer、Leisch和Hornik对SVM与其他分类器进行了比较。\[16\]

### 参数选择

SVM的有效性取决于核函数、核参数和软间隔参数 *C* 的选择。 通常会选只有一个参数 *\(\gamma\)* 的高斯核。*C* 和 \(\gamma\) 的最佳组合通常通过在 *C* 和 *\(\gamma\)* 为指数增长序列下来选取，例如 \(C \in \{ 2^{-5}, 2^{-3}, \dots, 2^{13},2^{15} \}\); \(\gamma \in \{ 2^{-15},2^{-13}, \dots, 2^{1},2^{3} \}\)。通常情况下，使用[交叉驗證](../Page/交叉驗證.md "wikilink")来检查参数选择的每一个组合，并选择具有最佳交叉验证精度的参数。或者，最近在贝叶斯优化中的工作可以用于选择C和γ，通常需要评估比网格搜索少得多的参数组合。或者，的最近进展可以用于选择 *C* 和 *\(\gamma\)*，通常需要计算的参数组合比网格搜索少得多。然后，使用所选择的参数在整个训练集上训练用于测试和分类新数据的最终模型。\[17\]

### 问题

SVM的潜在缺点包括以下方面：

  - 需要对输入数据进行完全标记
  - 未校准类成员概率
  - SVM仅直接适用于两类任务。因此，必须应用将多类任务减少到几个二元问题的算法；请参阅多类SVM一节。
  - 解出的模型的参数很难理解。

## 延伸

  - 支持向量聚類：支持向量聚類是一種建立在核函數上的類似方法，同適用於非監督學習和數據挖掘。它被認為是數據科學中的一種基本方法。
  - 轉導支持向量機
  - 多元分類支持向量機：SVM算法最初是為二值分類問題設計的，實現多分類的主要方法是將一個多分類問題轉化為多個二分類問題。常見方法包括“一對多法”和“一對一法”，一對多法是將某個類別的樣本歸為一類,其他剩餘的樣本歸為另一類，這樣k個類別的樣本就構造出了k個二分類SVM；一對一法則是在任意兩類樣本之間設計一個SVM。
  - 支持向量回歸
  - 結構化支持向量機：支持向量機可以被推廣為結構化的支持向量機，推廣後標籤空間是結構化的並且可能具有無限的大小。

## 实现

最大间隔超平面的参数是通过求解优化得到的。有几种专门的算法可用于快速解决由SVM产生的[QP问题](../Page/二次规划.md "wikilink")，它们主要依靠启发式算法将问题分解成更小、更易于处理的子问题。

另一种方法是使用，其使用类似[牛顿法](../Page/牛顿法.md "wikilink")的迭代找到[卡羅需－庫恩－塔克條件](../Page/卡羅需－庫恩－塔克條件.md "wikilink")下原型和对偶型的解。\[18\] 这种方法不是去解决一系列分解问题，而是直接完全解决该问题。为了避免求解核矩阵很大的线性系统，在核技巧中经常使用矩阵的低秩近似。

另一个常见的方法是普莱特的[序列最小优化算法](../Page/序列最小优化算法.md "wikilink")（SMO），它把问题分成了若干个可以解析求解的二维子问题，这样就可以避免使用数值优化算法和矩阵存储。\[19\]

线性支持向量机的特殊情况可以通过用于优化其类似问题[邏輯迴歸](../Page/邏輯迴歸.md "wikilink")的同类算法更高效求解；这类算法包括次梯度下降法（如PEGASOS\[20\]）和[坐标下降法](../Page/坐标下降法.md "wikilink")（如LIBLINEAR\[21\]）。LIBLINEAR有一些引人注目的训练时间上的特性。每次收敛迭代花费在读取训练数据上的时间是线性的，而且这些迭代还具有特性，使得算法非常快。

一般的核SVM也可以用次梯度下降法（P-packSVM\[22\]）更快求解，在允许并行化时求解速度尤其快。

许多机器学习工具包都可以使用核SVM，有、[MATLAB](../Page/MATLAB.md "wikilink")、[SAS](http://support.sas.com/documentation/cdl/en/whatsnew/64209/HTML/default/viewer.htm#emdocwhatsnew71.htm)、SVMlight、[kernlab](https://cran.r-project.org/package=kernlab)、、、[Weka](https://zh.wikipedia.org/wiki/Weka "wikilink")、[Shark](http://image.diku.dk/shark/)、[JKernelMachines](https://mloss.org/software/view/409/)、OpenCV等。

## 参见

  -
  -
  -
  - [预测分析](https://zh.wikipedia.org/wiki/预测分析 "wikilink")

  - [相关向量机](../Page/相关向量机.md "wikilink")，函数形式与SVM相同的概率稀疏核模型

  - [序列最小优化算法](../Page/序列最小优化算法.md "wikilink")

  -
## 参考文献

### 引用

### 来源

  - Theodoridis, Sergios; and Koutroumbas, Konstantinos; "Pattern Recognition", 4th Edition, Academic Press, 2009, ISBN 978-1-59749-272-0
  - Cristianini, Nello; and Shawe-Taylor, John; *[An Introduction to Support Vector Machines and other kernel-based learning methods](https://web.archive.org/web/20180627015707/https://www.support-vector.net/)*, Cambridge University Press, 2000. ISBN 0-521-78019-5 (SVM Book)
  - Huang, Te-Ming; Kecman, Vojislav; and Kopriva, Ivica (2006); *[Kernel Based Algorithms for Mining Huge Data Sets](http://learning-from-data.com)*, in *Supervised, Semi-supervised, and Unsupervised Learning*, Springer-Verlag, Berlin, Heidelberg, 260 pp. 96 illus., Hardcover, ISBN 3-540-31681-7
  - Kecman, Vojislav; *[Learning and Soft Computing — Support Vector Machines, Neural Networks, Fuzzy Logic Systems](http://www.support-vector.ws)*, The MIT Press, Cambridge, MA, 2001.
  - Schölkopf, Bernhard; and Smola, Alexander J.; *Learning with Kernels*, MIT Press, Cambridge, MA, 2002. ISBN 0-262-19475-9
  - Schölkopf, Bernhard; Burges, Christopher J. C.; and Smola, Alexander J. (editors); *[Advances in Kernel Methods: Support Vector Learning](https://web.archive.org/web/20071017030144/http://kernel-machines.org/nips97/book.html)*, MIT Press, Cambridge, MA, 1999. ISBN 0-262-19416-3.
  - Shawe-Taylor, John; and Cristianini, Nello; *[Kernel Methods for Pattern Analysis](http://www.kernel-methods.net)*, Cambridge University Press, 2004. ISBN 0-521-81397-2 *(Kernel Methods Book)*
  - Steinwart, Ingo; and Christmann, Andreas; *[Support Vector Machines](https://web.archive.org/web/20120220084913/http://www.staff.uni-bayreuth.de/~btms01/svm.html)*, Springer-Verlag, New York, 2008. ISBN 978-0-387-77241-7 *(SVM Book)*
  - Tan, Peter Jing; and [Dowe, David L.](http://www.csse.monash.edu.au/~dld) (2004); [*MML Inference of Oblique Decision Trees*](http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#TanDowe2004), Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag, [pp. 1082–1088](http://www.csse.monash.edu.au/~dld/Publications/2004/Tan+DoweAI2004.pdf). (This paper uses [minimum message length](https://zh.wikipedia.org/wiki/minimum_message_length "wikilink") ([MML](https://zh.wikipedia.org/wiki/Minimum_Message_Length "wikilink")) and actually incorporates probabilistic support vector machines in the leaves of [decision trees](https://zh.wikipedia.org/wiki/Decision_tree_learning "wikilink").)
  - Vapnik, Vladimir N.; *The Nature of Statistical Learning Theory*, Springer-Verlag, 1995. ISBN 0-387-98780-0
  - Vapnik, Vladimir N.; and Kotz, Samuel; *Estimation of Dependences Based on Empirical Data*, Springer, 2006. ISBN 0-387-30865-2, 510 pages \[this is a reprint of Vapnik's early book describing philosophy behind SVM approach. The 2006 Appendix describes recent development\].
  - Fradkin, Dmitriy; and Muchnik, Ilya; *[Support Vector Machines for Classification](https://web.archive.org/web/20161019164722/http://paul.rutgers.edu/~dfradkin/papers/svm.pdf)* in Abello, J.; and Carmode, G. (Eds); *Discrete Methods in Epidemiology*, DIMACS Series in Discrete Mathematics and Theoretical Computer Science, volume 70, pp. 13–20, 2006. Succinctly describes theoretical ideas behind SVM.
  - Bennett, Kristin P.; and Campbell, Colin; *[Support Vector Machines: Hype or Hallelujah?](http://kdd.org/exploration_files/bennett.pdf)*, SIGKDD Explorations, 2, 2, 2000, 1–13. Excellent introduction to SVMs with helpful figures.
  - Ivanciuc, Ovidiu; *[Applications of Support Vector Machines in Chemistry](http://www.ivanciuc.org/Files/Reprint/Ivanciuc_SVM_CCR_2007_23_291.pdf)*, in *Reviews in Computational Chemistry*, Volume 23, 2007, pp. 291–400.
  - Catanzaro, Bryan; Sundaram, Narayanan; and Keutzer, Kurt; *[Fast Support Vector Machine Training and Classification on Graphics Processors](https://web.archive.org/web/20120302180102/http://www.eecs.berkeley.edu/~catanzar/icml2008.pdf)*, in *International Conference on Machine Learning*, 2008
  - Campbell, Colin; and Ying, Yiming; *[Learning with Support Vector Machines](http://www.morganclaypool.com/doi/abs/10.2200/S00324ED1V01Y201102AIM010)*, 2011, Morgan and Claypool. ISBN 978-1-60845-616-1.
  - Ben-Hur, Asa, Horn, David, Siegelmann, Hava, and Vapnik, Vladimir; "Support vector clustering" (2001) Journal of Machine Learning Research, 2: 125–137.

## 外部链接

  - [www.support-vector.net](https://web.archive.org/web/20180627015707/https://www.support-vector.net/) The key book about the method, "An Introduction to Support Vector Machines" with online software
  - Burges, Christopher J. C.; [A Tutorial on Support Vector Machines for Pattern Recognition](http://research.microsoft.com/en-us/um/people/cburges/papers/svmtutorial.pdf), Data Mining and Knowledge Discovery 2:121–167, 1998
  - [www.kernel-machines.org](http://www.kernel-machines.org) *(general information and collection of research papers)*
  - [www.support-vector-machines.org](http://www.support-vector-machines.org) *(Literature, Review, Software, Links related to Support Vector Machines — Academic Site)*
  - [svmtutorial.online](https://svmtutorial.online) A simple introduction to SVM, easily accessible to anyone with basic background in mathematics
  - [videolectures.net](http://videolectures.net/Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines/) *(SVM-related video lectures)*
  - Karatzoglou, Alexandros et al.; [Support Vector Machines in R](http://www.jstatsoft.org/v15/i09/paper), Journal of Statistical Software April 2006, Volume 15, Issue 9.
  - [libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm/), [LIBSVM](https://zh.wikipedia.org/wiki/LIBSVM "wikilink") is a popular library of SVM learners
  - [liblinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear/) is a library for large linear classification including some SVMs
  - [Shark](https://web.archive.org/web/20100227221243/http://shark-project.sourceforge.net/) is a [C++](../Page/C++.md "wikilink") machine learning library implementing various types of SVMs
  - [dlib](http://dlib.net/ml.html) is a C++ library for working with kernel methods and SVMs
  - [SVM light](http://svmlight.joachims.org) is a collection of software tools for learning and classification using SVM
  - [SVMJS live demo](http://cs.stanford.edu/people/karpathy/svmjs/demo/) is a GUI demo for [JavaScript](../Page/JavaScript.md "wikilink") implementation of SVMs
  - [Gesture Recognition Toolkit](https://github.com/nickgillian/grt) contains an easy to use wrapper for [libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)

[Category:分類演算法](https://zh.wikipedia.org/wiki/Category:分類演算法 "wikilink") [Category:统计分类](https://zh.wikipedia.org/wiki/Category:统计分类 "wikilink")

1.
2.  Ben-Hur, Asa, Horn, David, Siegelmann, Hava, and Vapnik, Vladimir; "Support vector clustering" (2001) Journal of Machine Learning Research, 2: 125–137.

3.  \*

4.  Vapnik, V.: Invited Speaker. IPMU Information Processing and Management 2014)

5.  Barghout, Lauren. "Spatial-Taxon Information Granules as Used in Iterative Fuzzy-Decision-Making for Image Segmentation." Granular Computing and Decision-Making. Springer International Publishing, 2015. 285-318.

6.  Bilwaj Gaonkar, Christos Davatzikos Analytic estimation of statistical significance maps for support vector machine based multi-variate image analysis and classification

7.  R. Cuingnet, C. Rosso, M. Chupin, S. Lehéricy, D. Dormont, H. Benali, Y. Samson and O. Colliot, Spatial regularization of SVM for the detection of diffusion alterations associated with stroke outcome, Medical Image Analysis, 2011, 15 (5): 729–737

8.  Statnikov, A., Hardin, D., & Aliferis, C. (2006). Using SVM weight-based methods to identify causally relevant and non-causally relevant variables. sign, 1, 4.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.
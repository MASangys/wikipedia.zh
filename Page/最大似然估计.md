在[统计学](../Page/统计学.md "wikilink")中，**最大似然估计**（，缩写为MLE），也称**极大似然估计**、**最大概似估计**，是用来[估计一个](https://zh.wikipedia.org/wiki/估计函数 "wikilink")[概率模型](../Page/概率模型.md "wikilink")的参数的一种方法。

## 预备知识

下边的讨论要求读者熟悉[概率论中的基本定义](https://zh.wikipedia.org/wiki/概率论 "wikilink")，如[概率分布](../Page/概率分布.md "wikilink")、[概率密度函数](https://zh.wikipedia.org/wiki/概率密度函数 "wikilink")、[随机变量](../Page/随机变量.md "wikilink")、[数学期望等](https://zh.wikipedia.org/wiki/数学期望 "wikilink")。读者還須先熟悉[连续](https://zh.wikipedia.org/wiki/连续 "wikilink")[实函数](../Page/实函数.md "wikilink")的基本技巧，比如使用[微分](../Page/微分.md "wikilink")来求一个函数的[极值](../Page/极值.md "wikilink")（即[极大值或](https://zh.wikipedia.org/wiki/极大值 "wikilink")[极小值](https://zh.wikipedia.org/wiki/极小值 "wikilink")）。
同時，讀者須先擁有[似然函數的背景知識](https://zh.wikipedia.org/wiki/似然函數 "wikilink")，以了解最大似然估計的出發點及應用目的。

## 最大似然估计的原理

给定一个概率分布\(D\)，已知其[概率密度函数](https://zh.wikipedia.org/wiki/概率密度函数 "wikilink")（连续分布）或[概率质量函数](../Page/概率质量函数.md "wikilink")（离散分布）为\(f_D\)，以及一个分布参数\(\theta\)，我们可以从这个分布中抽出一个具有\(n\)个值的采样\(X_1, X_2,\ldots, X_n\)，利用\(f_D\)计算出其[似然函数](../Page/似然函数.md "wikilink")：

\[\mbox{L}(\theta\mid x_1,\dots,x_n) = f_\theta(x_1,\dots,x_n).\]

若\(D\)是离散分布，\(f_\theta\)即是在参数为\(\theta\)时观测到这一采样的概率。若其是连续分布，\(f_\theta\)则为\(X_1, X_2,\ldots, X_n\)联合分布的概率密度函数在观测值处的取值。一旦我们获得\(X_1, X_2,\ldots, X_n\)，我们就能求得一个关于\(\theta\)的估计。最大似然估计会寻找关于\(\theta\)的最可能的值（即，在所有可能的\(\theta\)取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在\(\theta\)的所有可能取值中寻找一个值使得似然[函数](../Page/函数.md "wikilink")取到最大值。这个使可能性最大的\(\widehat{\theta}\)值即称为\(\theta\)的**最大似然估计**。由定义，最大似然估计是样本的函数。

### 注意

  - 这裡的[似然函数](../Page/似然函数.md "wikilink")是指\(x_1,x_2,\ldots,x_n\)不变时，关于\(\theta\)的一个函数。
  - 最大似然估计不一定存在，也不一定唯一。

## 例子

### 离散分布，离散有限参数空间

考虑一个抛硬币的例子。假设这个硬币正面跟反面轻重不同。我们把这个硬币抛80次（即，我们获取一个采样\(x_1=\mbox{H}, x_2=\mbox{T}, \ldots, x_{80}=\mbox{T}\)并把正面的次数记下来，正面记为H，反面记为T）。并把抛出一个正面的概率记为\(p\)，抛出一个反面的概率记为\(1-p\)（因此，这裡的\(p\)即相当于上边的\(\theta\)）。假设我们抛出了49个正面，31个反面，即49次H，31次T。假设这个硬币是我们从一个装了三个硬币的盒子里头取出的。这三个硬币抛出正面的概率分别为\(p=1/3\), \(p=1/2\), \(p=2/3\).这些硬币没有标记，所以我们无法知道哪个是哪个。使用**最大似然估计**，基于**二项分布**中的**概率质量函数**公式，通过这些试验数据（即采样数据），我们可以计算出哪个硬币的可能性最大。这个似然函数取以下三个值中的一个：

\[\begin{matrix}
\mathbb{L}(p=1/3 \mid \mbox{H=49, T=31 }) & = & \mathbb{P}(\mbox{H=49, T=31 }\mid p=1/3) & = & {80\choose 49}(1/3)^{49}(1-1/3)^{31} \approx 0.000 \\
&&\\
\mathbb{L}(p=1/2 \mid \mbox{H=49, T=31 }) & = & \mathbb{P}(\mbox{H=49, T=31 }\mid p=1/2) & = & {80\choose 49}(1/2)^{49}(1-1/2)^{31} \approx 0.012 \\
&&\\
\mathbb{L}(p=2/3 \mid \mbox{H=49, T=31 }) & = & \mathbb{P}(\mbox{H=49, T=31 }\mid p=2/3) & = & {80\choose 49}(2/3)^{49}(1-2/3)^{31} \approx 0.054 \\
\end{matrix}\]

我们可以看到当\(\widehat{p}=2/3\)时，似然函数取得最大值。
顯然地，這硬幣的公平性和那種拋出後正面的機率是2/3的硬幣是最接近的。这就是\(p\)的最大似然估计。

### 离散分布，连续参数空间

现在假设例子1中的盒子中有无数个硬币，对于\(0\leq p \leq 1\)中的任何一个\(p\)， 都有一个抛出正面概率为\(p\)的硬币对应，我们来求其似然函数的最大值：

\[\begin{matrix}
\mbox{L}(\theta) & = & f_D(\mbox{H=49,T=80-49}\mid p) = {80\choose 49} p^{49}(1-p)^{31} \\
\end{matrix}\]

其中\(0\leq p \leq 1\). 我们可以使用[微分法来求](https://zh.wikipedia.org/wiki/微分法 "wikilink")[極值](https://zh.wikipedia.org/wiki/極值 "wikilink")。方程两边同时对\(p\)取[微分](../Page/微分.md "wikilink")，并使其为零。

\[\begin{matrix}
0 & = &  {80\choose 49}\frac{d}{dp} \left(  p^{49}(1-p)^{31} \right) \\
  &   & \\
  & \propto & 49p^{48}(1-p)^{31} - 31p^{49}(1-p)^{30} \\
  &   & \\
  & = & p^{48}(1-p)^{30}\left[ 49(1-p) - 31p \right] \\
\end{matrix}\]

[BinominalLikelihoodGraph.png](https://zh.wikipedia.org/wiki/File:BinominalLikelihoodGraph.png "fig:BinominalLikelihoodGraph.png")并在曲线的最大值处。\]\] 其解为\(p=0\), \(p=1\)，以及\(p=49/80\).使可能性最大的解显然是\(p=49/80\)（因为\(p=0\)和\(p=1\)这两个解会使可能性为零）。因此我们说**最大似然估计值**为\(\widehat{p}=49/80\).

这个结果很容易一般化。只需要用一个字母\(t\)代替49用以表达[伯努利试验中的被观察数据](https://zh.wikipedia.org/wiki/伯努利试验 "wikilink")（即样本）的“成功”次数，用另一个字母\(n\)代表伯努利试验的次数即可。使用完全同样的方法即可以得到**最大似然估计值**:

\[\widehat{p}=\frac{t}{n}\]

对于任何成功次数为\(t\)，试验总数为\(n\)的伯努利试验。

### 连续分布，连续参数空间

最常见的[连续概率分布是](https://zh.wikipedia.org/wiki/连续概率分布 "wikilink")[正态分布](../Page/正态分布.md "wikilink")，其概率密度函数如下：

\[f(x\mid \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]

现在有\(n\)个正态随机变量的采样点，要求的是一个这样的正态分布，这些采样点分布到这个正态分布可能性最大（也就是概率密度积最大，每个点更靠近中心点），其\(n\)个正态随机变量的采样的对应密度函数（假设其独立并服从同一分布）为：

\[f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} e^{-\frac{ \sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^2}}\]

或：

\[f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right)\],

这个分布有两个参数：\(\mu,\sigma^2\).有人可能会担心两个参数与上边的讨论的例子不同，上边的例子都只是在一个参数上对可能性进行最大化。实际上，在两个参数上的求最大值的方法也差不多：只需要分别把可能性\(\mbox{L}(\mu,\sigma) = f(x_1,,\ldots,x_n \mid \mu, \sigma^2)\)在两个参数上最大化即可。当然这比一个参数麻烦一些，但是一点也不复杂。使用上边例子同样的符号，我们有\(\theta=(\mu,\sigma^2)\).

最大化一个似然函数同最大化它的[自然对数是等价的](https://zh.wikipedia.org/wiki/自然对数 "wikilink")。因为[自然对数log是一个](https://zh.wikipedia.org/wiki/自然对数 "wikilink")[连续且在似然函数的](https://zh.wikipedia.org/wiki/连续 "wikilink")[值域](../Page/值域.md "wikilink")内[严格递增的上凸函数](https://zh.wikipedia.org/wiki/严格递增 "wikilink")。\[注意：可能性函数（似然函数）的自然对数跟[信息熵以及](https://zh.wikipedia.org/wiki/信息熵 "wikilink")[Fisher信息联系紧密](https://zh.wikipedia.org/wiki/Fisher信息 "wikilink")。\]求对数通常能够一定程度上简化运算，比如在这个例子中可以看到：

\[\begin{matrix}
0 & = & \frac{\partial}{\partial \mu} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} e^{-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}} \right) \\
  & = & \frac{\partial}{\partial \mu} \left( \log\left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right) \\
  & = & 0 - \frac{-2n(\bar{x}-\mu)}{2\sigma^2} \\
\end{matrix}\]

这个方程的解是\(\widehat{\mu} = \bar{x} = \sum^{n}_{i=1}x_i/n\).这的确是这个函数的最大值，因为它是\(\mu\)里头惟一的一阶导数等于零的点并且二阶导数严格小于零。

同理，我们对\(\sigma\)求导，并使其为零。

\[\begin{matrix}
0 & = & \frac{\partial}{\partial \sigma} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} e^{-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}} \right) \\
  & = & \frac{\partial}{\partial \sigma} \left( \frac{n}{2}\log\left( \frac{1}{2\pi\sigma^2} \right) - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right) \\
  & = & -\frac{n}{\sigma} + \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{\sigma^3}
\\
\end{matrix}\]

这个方程的解是\(\widehat{\sigma}^2 = \sum_{i=1}^n(x_i-\widehat{\mu})^2/n\).

因此，其关于\(\theta=(\mu,\sigma^2)\)的*最大似然估计*为：

\[\widehat{\theta}=(\widehat{\mu},\widehat{\sigma}^2) = (\bar{x},\sum_{i=1}^n(x_i-\bar{x})^2/n)\].

## 性质

### 泛函不变性（Functional invariance）

如果\(\hat{\theta}\)是\(\theta\)的一个最大似然估计，那么\(\alpha = g(\theta)\)的最大似然估计是\(\hat{\alpha} = g(\hat{\theta})\)。函数*g*无需是一个[双射](../Page/双射.md "wikilink")。\[1\]

### 渐近线行为

最大似然估计函数在采样样本总数趋于无穷的时候达到最小[方差](../Page/方差.md "wikilink")（其证明可见于[Cramer-Rao lower bound](https://zh.wikipedia.org/wiki/Cramer-Rao_lower_bound "wikilink")）。当最大似然估计非偏时，等价的，在极限的情况下我们可以称其有最小的[均方差](https://zh.wikipedia.org/wiki/均方差 "wikilink")。 对于独立的观察来说，最大似然估计函数经常趋于[正态分布](../Page/正态分布.md "wikilink")。

### 偏差

最大似然估计的[偏差是非常重要的](https://zh.wikipedia.org/wiki/非偏估计 "wikilink")。考虑这样一个例子，标有*1*到*n*的*n*张票放在一个盒子中。从盒子中随机抽取票。如果*n*是未知的话，那么*n*的最大似然估计值就是抽出的票上标有的*n*，尽管其期望值的只有\((n+1)/2\).为了估计出最高的*n*值，我们能确定的只能是*n*值不小于抽出来的票上的值。

## 历史

最大似然估计最早是由[羅納德·費雪在](https://zh.wikipedia.org/wiki/羅納德·費雪 "wikilink")1912年至1922年间推荐、分析并大范围推广的。\[2\]（虽然以前[高斯](https://zh.wikipedia.org/wiki/高斯 "wikilink")、[拉普拉斯](https://zh.wikipedia.org/wiki/拉普拉斯 "wikilink")、[T. N. Thiele和](https://zh.wikipedia.org/wiki/Thorvald_N._Thiele "wikilink")[F. Y. 埃奇沃思也使用过](../Page/弗朗西斯·伊西德罗·埃奇沃思.md "wikilink")）。\[3\] 许多作者都提供了最大似然估计发展的回顾。\[4\]

大部分的最大似然估计理论都在[贝叶斯统计中第一次得到发展](https://zh.wikipedia.org/wiki/贝叶斯统计 "wikilink")，并被后来的作者简化。\[5\]

## 参见

  - [均方差是衡量一个](https://zh.wikipedia.org/wiki/均方差 "wikilink")[估计函数的好坏的一个量](https://zh.wikipedia.org/wiki/估计函数 "wikilink")。

<!-- end list -->

  - 关于[Rao-Blackwell定理](https://zh.wikipedia.org/wiki/Rao-Blackwell定理 "wikilink")（Rao-Blackwell theorem）的文章里头讨论到如何利用Rao-Blackwellisation过程寻找最佳非偏估计（即使均方差最小）的方法。而最大似然估计通常是一个好的起点。

<!-- end list -->

  - 读者可能会对最大似然估计（如果存在）总是一个关于参数的[充分统计](https://zh.wikipedia.org/wiki/充分统计 "wikilink")（sufficient statistic）的函数感兴趣。

<!-- end list -->

  - 最大似然估计跟[一般化矩方法](https://zh.wikipedia.org/wiki/一般化矩方法 "wikilink")（generalized method of moments）有关。

## 参考文献

## 外部链接

  - [关于最大似然估计的历史的一篇论文，作者John Aldrich](https://web.archive.org/web/20070223105625/http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906)

{{-}}

[Category:估计理论](https://zh.wikipedia.org/wiki/Category:估计理论 "wikilink") [Category:统计学](https://zh.wikipedia.org/wiki/Category:统计学 "wikilink")

1.  请参见George Casella与Roger L. Berger所著的*Statistical Inference*定理Theorem 7.2.10的证明。（中国大陆出版的大部分教材上也可以找到这个证明。）

2.

3.   and

4.  , , , , and

5.
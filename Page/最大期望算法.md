**最大期望演算法**（**Expectation-maximization
algorithm**，又譯**期望最大化算法**）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。

在[统计](https://zh.wikipedia.org/wiki/统计 "wikilink")[计算中](https://zh.wikipedia.org/wiki/计算 "wikilink")，**最大期望（EM）算法**是在[概率模型中寻找](../Page/概率模型.md "wikilink")[参数](https://zh.wikipedia.org/wiki/参数 "wikilink")[最大似然估计或者](../Page/最大似然估计.md "wikilink")[最大后验估计的](../Page/最大后验概率.md "wikilink")[算法](../Page/算法.md "wikilink")，其中概率模型依赖于无法观测的[隐变量](https://zh.wikipedia.org/wiki/隐变量 "wikilink")。最大期望算法经常用在[机器学习和](../Page/机器学习.md "wikilink")[计算机视觉的](../Page/计算机视觉.md "wikilink")[数据聚类](https://zh.wikipedia.org/wiki/数据聚类 "wikilink")（Data
Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在E步上求得的[最大似然值来计算参数的值](../Page/最大似然估计.md "wikilink")。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

## 历史

最大期望值算法由，和在他们1977年发表的经典论文中提出。他们指出此方法之前其实已经被很多作者「在他们特定的研究领域中多次提出过」。

## EM简单教程

EM是一个在已知部分相关变量的情况下，估计未知变量的迭代技术。EM的算法流程如下：

1.  初始化分布参数
2.  重复直到收敛：
    1.  E步骤：根据参数的假设值，给出未知变量的期望估计，应用于缺失值。
    2.  M步骤：根据未知变量的估计值，给出当前的参数的极大似然估计。

## 最大期望过程说明

我们用\(\textbf{y}\)表示能够观察到的不完整的变量值，用\(\textbf{x}\)表示无法观察到的变量值，这样\(\textbf{x}\)和\(\textbf{y}\)一起组成了完整的数据。\(\textbf{x}\)可能是实际测量丢失的数据，也可能是能够简化问题的隐藏变量，如果它的值能够知道的话。例如，在[混合模型中](https://zh.wikipedia.org/wiki/混合模型 "wikilink")，如果“产生”样本的混合元素成分已知的话最大似然公式将变得更加便利（参见下面的例子）。

### 估计无法观测的数据

让\(p\,\)代表矢量\(\theta\):
\(p( \mathbf y, \mathbf x | \theta)\)定义的参数的全部数据的[機率密度函數](../Page/機率密度函數.md "wikilink")（连续情况下）或者[機率質量函數](https://zh.wikipedia.org/wiki/機率質量函數 "wikilink")（离散情况下），那么从这个函数就可以得到全部数据的[最大似然值](../Page/最大似然估计.md "wikilink")，另外，在给定的观察到的数据条件下未知数据的[条件分布可以表示为](https://zh.wikipedia.org/wiki/条件分布 "wikilink")：

\[p(\mathbf x |\mathbf y, \theta) = \frac{p(\mathbf y, \mathbf x | \theta)}{p(\mathbf y | \theta)} = \frac{p(\mathbf y|\mathbf x, \theta)p(\mathbf x |\theta)}{\int p(\mathbf y|\mathbf x, \theta) p(\mathbf x |\theta) d\mathbf x}\]

## 参见

  - [估计理论](../Page/估计理论.md "wikilink")
  - [数据聚类](https://zh.wikipedia.org/wiki/数据聚类 "wikilink")

## 参考文献

  - Arthur Dempster, Nan Laird, and Donald Rubin. "Maximum likelihood
    from incomplete data via the EM algorithm". *Journal of the Royal
    Statistical Society*, Series B, 39 (1):1–38, 1977
    [1](http://links.jstor.org/sici?sici=0035-9246%281977%2939%3A1%3C1%3AMLFIDV%3E2.0.CO%3B2-Z).
  - Robert Hogg, Joseph McKean and Allen Craig. *Introduction to
    Mathematical Statistics*. pp. 359-364. Upper Saddle River, NJ:
    Pearson Prentice Hall, 2005.
  - Radford Neal, [Geoffrey
    Hinton](https://zh.wikipedia.org/wiki/Geoffrey_Hinton "wikilink").
    "A view of the EM algorithm that justifies incremental, sparse, and
    other variants". In Michael I. Jordan (editor), *Learning in
    Graphical Models* pp 355-368. Cambridge, MA: MIT Press, 1999.
  - [The on-line textbook: Information Theory, Inference, and Learning
    Algorithms](http://www.inference.phy.cam.ac.uk/mackay/itila/)，by
    [David J.C.
    MacKay](https://zh.wikipedia.org/wiki/David_J.C._MacKay "wikilink")
    includes simple examples of the E-M algorithm such as clustering
    using the soft K-means algorithm, and emphasizes the variational
    view of the E-M algorithm.
  - [A Gentle Tutorial of the EM Algorithm and its Application to
    Parameter Estimation for Gaussian Mixture and Hidden Markov
    Models](http://citeseer.ist.psu.edu/bilmes98gentle.html)，by [J.
    Bilmes](https://zh.wikipedia.org/wiki/J._Bilmes "wikilink") includes
    a simplified derivation of the EM equations for Gaussian Mixtures
    and Gaussian Mixture Hidden Markov Models.
  - [Information Geometry of the EM and em Algorithms for Neural
    Networks](http://citeseer.ist.psu.edu/amari95information.html)，by
    [Shun-Ichi
    Amari](https://zh.wikipedia.org/wiki/Shun-Ichi_Amari "wikilink")
    give a view of EM algorithm from geometry view point.

[Category:估计理论](https://zh.wikipedia.org/wiki/Category:估计理论 "wikilink")
[Category:算法](https://zh.wikipedia.org/wiki/Category:算法 "wikilink")
[Category:機器學習演算法](https://zh.wikipedia.org/wiki/Category:機器學習演算法 "wikilink")
> 本文内容由[极限学习机](https://zh.wikipedia.org/wiki/极限学习机)转换而来。


**极限学习机**（[英文](https://zh.wikipedia.org/wiki/英文 "wikilink")：Extreme Learning Machines，縮寫ELM），又名**超限学习机**，為[人工智能](../Page/人工智能.md "wikilink")[機器學習領域中的一種](https://zh.wikipedia.org/wiki/機器學習 "wikilink")[人工神經網絡模型](https://zh.wikipedia.org/wiki/人工神經網絡 "wikilink")，是一种求解单隐层[前馈神经网络的学习](https://zh.wikipedia.org/wiki/前馈神经网络 "wikilink")[算法](../Page/算法.md "wikilink")。

## 特點

传统的[前馈神经网络](https://zh.wikipedia.org/wiki/前馈神经网络 "wikilink")（如BP神经网络）需要人为设置大量的网络训练[参数](https://zh.wikipedia.org/wiki/参数 "wikilink")，此算法卻只需要设定网络的结构，而不需设置其他参数，因此具有简单易用的特点。其输入层到隐藏层的权值是一次随机确定的，算法执行过程中不需要再调整，而隐藏层到输出层的[权值只需解一个](https://zh.wikipedia.org/wiki/權值_\(網路\) "wikilink")[线性方程组](../Page/线性方程组.md "wikilink")来确定，因此可以提升计算速度。

## 開發

极限学习机的名稱來自[新加坡](../Page/新加坡.md "wikilink")[南洋理工大學](https://zh.wikipedia.org/wiki/南洋理工大學 "wikilink")[黃廣斌教授所建立的模型](https://zh.wikipedia.org/wiki/黃廣斌 "wikilink")\[1\]。黃教授指出，此算法的泛化性能良好，且其學習速度比運用[反向传播算法](../Page/反向传播算法.md "wikilink")訓練的速度要快上1000倍\[2\]。

## 算法

极限学习机中最简单的原理如下：

\[\mathbf{\hat{Y}} = \mathbf{W}_2 \sigma(\mathbf{W}_1 x)\]

其中是输入向量到隐藏节点层的权重矩阵，是[激活函数](../Page/激活函数.md "wikilink")，是隐藏节点层到输出向量的权重矩阵。算法按下列步骤进行：

1.  用随机产生的[高斯噪声给矩阵](https://zh.wikipedia.org/wiki/高斯噪声 "wikilink")的每个元素赋值；
2.  用[最小二乘法](../Page/最小二乘法.md "wikilink")估计使期望输出与实际输出误差最小的输出权重矩阵，数学上能够证明计算隐藏节点层输出矩阵的[广义逆](../Page/摩尔－彭若斯广义逆.md "wikilink")  即可\[3\]：
    \[\mathbf{W}_2 = \sigma(\mathbf{W}_1 \mathbf{X})^+ \mathbf{Y}\]

## 參見

  - [機器學習](https://zh.wikipedia.org/wiki/機器學習 "wikilink")
  - [前馈神经网络](https://zh.wikipedia.org/wiki/前馈神经网络 "wikilink")
  - [人工神經網絡](https://zh.wikipedia.org/wiki/人工神經網絡 "wikilink")

## 參考資料

## 外部链接

  - [南洋理工学院极限学习机网站，附源代码、未决难题、ELM会议、教程、参考资料等](http://www.ntu.edu.sg/home/egbhuang/elm_codes.html)

  - [ELM的R程序包](https://cran.r-project.org/web/packages/ELMR/)

  -
  -
  -
[Category:神經網路](https://zh.wikipedia.org/wiki/Category:神經網路 "wikilink")

1.
2.
3.
> 本文内容由[正则化 \(数学\)](https://zh.wikipedia.org/wiki/正则化_\(数学\))转换而来。


在[数学](../Page/数学.md "wikilink")与[计算机科学](../Page/计算机科学.md "wikilink")中，尤其是在[机器学习](../Page/机器学习.md "wikilink")和[逆问题领域中](https://zh.wikipedia.org/wiki/逆问题 "wikilink")，**正则化**（英语：regularization）是指为解决[适定性问题或](https://zh.wikipedia.org/wiki/适定性问题 "wikilink")[过拟合而加入额外信息的过程](https://zh.wikipedia.org/wiki/过拟合 "wikilink")。\[1\]

在机器学习和逆问题的优化过程中，正则项往往被加在[目标函数当中](https://zh.wikipedia.org/wiki/目标函数 "wikilink")。

## 概述

概括来讲，[机器学习](../Page/机器学习.md "wikilink")的训练过程，就是要找到一个**足够好**的函数\(F^{*}\)用以在新的数据上进行[推理](../Page/推理.md "wikilink")。\[2\]为了定义什么是「好」，人们引入了[损失函数的概念](https://zh.wikipedia.org/wiki/损失函数 "wikilink")。一般地，对于样本\((\vec x, y)\)和模型\(F\)，有预测值\(\hat y = F(\vec x)\)。损失函数是定义在\(\mathbb R\times\mathbb R \to\mathbb R\)上的二元函数\(\ell(y, \hat y)\)，用来描述[基准真相和模型预测值之间的差距](https://zh.wikipedia.org/wiki/基准真相 "wikilink")。一般来说，损失函数是一个有[下确界的函数](https://zh.wikipedia.org/wiki/下确界 "wikilink")；当基准真相和模型预测值足够接近，损失函数的值也会接近该下确界。

因此，机器学习的训练过程可以被转化为[训练集](https://zh.wikipedia.org/wiki/训练集 "wikilink")\(\mathcal D\)上的[最小化问题](https://zh.wikipedia.org/wiki/最小化 "wikilink")。我们的目标是在[泛函空间内](https://zh.wikipedia.org/wiki/泛函空间 "wikilink")，找到使得全局损失\(L(F) = \sum_{i \in \mathcal D}\ell(y_i, \hat y_i)\)最小的模型\(F^{*}\)。

\(F^{*} := \mathop{\text{arg min}}_{F}L(F).\)

由于损失函数只考虑在训练集上的[经验风险](https://zh.wikipedia.org/wiki/经验风险 "wikilink")，这种做法可能会导致[过拟合](https://zh.wikipedia.org/wiki/过拟合 "wikilink")。为了对抗过拟合，我们需要向损失函数中加入描述模型复杂程度的正则项\(\Omega(F)\)，将经验风险最小化问题转化为结构风险最小化。

\(F^{*} := \mathop{\text{arg min}}_{F}\text{Obj}(F) = \mathop{\text{arg min}}_{F}\bigl(L(F) + \gamma\Omega(F)\bigr),\qquad \gamma > 0.\)

这里，\(\text{Obj}(F)\)称为[目标函数](https://zh.wikipedia.org/wiki/目标函数 "wikilink")，它描述模型的[结构风险](https://zh.wikipedia.org/wiki/结构风险 "wikilink")；\(L(F)\)是训练集上的损失函数；\(\Omega(F)\)是正则项，描述模型的复杂程度；\(\gamma\)是用于控制正则项重要程度的参数。正则项通常包括对[光滑度及](../Page/光滑函数.md "wikilink")[向量空间](../Page/向量空间.md "wikilink")内[范数](../Page/范数.md "wikilink")上界的限制。\[3\][\(L_{p}\)-范数是一种常见的正则项](../Page/Lp范数.md "wikilink")。

在看来，正则项是在模型训练过程中引入了某种模型参数的[先验分布](https://zh.wikipedia.org/wiki/先验 "wikilink")。

## \(L_{p}\)-正则项

所谓[范数](../Page/范数.md "wikilink")即是抽象之长度，通常意义上满足长度的三种性质：[非负性](https://zh.wikipedia.org/wiki/非负性 "wikilink")、[齐次性和](https://zh.wikipedia.org/wiki/齐次性 "wikilink")[三角不等式](../Page/三角不等式.md "wikilink")。

以函数的观点来看，范数是定义在\(\mathbb R^{n}\to\mathbb R\)的函数；并且它和损失函数类似，也具有下确界。后一性质是由范数的非负性和齐次性保证的\[4\]。这一特性使得\(L_{p}\)-范数天然适合做正则项，因为目标函数仍可用[梯度下降等方式求解最优化问题](https://zh.wikipedia.org/wiki/梯度下降 "wikilink")。\(L_{p}\)-范数作为正则项时被称为\(L_{p}\)-正则项。

### \(L_{0}\) 和 \(L_{1}\)-正则项

机器学习模型当中的参数，可形式化地组成参数向量，记为\(\vec\omega\)。不失一般性，以线性模型为例：

\(F(\vec x;\vec\omega) := \vec\omega^{\intercal}\cdot\vec x = \sum_{i = 1}^{n}\omega_{i}\cdot x_{i}.\)

由于[训练集当中](https://zh.wikipedia.org/wiki/训练集 "wikilink")[统计噪声的存在](https://zh.wikipedia.org/wiki/统计噪声 "wikilink")，冗余的特征可能成为[过拟合的一种来源](https://zh.wikipedia.org/wiki/过拟合 "wikilink")。这是因为，对于统计噪声，模型无法从有效特征当中提取信息进行[拟合](https://zh.wikipedia.org/wiki/拟合 "wikilink")，故而会转向冗余特征。为了对抗此类过拟合现象，人们会希望让尽可能多的\(\omega_{i}\)为零。为此，最直观地，可以引入\(L_{0}\)-正则项

\(\Omega\bigl(F(\vec x;\vec\omega)\bigr) := \gamma_0\frac{\lVert\vec\omega\rVert_0}{n},\;\gamma_{0} > 0.\)

通过引入\(L_{0}\)-正则项，人们实际上是向优化过程引入了一种惩罚机制：当优化算法希望增加模型复杂度（此处特指将原来为零的参数\(\omega_{i}\)更新为非零的情形）以降低模型的经验风险（即降低全局损失）时，在结构风险上进行大小为\(\tfrac{\gamma_{0}}{n}\)的惩罚。于是，当增加模型复杂度在经验风险上的收益不足\(\tfrac{\gamma_{0}}{n}\)时，整个结构风险实际上会增大而非减小。因此优化算法会拒绝此类更新。

引入\(L_{0}\)-正则项可使模型参数稀疏化，以及使得模型易于解释。但\(L_{0}\)-正则项也有无法避免的问题：非连续、非凸、不可微。因此，在引入\(L_{0}\)-正则项的目标函数上做最优化求解，是一个无法在多项式时间内完成的问题。于是，人们转而考虑\(L_{0}\)-范数的[最紧凸放松](https://zh.wikipedia.org/wiki/最紧凸放松 "wikilink")——\(L_{1}\)-范数，令

\(\Omega\bigl(F(\vec x;\vec\omega)\bigr) := \gamma_1\frac{\lVert\vec\omega\rVert_1}{n},\;\gamma_{1} > 0.\)

和引入\(L_{0}\)-正则项的情况类似，引入\(L_{1}\)-正则项是在结构风险上进行大小为\(\tfrac{\gamma_{1}|\omega_{i}|}{n}\)的惩罚，以达到稀疏化的目的。

\(L_{1}\)-正则项亦称[LASSO](../Page/Lasso算法.md "wikilink")-正则项。\[5\]\[6\]

### \(L_{2}\)-正则项

[Overfitting_show_case_for_polynomial_model_and_linear_model.png](https://zh.wikipedia.org/wiki/File:Overfitting_show_case_for_polynomial_model_and_linear_model.png "fig:Overfitting_show_case_for_polynomial_model_and_linear_model.png")，右侧是[验证集](https://zh.wikipedia.org/wiki/验证集 "wikilink")。训练集和验证集数据均是由线性函数加上一定的随机扰动生成的。图中橙色直线是以线性模型拟合训练集数据得到模型的函数曲线；绿色虚线则是以15-阶多项式模型拟合训练数据得到模型的函数曲线。可见，尽管多项式模型在训练集上的误差小于线性模型，但在验证机上的误差则显著大于线性模型。此外，多项式模型为了拟合[噪声点](https://zh.wikipedia.org/wiki/统计噪声 "wikilink")，在噪声点附近进行了高[曲率](../Page/曲率.md "wikilink")的弯折。这说明多项式模型[过拟合了训练集数据](https://zh.wikipedia.org/wiki/过拟合 "wikilink")。\]\]

在发生[过拟合时](https://zh.wikipedia.org/wiki/过拟合 "wikilink")，模型的函数曲线往往会发生剧烈的弯折，这意味着模型函数在局部的切线之斜率非常高。一般地，函数的曲率是函数参数的[线性组合](../Page/线性组合.md "wikilink")或非线性组合。为了对抗此类过拟合，人们会希望使得这些参数的值相对稠密且均匀地集中在零附近。于是，人们引入了\(L_{2}\)-范数，作为\(L_{2}\)-正则项。令

\(\Omega\bigl(F(\vec x;\vec w)\bigr) := \gamma_2\frac{\lVert\vec\omega\rVert_2^2}{2n},\;\gamma_2 > 0,\)

于是有目标函数

\(\text{Obj}(F) = L(F) + \gamma_2\frac{\lVert\vec\omega\rVert_2^2}{2n},\)

于是对于参数\(\omega_{i}\)，有偏导

\(\frac{\partial\text{Obj}}{\partial \omega_i} = \frac{\partial L}{\partial \omega_i} + \frac{\gamma_2}{n}\omega_i.\)

因此，在[梯度下降时](https://zh.wikipedia.org/wiki/梯度下降 "wikilink")，参数\(\omega_{i}\)的更新

\(\omega'_{i}\gets \omega_i - \eta\frac{\partial L}{\partial \omega_i} - \eta\frac{\gamma_2}{n}\omega_i = \Bigl(1 - \eta\frac{\gamma_2}{n}\Bigr)\omega_{i} - \eta\frac{\partial L}{\partial \omega_i}.\)

注意到\(\eta\tfrac{\gamma_2}{n}\)通常是介于\((0,\,1)\)之间的数\[7\]，\(L_{2}\)-正则项会使得参数接近零，从而对抗过拟合。

\(L_{2}\)-正则项又称[Tikhonov](https://zh.wikipedia.org/wiki/吉洪诺夫正则化 "wikilink")-正则项或Ringe-正则项。

## 提前停止

可看做是时间维度上的正则化。直觉上，随着迭代次数的增加，如梯度下降这样的训练算法倾向于学习愈加复杂的模型。在实践维度上进行正则化有助于控制模型复杂度，提升泛化能力。在实践中，提前停止一般是在[训练集上进行训练](https://zh.wikipedia.org/wiki/训练集 "wikilink")，而后在统计上独立的[验证集上进行评估](https://zh.wikipedia.org/wiki/验证集 "wikilink")；当模型在验证集上的性能不在提升时，就提前停止训练。最后，可在[测试集上对模型性能做最后测试](https://zh.wikipedia.org/wiki/测试集 "wikilink")。

## 参考文献

## 外部链接

  - [谈谈 \(L_1\) 与 \(L_2\)-正则项](https://liam.page/2017/03/30/L1-and-L2-regularizer/)

[Category:機器學習](https://zh.wikipedia.org/wiki/Category:機器學習 "wikilink") [Category:计算机科学](https://zh.wikipedia.org/wiki/Category:计算机科学 "wikilink")

1.
2.
3.
4.  范数的非负性保证了范数有下界。当齐次性等式\(\lVert c\cdot\vec x\rVert = |c| \cdot \lVert\vec x\rVert\)中的\(c\)取零时可知，零向量的范数是零，这保证了范数有下确界。
5.
6.
7.  可通过恰当地调整学习率\(\eta\)与正则系数\(\gamma_{2}\)来满足这一点。
> 本文内容由[歸納偏置](https://zh.wikipedia.org/wiki/歸納偏置)转换而来。


當學習器去預測其未遇到過的輸入的結果時，會做一些假設（Mitchell, 1980）。而學習[演算法中的](https://zh.wikipedia.org/wiki/演算法 "wikilink")**歸納偏置**（）則是這些假設的集合。

[機器學習試圖去建造一個可以學習的演算法](https://zh.wikipedia.org/wiki/機器學習 "wikilink")，用來預測某個目標的結果。要達到此目的，要給於學習演算法一些訓練样本，样本說明輸入與輸出之間的預期關係。然后假设學習器在预测中逼近正确的结果，其中包括在訓練中未出現的样本。既然未知状况可以是任意的結果，若沒有其它額外的假設，這任務就無法解決。這種關於目標函數的必要假設就称为*歸納偏置*（Mitchell, 1980; desJardins and Gordon, 1995）。

一個典型的歸納偏置例子是[奧卡姆剃刀](https://zh.wikipedia.org/wiki/奧卡姆剃刀 "wikilink")，它假設最簡單而又一致的假设是最佳的。這裡的一致是指學習器的假设會對所有樣本產生正確的結果。

歸納偏置比較正式的定義是基於數學上的[邏輯](https://zh.wikipedia.org/wiki/邏輯 "wikilink")。這裡，歸納偏置是一個與訓練样本一起的邏輯式子，其邏輯上會蘊涵學習器所產生的假设。然而在实际应用中，這種嚴謹形式常常無法適用。在有些情况下，学习器的歸納偏置可能只是一個很粗糙的描述（如在[人工神經網路中](https://zh.wikipedia.org/wiki/人工神經網路 "wikilink")），甚至更加简单。

## 歸納偏置的種類

以下是機器學習中常見的歸納偏置列表：

  - **最大[條件獨立性](https://zh.wikipedia.org/wiki/條件獨立性 "wikilink")**（conditional independence）：如果假說能轉成[貝葉斯模型架構](../Page/贝叶斯概率.md "wikilink")，則試著使用最大化條件獨立性。這是用於[朴素貝葉斯分類器](https://zh.wikipedia.org/wiki/朴素貝葉斯分類器 "wikilink")（Naive Bayes classifier）的偏置。
  - **最小[交叉驗證](../Page/交叉驗證.md "wikilink")误差**：當試圖在假說中做選擇時，挑選那個具有最低交叉驗證误差的假說，雖然交叉驗證看起來可能無關偏置，但[天下没有免费的午餐理論顯示交叉驗證已是偏置的](https://zh.wikipedia.org/wiki/天下没有免费的午餐 "wikilink")。
  - **最大邊界**：當要在兩個類別間畫一道分界線時，試圖去最大化邊界的寬度。這是用於[支持向量機的偏置](https://zh.wikipedia.org/wiki/支持向量機 "wikilink")。這個假設是不同的類別是由寬界線來區分。
  - **[最小描述長度](../Page/最小描述長度.md "wikilink")**（Minimum description length）：當构成一個假设時，試圖去最小化其假设的描述長度。假设越简单，越可能為真的。見[奧卡姆剃刀](https://zh.wikipedia.org/wiki/奧卡姆剃刀 "wikilink")。
  - **最少特徵數**（Minimum features）：除非有充分的證據顯示一個特徵是有效用的，否則它應当被刪除。這是[特徵選择](https://zh.wikipedia.org/wiki/特徵選择 "wikilink")（feature selection）算法背後所使用的假設。
  - **最近鄰居**：假設在[特徵空間](https://zh.wikipedia.org/wiki/特徵空間 "wikilink")（feature space）中一小區域內大部分的样本是同屬一類。給一個未知類別的样本，猜測它與它最緊接的大部分鄰居是同屬一類。這是用於[最近鄰居法](../Page/最近鄰居法.md "wikilink")的偏置。這個假設是相近的样本應傾向同屬於一類別。

## 偏置变换

雖然大部分的學習演算法使用固定的偏置，但有些算法在获得更多数据時可以變換它們的偏置。這不會取消偏置，因為偏置变换的過程本身就是一種偏置。

## 另見

  - [Bias](https://zh.wikipedia.org/wiki/Bias "wikilink")
  - [認知偏誤](../Page/認知偏誤.md "wikilink") [:en:Cognitive bias](https://zh.wikipedia.org/wiki/:en:Cognitive_bias "wikilink")
  - [天下没有免费的午餐](https://zh.wikipedia.org/wiki/天下没有免费的午餐 "wikilink") [:en:No free lunch in search and optimization](https://zh.wikipedia.org/wiki/:en:No_free_lunch_in_search_and_optimization "wikilink")

## 參考文獻

desJardins, M., and Gordon, D.F. (1995). [Evaluation and selection of biases in machine learning](http://citeseer.ist.psu.edu/article/desjardins95evaluation.html). Machine Learning Journal, 5:1--17, 1995.

Mitchell, T.M. (1980). [The need for biases in learning generalizations](http://citeseer.ist.psu.edu/mitchell80need.html). CBM-TR 5-110, Rutgers University, New Brunswick, NJ.

Utgoff, P.E. (1984). Shift of bias for inductive concept learning. Doctoral dissertation, Department of Computer Science, Rutgers University, New Brunswick, NJ.

[Category:機器學習](https://zh.wikipedia.org/wiki/Category:機器學習 "wikilink")
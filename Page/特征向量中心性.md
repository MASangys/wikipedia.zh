在[图论](../Page/图论.md "wikilink")中，**特征向量中心性**(**eigenvector
centrality**)是测量[节点对](https://zh.wikipedia.org/wiki/节点 "wikilink")[网络影响的一种方式](../Page/社会网络.md "wikilink")。针对连接数相同的节点，相邻节点分数更高的节点会比相邻节点分数更低的节点分数高，依据此原则给所有节点分配对应的分数。特征向量得分较高意味着该节点与许多自身得分较高的节点相连接。\[1\]\[2\]

[谷歌的](https://zh.wikipedia.org/wiki/谷歌 "wikilink")[PageRank](../Page/PageRank.md "wikilink")和[Katz中心性是特征向量中心性的变体](https://zh.wikipedia.org/wiki/Katz中心性 "wikilink")。\[3\]

## 利用邻接矩阵求特征向量中心性

给定一个节点集合为\(|V|\)的图\(G=(V,E)\)，定义其[邻接矩阵](../Page/邻接矩阵.md "wikilink")为\(A = (a_{v,t})\)，当\(v\)与\(t\)相连时\(a_{v,t} = 1\)，否则\(a_{v,t} = 0\)。则节点\(v\)中心性\(x\)的分数其求解公式为：

  -
    \(x_v = \frac 1 \lambda \sum_{t \in M(v)} x_t = \frac 1 \lambda \sum_{t \in G} a_{v,t} x_t\)

其中\(M(v)\)是节点\(v\)的相邻节点集合，\(\lambda\)是一个常数。经过一系列变形，该公式可变换为如下所示的[特征向量方程](https://zh.wikipedia.org/wiki/特征向量 "wikilink")：

  -
    \(\mathbf{Ax} = \lambda \mathbf{x}\)

通常来说，有许多不同的特征值\(\lambda\)能使得一个特征方程有非零解存在。然而，考虑到特征向量中的所有项均为非负值，根据[佩伦-弗罗贝尼乌斯定理](https://zh.wikipedia.org/wiki/佩伦-弗罗贝尼乌斯定理 "wikilink")，只有特征值最大时才能测量出想要的中心性。然后通过计算网络中的节点\(v\)
其特征向量的相关分量\(v^\text{th}\)便能得出其对应的中心性的分数。特征向量的定义只有一个公因子，因此各节点中心性的比例可以很好确定。为了确定一个绝对分数，必须将其中一个特征值标准化，例如所有节点评分之和为1或者节点数 *n*。[幂次迭代是许多特征值算法中的一种](https://zh.wikipedia.org/wiki/冪次迭代 "wikilink")，该算法可以用来寻找这种主导特征向量。此外，以上方法可以推广，使得矩阵*A*中每个元素可以是表示连接强度的实数，例如[随机矩阵](https://zh.wikipedia.org/wiki/随机矩阵 "wikilink")。

## 应用

在[神经科学](../Page/神经科学.md "wikilink")中，研究发现一个模型神经网络其神经元的特征向量中心性与神经元的相对激发率有关。\[4\]

特征向量中心性最早在埃德蒙·兰道(Edmund Landau)于1895年发表的一篇关于国际象棋比赛计分的论文中使用过。\[5\]<ref>\<

1.
2.
3.
4.
5.
**-{zh-tw:資料; zh-hk:數據; zh-cn:数据;}-率失真理论**（Rate distortion
theory）或稱**信息率-失真理論**（information rate-distortion
theory）是[信息论](../Page/信息论.md "wikilink")的主要分支，其的基本问题可以归结如下：对于一个给定的信源（source,
input
signal）分布与[失真](../Page/失真.md "wikilink")度量，在特定的[码率下能达到的最小期望失真](https://zh.wikipedia.org/wiki/码率 "wikilink")；或者为了满足一定的失真限制，可允許的最大码率為何，D
定義為失真的符號。

要完全避免失真幾乎不可能。處理信號時必須允許有限度的失真﹐可減小所必需的信息率。1959年﹐Claude Shannon
首先發表《逼真度準則下的離散信源編碼定理》一文，提出了率失真函數的概念。

## 失真函數

失真函數能量化輸入與輸出的差異，以便進行數學分析。令輸入信號為\(\chi\)，輸出信號為\(\hat{\chi}\)，定義失真函數為\(d(\chi,\hat{\chi})\)，失真函數可以有多種定義，其與[對應域為非負實數](../Page/到达域.md "wikilink")：

\(d:\chi \times \hat{\chi}\rightarrow R_+\)。

### 漢明失真

漢明失真函數能描述錯誤率，定義為：

\(d(x,\hat{x}) = \begin{cases} 0, & \text{if }x=\hat{x} \\ 1, & \text{if }x\neq\hat{x} \end{cases}\)，

對漢明失真函數取[期望值](../Page/期望值.md "wikilink")即為傳輸錯誤率。

### 平方誤差失真

最常用於量測連續字符傳輸的失真，定義為：

\(d(x,\hat{x}) = (x-\hat{x})^2\)，

平方誤差失真函數不適用於語音或影像方面，因為人類感官對於語音或影像的平方誤差失真並不敏感。

## 率失真函數

下列是率與失真（rate and distortion）的最小化關係函數:

\[\inf_{Q_{Y|X}(y|x)} I_Q(Y;X)\ \mbox{subject to}\ D_Q \le D^*.\]

這裡 *Q*<sub>*Y* | *X*</sub>(*y* | *x*), 有時被稱為一個測試頻道 （test channel）,
係一種[條件機率之](https://zh.wikipedia.org/wiki/條件機率 "wikilink")[機率密度函數](../Page/機率密度函數.md "wikilink")
(PDF)，其中頻道輸出 (compressed signal) *Y* 相對於來源 (original signal) *X*, 以及
*I*<sub>*Q*</sub>(*Y* ; *X*)
是一種**[互信息](../Page/互信息.md "wikilink")**（Mutual
Information），在 *Y* 與 *X* 之間被定義為

\[I(Y;X) = H(Y) - H(Y|X) \,\]

此處的 *H*(*Y*) 與 *H*(*Y* | *X*) 是指信宿（output signal） *Y*
的[熵](../Page/熵.md "wikilink")（entropy）以及基於信源（source
signal）和信宿（output
signal）相關的[條件熵](https://zh.wikipedia.org/wiki/條件熵 "wikilink")（conditional
entropy）, 分別為:

\[H(Y) = - \int_{-\infty}^\infty P_Y (y) \log_{2} (P_Y (y))\,dy\]

\[H(Y|X) =
       - \int_{-\infty}^{\infty} \int_{-\infty}^\infty Q_{Y|X}(y|x) P_X (x) \log_{2} (Q_{Y|X} (y|x))\, dx\, dy.\]

這一樣來便可推導出率失真的公式, 相關表示如下:

\[\inf_{Q_{Y|X}(y|x)} E[D_Q[X,Y]] \mbox{subject to}\ I_Q(Y;X)\leq R.\]

這兩個公式之間互為可逆推。

### 無記憶(獨立) 高斯訊號來源

如果我們假設 *P*<sub>*X*</sub>(*x*)
是[常態分配並取](https://zh.wikipedia.org/wiki/常態分配 "wikilink")[方差](../Page/方差.md "wikilink")
σ<sup>2</sup>, 並且假設訊號 *X* 是連續
[統計獨立性](https://zh.wikipedia.org/wiki/統計獨立性 "wikilink")
(或等同於來源無記憶或訊號不相關), 我們可以發現下列的率失真公式之「公式解」（analytical expression）:

\[R(D) = \left\{ \begin{matrix}
  \frac{1}{2}\log_2(\sigma_x^2/D ), & \mbox{if } 0 \le D \le \sigma_x^2 \\  \\
              0,                             & \mbox{if } D > \sigma_x^2.
                      \end{matrix} \right.\]\[1\]

下圖是本公式的幾何面貌:

[<File:Rate> distortion
function.png](https://zh.wikipedia.org/wiki/File:Rate_distortion_function.png "fig:File:Rate distortion function.png")

率失真理論告訴我們“沒有壓縮系統存在於灰色區塊之外”。可以說越是接近紅色邊界，執行效率越好。一般而言，想要接近邊界就必須透過增加碼塊（coding
block）的長度參數。然而，塊長度（blocklengths）的取得則來自率失真公式的量化（quantizers）有關。\[2\]

這樣的率失真理論（rate–distortion function）僅適用於高斯無記憶信源（Gaussian memoryless
sources）。

### 二元信號源

[白努力信號源](../Page/伯努利分布.md "wikilink")\(X\)，\(X\thicksim Bernoulli(p)\)，以漢明失真描述的率失真函數為：

\(R(D) = \begin{cases} H(p)-H(D), & 0\leq D\leq min\{ p,1-p\} \\ 0, & D\geq min\{p,1-p\} \end{cases}\)

### 平行高斯信號源

平行高斯信號源的率失真函數為一經典的反注水算法(Reverse water-filling
algorithm)，我們可以找出一閾值\(\lambda\)，只有[方差](../Page/方差.md "wikilink")大於\(\lambda\)的信號源才有必要配置位元來描述，其他信號源則可直接傳送與接收，不會超過最大可容許的失真範圍。

我們可以使用平方誤差失真函數，計算平行高斯信號源的率失真函數。注意，此處信號源不一定同分佈：

\(X_1,X_2...,X_m\)且\(X_i\thicksim N(0,\sigma^2_i)\)，此時率失真函數為，

\(R(D)=\sum_{i=1}^m {1\over2}log{{\sigma^2_i}\over{D_i}}\)

其中，

\(D_i = \begin{cases} \lambda, & \text{if }{\lambda}<{{\sigma^2_i} } \\ \sigma^2_i, & \text{if }{\lambda}\geq{{\sigma^2_i} } \end{cases}\)

且\(\lambda\)必須滿足限制：

\(\sum_{i=1}^m D_i=D\)。

## 注釋

[Category:信號處理](https://zh.wikipedia.org/wiki/Category:信號處理 "wikilink")

1.

2.
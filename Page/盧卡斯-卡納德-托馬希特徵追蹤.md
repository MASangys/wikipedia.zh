在[電腦視覺](https://zh.wikipedia.org/wiki/電腦視覺 "wikilink")，**盧卡斯-卡納德-托馬希特徵追蹤**（[英文](https://zh.wikipedia.org/wiki/英文 "wikilink")：）是用來抽取特徵的一種方法，最早被提出是為了解決傳統上的影像配准問題，傳統的影像配准技術通常都需要耗費大量資源，盧卡斯-卡納德-托馬希特徵善用空間上的資訊，也因此在找匹配特徵的時候搜尋的數量較少，結果就會比較快。

## 配準問題

傳統的影像配準可以用下列的方式來描述:

x是一個向量，分別對應到兩張圖，\(F(x)\)和\(G(x)\)分別代表位置x的值，我們希望能找到視差向量\(h\)來最小化\(F(x+h)\)和\(G(x)\)的差異，\(x\)可能是在我們有興趣的一塊區域\(R\)

一些常見用來量測\(F(x+h)\)和\(G(x)\)差異的函式:

  - L<sub>1</sub> 范數: \(\sum_{x\in R}\left\vert F(x+h)-G(x) \right\vert\)
  - L<sub>2</sub> 范數: \(\sqrt{\sum_{x\in R}\left [F(x+h)-G(x)\right ]^{2}}\)
  - 標準化的負相關: \(\dfrac{-\sum_{x\in R}F(x+h)G(x)}{\sqrt{\sum_{x\in R}F(x+h)^{2}}\sqrt{\sum_{x\in R}G(x)^{2}}}\)

## 配準演算法的基礎描述

盧卡斯-卡納德-托馬希特徵追蹤\[1\]是建立在兩篇論文的研究成果，在第一篇，盧卡斯和卡納德提出用影像的二次微分當作權重來對影像作局部搜尋

### 一維實例

如果\(h\)是兩張影像的位移，那麼\(F(x)\)和\(G(x) = F(x+h)\)就能以下列的式子近似:

  -
    \(F'(x) \approx \dfrac{F(x+h)-F(x)}{h}=\dfrac{G(x)-F(x)}{h}\,\)

於是

  -
    \(h \approx \dfrac{G(x)-F(x)}{F'(x)}\,\)

然而通常這個近似只有在位移\(h\)不是太大的時候準確，因為在這個近似裡，不同的\(x\)值會影響\(h\)，因此我們通常會對\(h\)取平均:

  -
    \(h \approx \dfrac{\sum_{x}\dfrac{G(x)-F(x)}{F'(x)}}{\sum_{x}1}.\)

平均也可以更進一步寫成下列的形式\(\left \vert F''(x) \right \vert\)成反比，

  -
    \(F''(x) \approx \dfrac{G'(x)-F'(x)}{h}.\)

另外我們可以定一個權重函式讓表達更方面:

  -
    \(w(x) = \dfrac{1}{\left \vert G'(x)-F'(x) \right \vert}.\)

因此\(h\)也可以寫成:

  -
    \(h = \dfrac{\sum_{x}\dfrac{w(x)\left [ G(x)-F(x) \right ]}{F'(x)}}{\sum_{x}w(x)}.\)

接著，可以運用牛頓法的寫出下列的遞迴式，這個序列最後會收斂到最佳的\(h\)

\(\begin{cases}
h_{0} = 0 \\
h_{k+1} = h_{k} + \dfrac{\sum_{x}\dfrac{w(x)\left [ G(x)-F(x+h_{k})\right ]}{F'(x+h_{k})}}{\sum_{x}w(x)}
\end{cases}\)

### 另一種推導

上述的推導無法被一般化因為二維的線性近似不太依樣，因此近似要改成下列的式子:

  -
    \(F(x+h) \approx F(x)+hF'(x),\)

l2 泛數形式的誤差可以寫成下列

  -
    \(E=\sum_{x}\left [F(x+h)-G(x)\right ]^{2}.\)

為了得到找到慧滿足最小誤差的\(h\)，對\(E\)作偏微分並令為0:

  -
    <math>

\\begin{align}

`0 & = \dfrac{\partial E}{\partial h} \\`
`  & \approx \dfrac{\partial}{\partial h}\sum_{x}\left [F(x)+hF'(x)-G(x)\right ]^{2} \\`
`  & = \sum_{x}2F'(x)\left [F(x)+hF'(x)-G(x)\right ]`

\\end{align}</math>,

  -
    \(\Rightarrow h \approx \dfrac{\sum_{x} F'(x)[G(x)-F(x)]}{\sum_{x} F'(x)^{2}}\,\)

這個步驟基本上跟一維的實例是一樣的，只是權重函式必須寫成\(w(x)=F'(x)^2.\) 所以遞迴關係可以表達成:

\(\begin{cases}
h_0 = 0 \\
h_{k+1}=h_k + \dfrac{\sum_x w(x)F'(x+h_k) \left [G(x)-F(x+h_k)\right ]}{\sum_x w(x)F'(x+h_k)^2}
\end{cases}\)

### 效能

在評估這個演算法的效能時，我們通常會好奇\(h_k\)'可以多快收斂到真正的\(h\)。

如果我們看看下面這個例子:

  -
    \(F(x)=\sin x,\)
    \(G(x)=F(x+h)=\sin (x+h).\)

當\(\left\vert h\right\vert < \pi\)，兩種版本的配準演算法都會收斂到正確的\(h\)。我們利用壓抑影像中的高頻來改進收斂的範圍，也就是對影像作平滑化，當然同時一些細節也會喪失。但要注意的是，如果選用的平滑窗格比匹配的物體的大小大太多，物件可能會被壓縮太多，使得找不到對應的匹配。

由於經過低通濾波器的影像可以用更低的解析度去取樣，我們採用由粗到精的層次化匹配策略。一張平滑化的低解析度影像可以用來近似匹配，而之後再把演算法用在高解析度影像即可以讓前面算出的匹配更準。

權重函式加速了收斂速度也增加了近似的準度，如果沒有加權且\(F(x)=\sin x\)，當位移接近0.5個波長，計算出來的\(h_1\)便會來第一個遞迴變成0。

### 實作

實作盧卡斯-卡納德-托馬希特徵追蹤需要計算加權和\(F'G,\) \(F'F,\) and \((F')^2\) ，雖然\(F'(x)\)沒辦法準確地算出，卻可以用下式估計:

  -
    \(F'(x) \approx \dfrac{F(x+\Delta x)-F(x)}{\Delta x},\)

### 多維的一般化推廣

一維和二維的配準演算法可以延伸到多維，同樣地，我們也需要去最小化L<sub>2</sub>泛數

  -
    \(E=\sum_{\mathbf{x}\in R}\left [F(\mathbf{x}+\mathbf{h})-G(\mathbf{x})\right ]^{2},\)

\(\mathbf{x}\)和 \(\mathbf{h}\)代表n維的行向量

線性近似:

  -
    \(F(\mathbf{x}+\mathbf{h}) \approx F(\mathbf{x})+\mathbf{h}\left(\dfrac{\partial}{\partial \mathbf{x}}F(\mathbf{x})\right)^{T}.\)

接著將\(E\)對\(\mathbf{h}\)作偏微分:

  -
    <math>\\begin{align}

`0 & = \dfrac{\partial E}{\partial \mathbf{h}} \\`
`  & \approx \dfrac{\partial}{\partial \mathbf{h}}\sum_{\mathbf{x}}\left [F(\mathbf{x})+\mathbf{h}\left(\dfrac{\partial F}{\partial \mathbf{x}}\right)^{T}-G(\mathbf{x})\right ]^{2} \\`
`  & = \sum_{\mathbf{x}}2\left [F(\mathbf{x})+\mathbf{h}\left(\dfrac{\partial F}{\partial \mathbf{x}}\right)^{T}-G(\mathbf{x})\right ] \left(\dfrac{\partial F}{\partial \mathbf{x}}\right)`

\\end{align}</math>,

  -
    \(\Rightarrow \mathbf{h} \approx \left [\sum_{\mathbf{x}}\left [G(\mathbf{x})-F(\mathbf{x})\right ]\left (\dfrac{\partial F}{\partial\mathbf{x}}\right )\right ] \left [\sum_{\mathbf{x}}\left (\dfrac{\partial F}{\partial\mathbf{x}}\right )^{T}\left (\dfrac{\partial F}{\partial\mathbf{x}}\right )\right ]^{-1},\)

過程其實跟一維的推導很像。

### 更進一步的延伸

此方法也可以延伸到更複雜的矩陣變換，例如轉動、放大縮小、剪切

  -
    \(G(x) = F(Ax+h),\)

\(A\)是一個線性轉換，誤差可以表示成下列的式子:

  -
    \(E=\sum_{x}\left [F(Ax+h)-G(x)\right ]^2.\)

接著可以再次利用線性估計來決定\(\Delta A\)和\(\Delta h\)的值:

  -
    \(F(x(A+\Delta A)+(h+\Delta h))\)
    \(\approx F(Ax+h)+(\Delta Ax+\Delta h)\dfrac{\partial}{\partial x}F(x).\)

上述類似的近似手法也可以用來找誤差表達式，在這裡是個二次方程式，因此可藉由微分尋找最小值。

當兩張不同視角影像的亮度不同時，需要將線性轉換假設成

  -
    \(F(x)=\alpha G(x)+\beta,\)

\(\alpha\)代表對比度調整而\(\beta\)代表亮度調整

將此式與一般的線性轉換結合後，即可得

  -
    \(E=\sum_{x}\left [F(Ax+h)-(\alpha G(x)+\beta)\right ]^2\)

所以我們可以用 \(\alpha,\) \(\beta,\) \(A,\) 和\(h\)去最小化*E*

## 點特徵的偵測及追蹤

在另一篇論文裡\[2\]，托馬希和卡納德用類似的方法提出了另外一種特徵選取，如果其特徵值和梯度矩陣皆大於某個阈值，則選取這個特徵點，藉由與上述相似的推導，我們可以把問題寫成

  -
    \(\nabla d = e\,\)

在這裡\(\nabla\)代表梯度。很巧的是，此式與上面的最後一個盧卡斯和卡納德所提出的最後一個式子相同。如果梯度矩陣的兩個特徵值皆大於某阀值，則這個局部小塊就會被認為是良好的特徵點。

## 參考資料

<references />

## 更多

  - [Kanade–Tomasi features](https://zh.wikipedia.org/wiki/:en:Corner_detection#The_Shi_and_Tomasi_corner_detection_algorithm "wikilink") in the context of feature detection
  - [Lucas–Kanade method](https://zh.wikipedia.org/wiki/:en:Lucas–Kanade_method "wikilink") An optical flow algorithm derived from reference 1.

<!-- end list -->

1.  Bruce D. Lucas and Takeo Kanade. An Iterative Image Registration Technique with an Application to Stereo Vision. *International Joint Conference on Artificial Intelligence*, pages 674–679, 1981.
2.  Carlo Tomasi and Takeo Kanade. Detection and Tracking of Point Features. *Carnegie Mellon University Technical Report CMU-CS-91-132*, April 1991.
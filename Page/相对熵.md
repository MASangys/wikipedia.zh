**KL散度**（**Kullback–Leibler divergence**，簡稱**KLD**）\[1\]，在信息系统中称为**相对熵**（relative entropy），在连续时间序列中称为randomness，在统计模型推断中称为信息增益（information gain）。也称信息散度（information divergence）。

**KL散度**是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来度量使用基于Q的分布来编码服从P的分布的样本所需的额外的平均比特数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或P的近似分布。

[Solomon Kullback](https://zh.wikipedia.org/wiki/Solomon_Kullback "wikilink") and [Richard Leibler](https://zh.wikipedia.org/wiki/Richard_Leibler "wikilink") \[2\]

## 定義

對於[离散隨機变量](https://zh.wikipedia.org/wiki/离散隨機变量 "wikilink")，其概率分布*P* 和 *Q*的KL散度可按下式定義為

\[D_{\mathrm{KL}}(P\|Q) = -\sum_i P(i) \ln \frac{Q(i)}{P(i)}. \!\]

等价于

\[D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \ln \frac{P(i)}{Q(i)}. \!\]

即按概率*P*求得的*P*和*Q*的[對數商的平均值](https://zh.wikipedia.org/wiki/對數 "wikilink")。KL散度僅當概率*P*和*Q*各自總和均為1，且對於任何*i*皆滿足\(Q(i)>0\)及\(P(i)>0\)時，才有定義。式中出現\(0 \ln 0\)的情況，其值按0處理。

對於[連續隨機變量](https://zh.wikipedia.org/wiki/連續隨機變量 "wikilink")，其概率分佈*P*和*Q*可按積分方式定義為 \[3\]

  -
    \(D_{\mathrm{KL}}(P\|Q) = \int_{-\infty}^\infty p(x) \ln \frac{p(x)}{q(x)} \, {\rm d}x, \!\)

其中*p*和*q*分別表示分佈*P*和*Q*的密度。

更一般的，若*P*和*Q*為集合*X*的概率[測度](https://zh.wikipedia.org/wiki/測度 "wikilink")，且*P*關於*Q*[絕對連續](../Page/绝对连续.md "wikilink")，則從*P*到*Q*的KL散度定義為

\[D_{\mathrm{KL}}(P\|Q) = \int_X \ln \frac{{\rm d}P}{{\rm d}Q} \,{\rm d}P, \!\]

其中，假定右側的表達形式存在，則\(\frac{{\rm d}Q}{{\rm d}P}\)為*Q*關於*P*的[R–N導數](https://zh.wikipedia.org/wiki/拉东-尼科迪姆定理 "wikilink")。

相應的，若*P*關於*Q*[絕對連續](../Page/绝对连续.md "wikilink")，則

\[D_{\mathrm{KL}}(P\|Q) = \int_X \ln \frac{{\rm d}P}{{\rm d}Q} \,{\rm d}P
                      = \int_X \frac{{\rm d}P}{{\rm d}Q} \ln\frac{{\rm d}P}{{\rm d}Q}\,{\rm d}Q,\]

即為*P*關於*Q*的相對熵。

## 特性

[相對熵的值為非負數](https://zh.wikipedia.org/wiki/相對熵 "wikilink")：

\[D_{\mathrm{KL}}(P\|Q) \geq 0, \,\] 由[吉布斯不等式](../Page/吉布斯不等式.md "wikilink")可知，當且僅當*P* = *Q*時*D*<sub>KL</sub>(*P*||*Q*)為零。

尽管从直觉上KL散度是个[度量](../Page/度量.md "wikilink")或距离函数, 但是它实际上并不是一个真正的度量或距離。因為KL散度不具有对称性：从分布*P*到*Q*的距离通常并不等于从*Q*到*P*的距离。

\[D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P)\]

## KL散度和其它量的关系

[自信息](../Page/自信息.md "wikilink")和KL散度

\[I(m) = D_{\mathrm{KL}}(\delta_{im} \| \{ p_i \}),\]
[互信息](../Page/互信息.md "wikilink")和KL散度

\[\begin{align}I(X;Y) & = D_{\mathrm{KL}}(P(X,Y) \| P(X)P(Y) ) \\
& = \mathbb{E}_X \{D_{\mathrm{KL}}(P(Y|X) \| P(Y) ) \} \\
& = \mathbb{E}_Y \{D_{\mathrm{KL}}(P(X|Y) \| P(X) ) \}\end{align}\]
[信息熵和KL散度](https://zh.wikipedia.org/wiki/信息熵 "wikilink")

\[\begin{align}H(X) & = \mathrm{(i)} \, \mathbb{E}_x \{I(x)\} \\
& = \mathrm{(ii)} \log N - D_{\mathrm{KL}}(P(X) \| P_U(X) )\end{align}\]
[条件熵](../Page/条件熵.md "wikilink")和KL散度

\[\begin{align}H(X|Y) & = \log N - D_{\mathrm{KL}}(P(X,Y) \| P_U(X) P(Y) ) \\
& = \mathrm{(i)} \,\, \log N - D_{\mathrm{KL}}(P(X,Y) \| P(X) P(Y) ) - D_{\mathrm{KL}}(P(X) \| P_U(X)) \\
& = H(X) - I(X;Y) \\
& = \mathrm{(ii)} \, \log N - \mathbb{E}_Y \{ D_{\mathrm{KL}}(P(X|Y) \| P_U(X)) \}\end{align}\]
[交叉熵](../Page/交叉熵.md "wikilink")和KL散度

\[\mathrm{H}(p, q) = \mathrm{E}_p[-\log q] = \mathrm{H}(p) + D_{\mathrm{KL}}(p \| q).\!\]

## 參考文獻

[Category:概率与统计](https://zh.wikipedia.org/wiki/Category:概率与统计 "wikilink") [Category:應用數學](https://zh.wikipedia.org/wiki/Category:應用數學 "wikilink") [Category:概率论](https://zh.wikipedia.org/wiki/Category:概率论 "wikilink") [Category:信息论](https://zh.wikipedia.org/wiki/Category:信息论 "wikilink") [Category:信息學熵](https://zh.wikipedia.org/wiki/Category:信息學熵 "wikilink")

1.
2.
3.  C. Bishop (2006). Pattern Recognition and Machine Learning. p. 55.
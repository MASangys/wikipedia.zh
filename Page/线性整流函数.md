{{ request translation }}
[线性整流函数](https://zh.wikipedia.org/wiki/File:Ramp_function.svg "fig:线性整流函数")

**线性整流函数**（Rectified Linear Unit, **ReLU**）,又称**修正线性单元**,
是一种[人工神经网络中常用的激活函数](../Page/人工神经网络.md "wikilink")（activation
function），通常指代以[斜坡函数及其变种为代表的非线性函数](../Page/斜坡函数.md "wikilink")。

比较常用的线性整流函数有[斜坡函数](../Page/斜坡函数.md "wikilink")
\(f(x) = \max(0, x)\)，以及带泄露整流函数 (Leaky ReLU)，其中 \(x\)
为神经元(Neuron)的输入。线性整流被认为有一定的生物学原理\[1\]，并且由于在实践中通常有着比其他常用激活函数（譬如[逻辑函数](https://zh.wikipedia.org/wiki/逻辑函数 "wikilink")）更好的效果，而被如今的[深度神经网络广泛使用于诸如图像识别等](https://zh.wikipedia.org/wiki/深度学习 "wikilink")[计算机视觉](../Page/计算机视觉.md "wikilink")\[2\]人工智能领域。

## 定义

通常意义下，线性整流函数指代数学中的[斜坡函数](../Page/斜坡函数.md "wikilink")，即

\[f(x) = \max(0, x)\]

而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换
\(\mathbf{w}^T \mathbf{x} + b\)之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量
\(x\)，使用线性整流激活函数的神经元会输出

\[\max(0, \mathbf{w}^T \mathbf{x} + b)\]

至下一层神经元或作为整个神经网络的输出（取决现神经元在网络结构中所处位置）。

## 变种

线性整流函数在基于[斜坡函数的基础上有其他同样被广泛应用于深度学习的变种](../Page/斜坡函数.md "wikilink")，譬如带泄露线性整流(Leaky
ReLU)\[3\]， 带泄露随机线性整流(Randomized Leaky ReLU)\[4\]，以及噪声线性整流(Noisy
ReLU)\[5\].

### 带泄露线性整流

在输入值 \(x\) 为负的时候，**带泄露线性整流函数**（Leaky ReLU）的梯度为一个常数
\(\lambda \in (0, 1)\)，而不是0。在输入值为正的时候，带泄露线性整流函数和普通斜坡函数保持一致。换言之，

\[f(x)  = \begin{cases}
    x & \mbox{if } x > 0 \\
    \lambda x & \mbox{if } x \leq 0
\end{cases}\]

在深度学习中，如果设定 \(\lambda\)
为一个可通过[反向传播算法](../Page/反向传播算法.md "wikilink")（Backpropagation）学习的变量，那么带泄露线性整流又被称为**参数线性整流**（Parametric
ReLU）\[6\]。

### 带泄露随机线性整流

**带泄露随机线性整流**（Randomized Leaky ReLU,
**RReLU**）最早是在Kaggle全美数据科学大赛（NDSB）中被首先提出并使用的。相比于普通带泄露线性整流函数，带泄露随机线性整流在负输入值段的函数梯度
\(\lambda\)
是一个取自[连续性均匀分布](https://zh.wikipedia.org/wiki/连续性均匀分布 "wikilink")
\(U(l,u)\) 概率模型的随机变量，即

\[f(x)  = \begin{cases}
    x & \mbox{if } x > 0 \\
    \lambda x & \mbox{if } x \leq 0
\end{cases}\]

其中 \(\lambda \sim U(l,u), l < u\) 且 \(l,u \in [0,1)\)。

### 噪声线性整流

**噪声线性整流**（Noisy
ReLU）是修正线性单元在考虑[高斯噪声的基础上进行改进的变种激活函数](https://zh.wikipedia.org/wiki/高斯噪声 "wikilink")。对于神经元的输入值
\(x\)，噪声线性整流加上了一定程度的正态分布的不确定性，即

\[f(x)=\max(0, x+Y)\]

其中随机变量
\(Y \sim \mathcal{N}(0, \sigma(x))\)。目前，噪声线性整流函数在[受限玻尔兹曼机](https://zh.wikipedia.org/wiki/受限玻尔兹曼机 "wikilink")（Restricted
Boltzmann Machine）在计算机图形学的应用中取得了比较好的成果\[7\]。

## 优势

相比于传统的神经网络激活函数，诸如[逻辑函数](https://zh.wikipedia.org/wiki/逻辑函数 "wikilink")（Logistic
sigmoid）和tanh等[双曲函数](../Page/双曲函数.md "wikilink")，线性整流函数有着以下几方面的优势：

  - 仿生物学原理：相关大脑方面的研究表明生物神经元的信息编码通常是比较分散及稀疏的\[8\]。通常情况下，大脑中在同一时间大概只有1%-4%的神经元处于活跃状态。使用线性修正以及正则化（regularization）可以对机器神经网络中神经元的活跃度（即输出为正值）进行调试；相比之下，逻辑函数在输入为0时达到
    \(\frac{1}{2}\)，即已经是半饱和的稳定状态，不够符合实际生物学对模拟神经网络的期望\[9\]。不过需要指出的是，一般情况下，在一个使用修正线性单元（即线性整流）的神经网络中大概有50%的神经元处于激活态\[10\]。

<!-- end list -->

  - 更加有效率的[梯度下降以及反向传播](https://zh.wikipedia.org/wiki/梯度下降法 "wikilink")：避免了梯度爆炸和[梯度消失问题](https://zh.wikipedia.org/wiki/梯度消失问题 "wikilink")

<!-- end list -->

  - 简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；同时活跃度的分散性使得神经网络整体计算成本下降

## 参考资料

## 外部链接

  - [Quora: What is special about rectifier neural units used in NN
    learning?](https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning)

[Category:使用创建条目精灵建立的页面](https://zh.wikipedia.org/wiki/Category:使用创建条目精灵建立的页面 "wikilink")
[Category:人工神经网络](https://zh.wikipedia.org/wiki/Category:人工神经网络 "wikilink")

1.

2.
3.

4.

5.

6.

7.
8.

9.
10.
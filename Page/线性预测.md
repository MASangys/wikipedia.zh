**线性预测**是根据已有采样点按照[线性函数计算未来某一](https://zh.wikipedia.org/wiki/线性算子 "wikilink")[离散信号的数学方法](https://zh.wikipedia.org/wiki/离散信号 "wikilink")。

在[数字信号处理](../Page/数字信号处理.md "wikilink")中，线性预测经常称为[线性预测编码](../Page/线性预测编码.md "wikilink")（LPC），因此也可以看作是[数字滤波器](../Page/数字滤波器.md "wikilink")的一部分。在[系统分析](../Page/系统分析.md "wikilink")中，线性预测可以看作是[数学建模](../Page/数学建模.md "wikilink")或者[最优化](../Page/最优化.md "wikilink")的一部分。

## 预测模型

最常见的表示是

\[\widehat{x}(n) = -\sum_{i=1}^p a_i x(n-i)\]，

其中\(\widehat{x}(n)\)是预测的信号值，\(x(n-i)\)是前面观测到的值，\(a_i\)是预测系数。这种预测产生的误差是

\[e(n) = x(n) - \widehat{x}(n)\]，

其中\(x_n\)是真正的信号值。

这个等式对于所有类型的一维线性预测都是有效的，它们的不同之处是参数\(a_i\)选择方式的不同。

对于多维信号，误差经常定义为

\[e(n) = ||x(n) - \widehat{x}(n)||\]，

其中\(||.||\)是适当选择的矢量[范数](../Page/范数.md "wikilink")。

## 预测参数

在参数\(a_i\)优化中最常见的选择是[均方根准则](https://zh.wikipedia.org/wiki/均方根 "wikilink")，也称为[自相关准则](https://zh.wikipedia.org/wiki/自相关 "wikilink")。在这种方法中减小了最小均方误差E\[e<sup>2</sup>(n)\]的期望值，这样就得到等式

\[\sum_{i=1}^p a_i R(i-j) = -R(j)\]，

对于1 ≤ *j* ≤ *p*,其中*R*是信号*x*<sub>*n*</sub>的[自相关](https://zh.wikipedia.org/wiki/自相关 "wikilink")，定义为

\[\ R(i) = E\{x(n)x(n-i)\}\]，

其中*E*是[期望值](../Page/期望值.md "wikilink")。在多维情况下，这相当于最小化[L<sub>2</sub>范数](../Page/Lp空间.md "wikilink")。

上面的方程称为normal方程或者Yule-Walker方程，在矩阵形式下这个方程也可以写作

\[Ra = -r,\]，

其中自相关矩阵*R*是元素为*r*<sub>*i*,*j*</sub> = *R*(*i* − *j*)的对称[轮换矩阵](https://zh.wikipedia.org/wiki/轮换矩阵 "wikilink")（[:en:circulant matrix](https://zh.wikipedia.org/wiki/:en:circulant_matrix "wikilink")），矢量*r*是自相关矢量*r*<sub>*j*</sub> = *R*(*j*)，矢量*a*是参数矢量。

另外一个更为通用的实现是最小化

\[e(n) = x(n) - \widehat{x}(n) = x(n) + \sum_{i=1}^p a_i x(n-i) = \sum_{i=0}^p a_i x(n-i)\]

其中通常使用\(a_0=1\)约束参数\(a_i\)以避免trivial解。这个约束产生与上面同样的预测，但是normal方程是

\[\ Ra = [1, 0, ... , 0]^{\mathrm{T}}\]

其中索引*i*的范围是从0到*p*，并且*R*是 (*p* + 1)×(*p* + 1)矩阵。

参数优化是一个非常广泛的话题，人们已经提出了大量的其它实现方法。

但是，自相关方法仍然是最为常用的方法，例如在[GSM](../Page/GSM.md "wikilink")标准中的[语音编码就在使用这种方法](https://zh.wikipedia.org/wiki/语音编码 "wikilink")。

矩阵方程*Ra* = *r*的求解计算上工作量很大，[高斯消去法](../Page/高斯消去法.md "wikilink")求矩阵的逆可能是最为古老的解法了，但是这种方法没有有效地利用*R*和*r*的对称性。一种更快的算法是[Norman Levinson在](https://zh.wikipedia.org/wiki/Norman_Levinson "wikilink")1947年提出的[Levinson递归法](https://zh.wikipedia.org/wiki/Levinson递归法 "wikilink")（[:en:Levinson recursion](https://zh.wikipedia.org/wiki/:en:Levinson_recursion "wikilink")），它递归地计算方程的解。后来[Delsarte](https://zh.wikipedia.org/wiki/Philippe_Delsarte "wikilink") et al.提出了一种称为[split Levinson recursion的改进方法](https://zh.wikipedia.org/wiki/split_Levinson_recursion "wikilink")，它仅需要一半的乘除计算量，它在随后的递归层面上使用了参数矢量的特殊对称特性。

## 参见

  - [:en:Forecasting](https://zh.wikipedia.org/wiki/:en:Forecasting "wikilink")
  - [线性回归](https://zh.wikipedia.org/wiki/线性回归 "wikilink")
  - [预测区间](https://zh.wikipedia.org/wiki/预测区间 "wikilink")
  - [去卷积](https://zh.wikipedia.org/wiki/去卷积 "wikilink")

## 参考文献

  - G. U. Yule. On a method of investigating periodicities in disturbed series, with special reference to wolfer’s sunspot numbers. Phil. Trans. Roy. Soc., 226-A:267–298, 1927.
  - J. Makhoul. Linear prediction: A tutorial review. Proceedings of the IEEE, 63 (5):561–580, April 1975.
  - M. H. Hayes. Statistical Digital Signal Processing and Modeling. J. Wiley & Sons, Inc., New York, 1996.

[Category:时间序列](https://zh.wikipedia.org/wiki/Category:时间序列 "wikilink") [Category:信号处理](https://zh.wikipedia.org/wiki/Category:信号处理 "wikilink") [Category:估计理论](https://zh.wikipedia.org/wiki/Category:估计理论 "wikilink") [Category:回归分析](https://zh.wikipedia.org/wiki/Category:回归分析 "wikilink")
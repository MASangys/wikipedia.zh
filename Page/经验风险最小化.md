> 本文内容由[经验风险最小化](https://zh.wikipedia.org/wiki/经验风险最小化)转换而来。


**经验风险最小化** （ERM）是[统计学习理论](../Page/统计学习理论.md "wikilink")里的一项原则，该原则下有一系列[学习算法](../Page/机器学习.md "wikilink") ，经验风险最小化用于为这些算法的性能提供理论上的界。核心思想是我们无法确切知道算法在实际中的运行情况（真正的“风险”），因为我们不知道算法将在其上运行的数据的真实分布，但我们可以在一组已知的训练数据（“经验”风险）上衡量其性能。

## 背景

以下情况是许多[有监督学习问题的一般设置](../Page/監督式學習.md "wikilink")。我们有两个空间，输入空间\(X\)和输出空间\(Y\)，想学习一个函数\(\ h: X \to Y\) （通常称为*假设* ），这个函数在给定\(x \in X\)，输出一个对象\(y \in Y\) 。为此，我们可以使用一个包含 \(n\)个例子的*训练集*\(\ (x_1, y_1), \ldots, (x_n, y_n)\)其中\(x_i \in X\)是输入， \(y_i \in Y\)是我们希望从\(\ h(x_i)\)中得到的相应输出 。

更正式地说，我们假设在\(X\)和\(Y\)存在[联合概率分布](../Page/联合分布.md "wikilink") \(P(x, y)\) ，并且训练集包括\(n\)个实例\(\ (x_1, y_1), \ldots, (x_n, y_n)\)[IID地从](../Page/独立同分布.md "wikilink")\(P(x, y)\) 抽取。请注意，联合概率分布的假设使我们可以对预测中的不确定性进行建模（例如，来自数据中的噪声），因为\(y\)不是关于\(x\)的确定性函数 ，而是在固定\(x\) 时具有\[\[条件概率分布|条件分布\(P(y]]的[[随机变量|随机变量]] 。

我们还假定给定非负实值[[损失函数|损失函数]] <math>L(\hat{y}, y)\)来衡量预测\(\hat{y}\)与真实结果\(y\)的差异。则假设\(h(x)\)的风险 定义为损失函数的[期望值](../Page/期望值.md "wikilink") ：

  -
    \(R(h) = \mathbf{E}[L(h(x), y)] = \int L(h(x), y)\,dP(x, y).\)

理论上常用的损失函数是[0-1损失函数](https://zh.wikipedia.org/wiki/损失函数 "wikilink") ： \(L(\hat{y}, y) = \begin{cases} 1 & \mbox{ If }\quad \hat{y} \ne y \\ 0 & \mbox{ If }\quad \hat{y} = y \end{cases}\) 。

学习算法的最终目标是在固定函数类\(\mathcal{H}\)中找到风险\(R(h)\)最小的假设\(h^*\)：

  -
    \(h^* = \arg \min_{h \in \mathcal{H}} R(h).\)

## 经验风险最小化

通常，无法计算风险\(R(h)\)，因为学习算法不知道分布\(P(x, y)\)（这种情况称为无知学习）。但是，我们可以通过对训练集上的损失函数取平均值来计算一个近似值，称为*经验风险*：

  -
    \(\! R_\text{emp}(h) = \frac{1}{n} \sum_{i=1}^n L(h(x_i), y_i).\)

经验风险最小化原理\[1\]指出学习算法应选择一个假设\(\hat{h}\)将经验风险降到最低：

  -
    \(\hat{h} = \arg \min_{h \in \mathcal{H}} R_{\text{emp}}(h).\)

因此，由ERM原理定义的学习算法在于解决上述[优化问题](../Page/最优化.md "wikilink")。

## 性质

### 计算复杂度

对于具有[0-1损失函数的分类问题](https://zh.wikipedia.org/wiki/损失函数 "wikilink")，即使对于像[线性分类器](../Page/线性分类器.md "wikilink")这样的相对简单的函数类，经验风险最小化也被认为是[NP难题](../Page/NP困难.md "wikilink")。 \[2\]但是，当最小经验风险为零（即数据是线性可分离的）时，可以有效解决。

在实践中，机器学习算法可以通过对0-1损失函数（例如[SVM的](../Page/支持向量机.md "wikilink") [铰链损失](../Page/Hinge_loss.md "wikilink") ）采用[凸近似来解决该问题](../Page/凸優化.md "wikilink")，这种方法更容易优化，或者对分布进行假设\(P(x, y)\) （因此不再是上述结果适用的不可知论学习算法）。

## 參見

  - [最大似然估计](../Page/最大似然估计.md "wikilink")
  - M估计器

## 参考文献

## 进一步阅读

  - <bdi>  </bdi>

[Category:机器学习](https://zh.wikipedia.org/wiki/Category:机器学习 "wikilink")

1.  V. Vapnik (1992). \[<http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf> *Principles of Risk Minimization* for Learning Theory.*\]*
2.  V. Feldman, V. Guruswami, P. Raghavendra and Yi Wu (2009). [*Agnostic Learning of Monomials by Halfspaces is Hard.*](https://zh.wikipedia.org/wiki/arxiv:1012.0729 "wikilink") (See the paper and references therein)
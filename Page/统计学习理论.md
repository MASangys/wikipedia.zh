**统计学习理论**（），一種[機器學習的架構](https://zh.wikipedia.org/wiki/機器學習 "wikilink")，根據[統計學與](https://zh.wikipedia.org/wiki/統計學 "wikilink")[泛函分析](../Page/泛函分析.md "wikilink")（Functional
Analysis）而建立。統計學習理論基於資料（data），找出預測性函數，之後解決問題。[支持向量机](../Page/支持向量机.md "wikilink")（Support
Vector Machine）的理論基礎來自於統計學習理論。

## 形式定义

令\(X\)为所有可能的输入组成的向量空间，
\(Y\)为所有可能的输出组成的向量空间。统计学习理论认为，积空间\(Z = X \times Y\)上存在某个未知的概率分布\(p(z) = p(\vec{x},y)\)。训练集由这个概率分布中的\(n\)个样例构成，并用\(S = \{(\vec{x}_1,y_1), \dots ,(\vec{x}_n,y_n)\} = \{\vec{z}_1, \dots ,\vec{z}_n\}\)表示。每个\(\vec{x}_i\)都是训练数据的一个输入向量，
而\(y_i\)则是对应的输出向量。

## 损失函数

损失函数的选择是机器学习算法所选的函数\(f_S\)中的决定性因素。 损失函数也影响着算法的收敛速率。损失函数的凸性也十分重要。\[1\]

根据问题是回归问题还是分类问题，我们可以使用不同的损失函数。

### 回归问题

回归问题中最常用的损失函数是平方损失函数（也被称为[L2-范数](../Page/范数.md "wikilink"))。类似的损失函数也被用在[普通最小二乘回归](https://zh.wikipedia.org/wiki/普通最小二乘回归 "wikilink")。其形式是：

\[V(f(\vec{x}),y) = (y - f(\vec{x}))^2\]

另一个常见的损失函数是绝对值范数（[L1-范数](../Page/范数.md "wikilink")）：

\[V(f(\vec{x}),y) = |y - f(\vec{x})|\]

### 分类问题

某种程度上说0-1[指示函数是分类问题中最自然的损失函数](../Page/指示函数.md "wikilink")。它在预测结果与真实结果相同时取0，相异时取1。对于\(Y = \{-1, 1\}\)的二分类问题，这可以表示为：

\[V(f(\vec{x}),y) = \theta(- y f(\vec{x}))\]
其中\(\theta\)为[单位阶跃函数](../Page/单位阶跃函数.md "wikilink")。

### 正则化

[Overfitting_on_Training_Set_Data.pdf](https://zh.wikipedia.org/wiki/File:Overfitting_on_Training_Set_Data.pdf "fig:Overfitting_on_Training_Set_Data.pdf")

机器学习的一大常见问题是[过拟合](https://zh.wikipedia.org/wiki/过拟合 "wikilink")。由于机器学习是一个预测问题，其目标并不是找到一个与（之前观测到的）数据最拟合的的函数，而是寻找一个能对未来的输入作出最精确预测的函数。[经验风险最小化有过拟合的风险](https://zh.wikipedia.org/wiki/经验风险最小化 "wikilink")：找到的函数完美地匹配现有数据但并不能很好地预测未来的输出。

过拟合的常见表现是不稳定的解：训练数据的一个小的扰动会导致学到的函数的巨大波动。可以证明，如果解的稳定性可以得到保证，那么其可推广性和一致性也同样能得到保证。\[2\]\[3\]
[正则化可以解决过拟合的问题并增加解的稳定性](https://zh.wikipedia.org/wiki/正则化_\(数学\) "wikilink")。

正则化可以通过限制假设空间\(\mathcal{H}\)来完成。一个常见的例子是把\(\mathcal{H}\)限制为线性函数：这可以被看成是把问题简化为标准设计的[线性回归](https://zh.wikipedia.org/wiki/线性回归 "wikilink")。\(\mathcal{H}\)也可以被限制为\(p\)次多项式，指数函数，或[L1上的有界函数](../Page/Lp空间.md "wikilink")。对假设空间的限制能防止过拟合的原因是，潜在的函数的形式得到了限制，因此防止了那些能给出任意接近于0的经验风险的复杂函数。

一个正则化的样例是[吉洪诺夫正则化](https://zh.wikipedia.org/wiki/吉洪诺夫正则化 "wikilink")，即最小化如下损失函数

\[\frac{1}{n} \displaystyle \sum_{i=1}^n V(f(\vec{x}_i),y_i) + \gamma
\|f\|_{\mathcal{H}}^2\]
其中正则化参数\(\gamma\)为一个固定的正参数。吉洪诺夫正则化保证了解的存在性、唯一性和稳定性。\[4\]

[Category:机器学习](https://zh.wikipedia.org/wiki/Category:机器学习 "wikilink")

1.  Rosasco, L., Vito, E.D., Caponnetto, A., Fiana, M., and Verri A.
    2004. *Neural computation* Vol 16, pp 1063-1076
2.  Vapnik, V.N. and Chervonenkis, A.Y. 1971. [On the uniform
    convergence of relative frequencies of events to their
    probabilities](http://ai2-s2-pdfs.s3.amazonaws.com/a36b/028d024bf358c4af1a5e1dc3ca0aed23b553.pdf).
    *Theory of Probability and its Applications* Vol 16, pp 264-280.
3.  Mukherjee, S., Niyogi, P. Poggio, T., and Rifkin, R. 2006. Learning
    theory: stability is sufficient for generalization and necessary and
    sufficient for consistency of empirical risk minimization. *Advances
    in Computational Mathematics*. Vol 25, pp 161-193.
4.  Tomaso Poggio, Lorenzo Rosasco, et al. *Statistical Learning Theory
    and Applications*, 2012,
    [Class 2](http://www.mit.edu/~9.520/spring12/slides/class02/class02.pdf)
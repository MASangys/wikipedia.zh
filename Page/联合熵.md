[Entropy-mutual-information-relative-entropy-relation-diagram.svg](https://zh.wikipedia.org/wiki/File:Entropy-mutual-information-relative-entropy-relation-diagram.svg "fig:Entropy-mutual-information-relative-entropy-relation-diagram.svg")

**联合[熵](../Page/熵_\(信息论\).md "wikilink")**是一集变量之间不确定性的衡量手段。

## 定义

两个变量\(X\) 和 \(Y\) 的联合[信息熵定义为](../Page/信息熵.md "wikilink")：

\[\Eta(X,Y) = -\sum_{x} \sum_{y} P(x,y) \log_2[P(x,y)] \!\]

其中 \(x\) 和 \(y\) 是 \(X\) 和 \(Y\)的特定值, 相应地, \(P(x,y)\)
是这些值一起出现的[联合概率](../Page/联合分布.md "wikilink"),
若\(P(x,y)=0\) 为0，则\(P(x,y) \log_2[P(x,y)]\) 定义为0。

对于两个以上的变量 \(X_1, ..., X_n\) ，该式的一般形式为：

\[\Eta(X_1, ..., X_n) = -\sum_{x_1} ... \sum_{x_n} P(x_1, ..., x_n) \log_2[P(x_1, ..., x_n)] \!\]

其中 \(x_1,...,x_n\) 是 \(X_1,...,X_n\) 的特定值，相应地，\(P(x_1, ..., x_n)\)
是这些变量同时出现的概率，若\(P(x_1, ..., x_n)=0\)为0，则
\(P(x_1, ..., x_n) \log_2[P(x_1, ..., x_n)]\) 被定义为0.

## 性質

### 大于每个独立的熵

一集变量的联合熵大于或等于这集变量中任一个的独立熵。

\[\Eta(X,Y) \geq \max[\Eta(X),\Eta(Y)]\]

\[\Eta(X_1, ..., X_n) \geq \max[H(X_1), ..., H(X_n)]\]

### 少于或等于独立熵的和

一集变量的联合熵少于或等于这集变量的独立熵之和。这是[次可加性的一个例子](../Page/次加性.md "wikilink")。该不等式有且只有在\(X\)和\(Y\)均为统计独立的时候相等。

\[\Eta(X,Y) \leq \Eta(X) + \Eta(Y)\]

\[\Eta(X_1, ..., X_n) \leq \Eta(X_1) + ... + \Eta(X_n)\]

## 与其他熵测量手段的关系

在[条件熵的定义中](../Page/条件熵.md "wikilink")，使用了联合熵

\[\Eta(X|Y) = \Eta(X,Y) - \Eta(Y)\,\]

[互信息的定义中也出现了联合熵的身影](../Page/互信息.md "wikilink")：

\[I(X;Y) = \Eta(X) + \Eta(Y) - \Eta(X,Y)\,\]

在[量子信息理论中](../Page/量子信息.md "wikilink")， 联合熵被扩展到。

[Category:信息學熵](https://zh.wikipedia.org/wiki/Category:信息學熵 "wikilink")
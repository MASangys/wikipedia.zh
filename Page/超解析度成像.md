**超解析度成像**（Super-resolution
imaging，縮寫SR），是一種提高影片解析度的技術。在一些稱為「光學SR」的SR技術中，系統的繞射極限被超越；而在其他所謂的「幾何SR」中，數位[感光元件的分辨率因而提高](https://zh.wikipedia.org/wiki/感光元件 "wikilink")。超解析度成像技術用於一般圖像處理和超高解析度顯微鏡。

## 深度神經網路相關技術

隨著神經網路的流行，相關技術也被應用在提高圖片解析度。

### SRCNN\[1\]

SRCNN ( Super-resolution convolution neural network
)是一個神經網路，輸入是一個低解析度（视觉上）的圖像，而輸出是一個高解析度的圖像，這裡需要注意的是，在將圖像餵進神經網路前，需要先經過一個預處理bicubic
interpolation，將原始圖片變成跟想要的高解析度圖像一樣大小後，再餵進神經網路中。而神經網路做的事情，主要分成三個步驟區塊特徵抽取與表達（Patch
extraction and representation）、非線性對應（non-linear
mapping）以及重建（reconstruction）。

#### 區塊特徵抽取與表達（Patch extraction and representation）

這一步就如同一般的CNN ( convolution neural network )，只是沒有經過max-pooling，公式如下。

<center>

\(F_1(Y) = \max(0, W_1 \ast Y + B_1)\)

</center>

\(Y\)代表已經經過bicubic
interpolation的圖像，\(F_1(Y)\)則為這層神經網路的輸出，\(W_1\)代表\(n_1\)個\(c \times f_1 \times f_1\)的filter（\(c\)是圖像的channel數量，而\(f_1\)則為filter的大小），\(\ast\)代表卷積（convolution），\(B_1\)是偏移量（bias），最後的\(\max\)則代表激活函數RELU。

#### 非線性對應（non-linear mapping）

非線性對應，基本上就是持續利用一般CNN的方式將前一步每一塊的\(n_1\)維的特徵向量，分別轉換成\(n_2\)維的特徵向量，公式如下。

<center>

\(F_2(Y) = \max(0, W_2 \ast F_1(Y) + B_2)\)

</center>

#### 重建（reconstruction）

在重建的步驟中，我們要考慮的是每一個像素所要的值是多少，這個步驟可以想成在多個相關的高維度的特徵向量中，取一個平均，很湊巧的，這剛好也很像一般的卷積層（convolution
layer），公式如下。

<center>

\(F(Y) = W_3 \ast F_2(Y) + B_3\)

</center>

#### 訓練方法

在SRCNN中所採用的差異函數（Loss Function）是簡單的平均方根差（Mean Square
Error），定義為重建後的相片每一個像素與真正的圖片的每一個像素的差異，公式如下。

<center>

\(L(\theta)=\frac{1}{n}\sum_{i=1}^n \| F(Y_i; \theta) - X_i \|^2\)

</center>

\(\theta\)為SRCNN的參數，\(F(Y_i;\theta)\)為給定的SRCNN重建\(Y_i\)的圖像，\(X_i\)則為真正的高解析度圖像，\(n\)為拿來訓練神經網路的圖像數量或者是一個batch中所有的圖像數量。

### [Losses for Real-Time Style Transfer and Super-Resolution"](https://arxiv.org/abs/1603.08155%22Perceptual)\[2\]

這篇論文提供了一個做法，可以應用在圖像風格轉移（Style Transfer）以及超高解析度（Super-resolution）。

整個系統由兩個神經網路組成，其中一個是圖像轉移網路\(f_W\)，另一個則是可以用來定義各種差異的差異網路\(\phi\)。

#### 圖像轉移網路\(f_W\)

圖像轉移網路的輸入為一張圖像，輸出也是一張圖像，而這個網路的參數以\(W\)表示。

這個圖像轉移網路由5個residual block\[3\]所組成，而所有非residual的convolution
layer後面都會接上batch normalization。激活函數（activation
function）的部分，除了在最後的輸出層（output layer）使用scaled
tanh使得輸出的數值在0到255之間，其他都是使用RELU。

convolution
layer的filter（kernel）的數量上，第一層和最後一層使用\(9 \times 9\)個，其他層則是使用\(3 \times 3\)個。

#### 差異網路\(\phi\)

差異網路定義了各種差異函數（loss
function），輸入為兩張圖像，一張來自圖像轉移網路，一張則是真正的高解析度影像，輸出為一個實數（scalar）。

而這篇論文所使用的差異網路是16層的VGG網路\[4\]，並事先利用Image
Net訓練過。差異函數的部分，使用了兩個不同於傳統簡單的差異函數。（CHW代表feature
map各個維度的數值）

##### 特徵重建差異（Feature Reconstruction Loss）

這個差異函數的設計理念在於，當我們在看兩張圖片像不像時，我們並不是一個一個像素的比較，而是比較兩張圖片中的特徵像不像。因此，他拿差異網路中某一層的輸出，當作一個圖片特徵值，再以兩張圖片的特徵值的Euclidean
Distance當作差異。

<center>

\(l_{feat}^{\phi, j}(\hat{y},y)=\frac{1}{C_j H_j W_j}\| \phi_j(\hat{y}) - \phi_j(y)\|_2^2\)

</center>

##### 風格重建差異（Style Reconstruction Loss）\[5\]\[6\]

除了一般的特徵以外，我們也會需要圖像轉移網路正確的重建顏色、材質等等的內容，因此必須再加上風格重建差異函數。
在定義風格重建差異之前，我們先定義Gram矩陣\(G_j^{\phi}(x)_{c,c'}\)。

<center>

\(G_j^{\phi}(x)_{c,c'}=\frac{1}{C_j H_j W_j}\sum_{h=1}^{H_j}\sum_{w=1}^{W_j}\phi_j(x)_{h,w,c}\phi_j(x)_{h,w,c'}\)

</center>

接著差異函數就可以定義為

<center>

\(l_{style}^{\phi,j}(\hat{y},y)=\| G_j^{\phi}(\hat{y}) - G_j^{\phi}(y) \|_F^2\)

</center>

而一般比較每一個像素差異的差異函數，則可以寫為

<center>

\(l_{pixel}(\hat{y},y)=\|\hat{y}-y\|_2^2/CHW\)

</center>

有了這兩個網路後，訓練圖像轉移網路的方法則是最小化各式差異函數的權重和（weighted sum），優化的方法是梯度下降法（Stochastic
Gradient Descent（l()是差異函數（loss function）））。

<center>

\(W^* = arg \min_W E_{x,\{y_i\}}[\sum_{i=1} \lambda_i l_i( f_W(x), y_i ) ]\)

</center>

這篇論文在高解析度圖像這個傳統問題上，給了一個快速且有效的解法，快速的原因在於，在遇到一張新的圖片時，只需要把圖像餵進圖像轉移網路就好（一次forward
pass）。而在結果上，也大大的超越了之前的做法（一樣使用深度神經網路）SRCNN。

## 参见条目

  -
  - [光學解析度](https://zh.wikipedia.org/wiki/光學解析度 "wikilink")

  - [混疊](../Page/混疊.md "wikilink")

  - [过采样](https://zh.wikipedia.org/wiki/过采样 "wikilink")

  - [壓縮感知](../Page/壓縮感知.md "wikilink")

  - [核磁共振成像](../Page/核磁共振成像.md "wikilink")（MRI）

  - [迭代稀疏漸近最小方差算法](../Page/迭代稀疏漸近最小方差算法.md "wikilink")

## 參考資料

[Category:图像处理](https://zh.wikipedia.org/wiki/Category:图像处理 "wikilink")
[Category:光學](https://zh.wikipedia.org/wiki/Category:光學 "wikilink")
[Category:信号处理](https://zh.wikipedia.org/wiki/Category:信号处理 "wikilink")

1.
2.
3.
4.
5.
6.
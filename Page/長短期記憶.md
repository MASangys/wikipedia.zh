{{ request translation }} **长短期记忆**（，）是一种[时间递归神经网络](../Page/循环神经网络.md "wikilink")（RNN）\[1\]，论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测[时间序列中间隔和延迟非常长的重要事件](https://zh.wikipedia.org/wiki/时间序列 "wikilink")。

LSTM的表现通常比[时间循环神经网络及](../Page/循环神经网络.md "wikilink")[隐马尔科夫模型](https://zh.wikipedia.org/wiki/隐马尔科夫模型 "wikilink")（HMM）更好，比如用在不分段连续[手写识别](../Page/手写识别.md "wikilink")上\[2\]。2009年，用LSTM构建的人工神经网络模型赢得过[ICDAR手写识别比赛冠军](https://zh.wikipedia.org/wiki/ICDAR "wikilink")。LSTM还普遍用于自主[语音识别](../Page/语音识别.md "wikilink")，2013年運用[TIMIT自然演講資料庫達成](https://zh.wikipedia.org/wiki/TIMIT "wikilink")17.7%錯誤率的紀錄。作为[非线性模型](https://zh.wikipedia.org/wiki/非线性 "wikilink")，LSTM可作为复杂的非线性单元用于构造更大型[深度神经网络](../Page/深度学习.md "wikilink")。

## 历史

LSTM最早由[Sepp Hochreiter和](https://zh.wikipedia.org/wiki/Sepp_Hochreiter "wikilink")[Jürgen Schmidhuber于](https://zh.wikipedia.org/wiki/Jürgen_Schmidhuber "wikilink")1997年提出。最初的版本包含了cells, input以及output gates。

## 结构

[Lstm_block.svg](https://zh.wikipedia.org/wiki/File:Lstm_block.svg "fig:Lstm_block.svg") LSTM是一種含有LSTM區塊（blocks）或其他的一種類神經網路，文獻或其他資料中LSTM區塊可能被描述成智慧型網路單元，因為它可以記憶不定時間長度的數值，區塊中有一個gate能夠決定input是否重要到能被記住及能不能被輸出output。

右圖底下是四個S函數單元，最左邊函數依情況可能成為區塊的input，右邊三個會經過gate決定input是否能傳入區塊，左邊第二個為input gate，如果這裡產出近似於零，將把這裡的值擋住，不會進到下一層。左邊第三個是forget gate，當這產生值近似於零，將把區塊裡記住的值忘掉。第四個也就是最右邊的input為output gate，他可以決定在區塊記憶中的input是否能輸出 。

LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit）\[3\]，根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate\[4\]。

## 训练方法

為了最小化訓練誤差，梯度下降法（Gradient descent）如：，可用來依據錯誤修改每次的權重。梯度下降法在循環神經網路（RNN）中主要的問題初次在1991年發現，就是誤差梯度隨著事件間的時間長度成指數般的消失。當設置了LSTM 區塊時，誤差也隨著倒回計算，從output影響回input階段的每一個gate，直到這個數值被過濾掉。因此正常的倒循環類神經是一個有效訓練LSTM區塊記住長時間數值的方法。

## 应用

## 参见

  - [人工神经网络](../Page/人工神经网络.md "wikilink")

  - （PBWM）

  - [循环神经网络](../Page/循环神经网络.md "wikilink")

  - [时间序列](https://zh.wikipedia.org/wiki/时间序列 "wikilink")

## 完整阅读

  - [理解LSTM网络](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)，作者Christopher Olah，更新于2015年八月。

## 参考

[Category:人工智能](https://zh.wikipedia.org/wiki/Category:人工智能 "wikilink") [Category:人工神经网络](https://zh.wikipedia.org/wiki/Category:人工神经网络 "wikilink")

1.  S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.
2.  A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.
3.  [Neural Machine Translation by Jointly Learning to Align and Translate](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:bahdanau-iclr2015.pdf)，Cho et al. 2014年。
4.  [递归神经网络结构经验之谈](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)，2015年。
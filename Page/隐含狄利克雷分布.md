**隐含狄利克雷分布**（，简称**LDA**），是一种[主题模型](../Page/主题模型.md "wikilink")，它可以将文档集中每篇文档的主题按照[概率分布](../Page/概率分布.md "wikilink")的形式给出。同时它是一种[无监督学习算法](https://zh.wikipedia.org/wiki/非監督式學習 "wikilink")，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量k即可。此外LDA的另一个优点则是，对于每一个主题均可找出一些词语来描述它。

LDA首先由 David M. Blei、[吴恩达](../Page/吴恩达.md "wikilink")和[迈克尔·I·乔丹于](../Page/迈克尔·乔丹_\(学者\).md "wikilink")2003年提出\[1\]，目前在[文本挖掘](../Page/文本挖掘.md "wikilink")领域包括文本主题识别、文本分类以及文本相似度计算方面都有应用。

## 数学模型

[缩略图](https://zh.wikipedia.org/wiki/File:Smoothed_LDA.png "fig:缩略图") LDA是一种典型的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

另外，正如[Beta分布是](../Page/Β分布.md "wikilink")[二项式分布的共轭](https://zh.wikipedia.org/wiki/二项式分布 "wikilink")[先验概率分布](https://zh.wikipedia.org/wiki/先验概率 "wikilink")，狄利克雷分布作为多项式分布的共轭[先验概率分布](https://zh.wikipedia.org/wiki/先验概率 "wikilink")。因此正如LDA[贝叶斯网络结构中所描述的](../Page/貝氏網路.md "wikilink")，在LDA模型中一篇文档生成的方式如下:

  - 从狄利克雷分布\(\alpha\)中取样生成文档i的主题分布\(\theta_i\)
  - 从主题的多项式分布\(\theta_i\)中取样生成文档i第j个词的主题\(z_{i, j}\)
  - 从狄利克雷分布\(\beta\)中取样生成主题\(z_{i, j}\)的词语分布\(\phi_{z_{i, j}}\)
  - 从词语的多项式分布\(\phi_{z_{i, j}}\)中采样最终生成词语\(w_{i, j}\)

因此整个模型中所有可见变量以及隐藏变量的[联合分布](../Page/联合分布.md "wikilink")是

\[p(w_i, z_i, \theta_i, \Phi | \alpha, \beta) = \prod_{j = 1}^{N} p(\theta_i|\alpha)p(z_{i, j}|\theta_i)p(\Phi|\beta)p(w_{i, j}|\phi_{z_{i, j}})\]

最终一篇文档的单词分布的[最大似然估计](../Page/最大似然估计.md "wikilink")可以通过将上式的\(\theta_i\)以及\(\Phi\)进行积分和对\(z_i\)进行求和得到

\[p(w_i | \alpha, \beta)  = \int_{\theta_i}\int_{\Phi }\sum_{z_i}p(w_i, z_i, \theta_i, \Phi | \alpha, \beta)\]

根据\(p(w_i | \alpha, \beta)\)的最大似然估计，最终可以通过[吉布斯采样](../Page/吉布斯采样.md "wikilink")等方法估计出模型中的参数。

## 使用吉布斯采样估计LDA参数

在LDA最初提出的时候，人们使用EM算法进行求解，后来人们普遍开始使用较为简单的Gibbs Sampling，具体过程如下：

  - 首先对所有文档中的所有词遍历一遍，为其都随机分配一个主题，即\(z_{m,n}=k\sim Mult(1/K)\)，其中m表示第m篇文档，n表示文档中的第n个词，k表示主题，K表示主题的总数，之后将对应的\(n_{m}^k+1\)，\(n_{m}+1\)，\(n_{k}^t+1\)，\(n_{k}+1\)，他们分别表示在m文档中k主题出现的次数，m文档中主题数量的和，k主题对应的t词的次数，k主题对应的总词数。
  - 之后对下述操作进行重复迭代。
  - 对所有文档中的所有词进行遍历，假如当前文档m的词t对应主题为k，则\(n_{m}^k-1\)，\(n_{m}-1\)，\(n_{k}^t-1\)，\(n_{k}-1\)，即先拿出当前词，之后根据LDA中topic sample的概率分布sample出新的主题，在对应的\(n_{m}^k\)，\(n_{m}\)，\(n_{k}^t\)，\(n_{k}\)上分别+1。

\[p(z_i=k|z_{-i},w)\]∝\((n^{(t)}_{k,-i}+\beta_t)(n_{m,-i}^{(k)}+\alpha_k)/(\sum_{t=1}^{V}n_{k,-i}^{(t)}+\beta_t)\)

  - 迭代完成后输出主题-词参数矩阵φ和文档-主题矩阵θ

\[\phi_{k,t}=(n_k^{(t)}+\beta_t)/(n_k+\beta_t)\]

\[\theta_{m,k}=(n_m^{(k)}+\alpha_k)/(n_m+\alpha_k)\]

## 参考文献

[Category:概率模型](https://zh.wikipedia.org/wiki/Category:概率模型 "wikilink") [Category:机器学习](https://zh.wikipedia.org/wiki/Category:机器学习 "wikilink") [Category:自然語言處理](https://zh.wikipedia.org/wiki/Category:自然語言處理 "wikilink")

1.
在[统计学](../Page/统计学.md "wikilink")中，**高斯-马尔可夫定理(Gauss-Markov Theorem)**陈述的是：在[线性回归模型中](https://zh.wikipedia.org/wiki/线性回归 "wikilink")，如果误差满足零[均值](https://zh.wikipedia.org/wiki/均值 "wikilink")、[同方差且](../Page/协方差.md "wikilink")[互不相关](https://zh.wikipedia.org/wiki/Uncorrelated_random_variables "wikilink")，则回归系数的最佳线性[无偏](https://zh.wikipedia.org/wiki/偏差 "wikilink")[估计](../Page/估计量.md "wikilink")(**BLUE**, Best Linear unbiased estimator)就是[普通最小二乘法估计](../Page/最小二乘法.md "wikilink")。

  - 这里**最佳**的意思是指相较于其他估计量有更小[方差](../Page/方差.md "wikilink")的估计量，同时把对估计量的寻找限制在**所有可能的线性无偏估计量**中。
  - **值得注意**的是这里不需要假定误差满足或[正态分布](../Page/正态分布.md "wikilink")，而仅需要满足**零均值**、**不相关**及**同方差**这三个稍弱的条件。

## 表述

### 简单（一元）线性回归模型

对于简单（一元）线性回归模型，

\[y_i=\beta_0+\beta_1 x_i+\varepsilon_i; \quad i = 1, \dots n.\]

其中\(\beta_0\)和\(\beta_1\)是**非随机**但不能观测到的参数，\(x_i\)是**非随机**且可观测到的一般变量，\(\varepsilon_i\)是**不可观测**的随机变量，或称为随机误差或噪音，因此\(y_i\)是**可观测**的随机变量。

**高斯-马尔可夫定理的假设条件**是：

  - \({\rm E}\left(\varepsilon_i\right)=0\) ，\(\forall i\)（零均值），
  - \({\rm Var}\left(\varepsilon_i\right)=\sigma^2<\infty\)，\(\forall i\)（同方差），
  - \({\rm Cov}\left(\varepsilon_i,\varepsilon_j\right)=0\)，\(\forall i\not=j\)（不相关）。

则对\(\beta_0\)和\(\beta_1\)的最佳线性无偏估计为，

\[\hat\beta_1 = \frac{ \sum{x_iy_i} - \frac{1}{n}\sum{x_i}\sum{y_i} }
                     { \sum{x_i^2} - \frac{1}{n}(\sum{x_i})^2 } = \frac{ \operatorname{Cov}(x,y) }{ \sigma^2_x } , \quad
    \hat\beta_0 = \overline{y} - \hat\beta_1\,\overline{x}\ .\]

### 多元线性回归模型

对于多元线性回归模型，

\[y_i=\sum_{j=0}^p \beta_j x_{ij}+\varepsilon_i\], \(x_{i0}=1; \quad i = 1, \dots n.\)

使用矩阵形式，线性回归模型可简化记为\(\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}\)，其中采用了以下记号：

\(\mathbf{Y}=(y_1, y_2, \dots, y_n)^T\) (观测值向量，Vector of Responses),

\(\mathbf{X}=(x_{ij})=
    \begin{bmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1p} \\
    1 & x_{21} & x_{22} & \cdots & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & x_{n2} & \cdots & x_{np}
    \end{bmatrix}\) (设计矩阵，Design Matrix),

\(\boldsymbol{\beta}=(\beta_0, \beta_1, \dots, \beta_p)^T\) (参数向量，Vector of Parameters),

\(\boldsymbol{\varepsilon}=(\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n)^T\) (随机误差向量，Vectors of Error)。

**高斯-马尔可夫定理的假设条件**是：

  - \({\rm E}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right)=0\) ，\(\forall \mathbf{X}\)（零均值），
  - \({\rm Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right)={\rm E}\left(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^T\mid\mathbf{X}\right)=\sigma^2_\varepsilon\mathbf{I_n}\)，（同方差且不相关），其中\(\mathbf{I_n}\)为n阶[单位矩阵](https://zh.wikipedia.org/wiki/单位矩阵 "wikilink")(Identity Matrix)。

则对\(\boldsymbol{\beta}\)的最佳线性无偏估计为

\[\hat\boldsymbol{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\]

## 证明

**首先**，注意的是这里数据是\(\mathbf{Y}\)而非\(\mathbf{X}\)，我们希望找到\(\boldsymbol{\beta}\)对于\(\mathbf{Y}\)的线性估计量，记作

\[\hat\boldsymbol{\beta} = \mathbf{M}+\mathbf{N}\mathbf{Y}\]

其中\(\hat\boldsymbol{\beta}\)，\(\mathbf{M}\)，\(\mathbf{N}\)和\(\mathbf{Y}\)分别是\((p+1)\times1\)，\((p+1)\times1\)，\((p+1)\times n\)和\(n\times1\)矩阵。

根据零均值假设所得，

\[{\rm E}\left(\hat\boldsymbol{\beta}\mid\mathbf{X}\right) = \mathbf{M}+\mathbf{N}{\rm E}\left(\mathbf{Y}\mid\mathbf{X}\right)=\mathbf{M}+\mathbf{N}\mathbf{X}\boldsymbol{\beta}\]

**其次**，我们同时限制寻找的估计量为无偏的估计量，即要求\({\rm E}\left(\hat\boldsymbol{\beta}\right) = \boldsymbol{\beta}\)，因此有

\[\mathbf{M}=\mathbf{0}\]（零矩阵），\(\mathbf{N}\mathbf{X}=\mathbf{I_{p+1}}\)

## 外部連結

  - [Earliest Known Uses of Some of the Words of Mathematics: G](https://web.archive.org/web/19990508225359/http://members.aol.com/jeff570/g.html) (brief history and explanation of its name)
  - [Proof of the Gauss Markov theorem for multiple linear regression](http://www.xycoon.com/ols1.htm) (makes use of matrix algebra)
  - [A Proof of the Gauss Markov theorem using geometry](https://web.archive.org/web/20040213071852/http://emlab.berkeley.edu/GMTheorem/index.html)

[Category:数学定理](https://zh.wikipedia.org/wiki/Category:数学定理 "wikilink") [Category:统计学](https://zh.wikipedia.org/wiki/Category:统计学 "wikilink")